[
  {
    "title": "What is Application Integration?",
    "raw": "what is application integration? this seems to be a very easy question, ain't it? but if you go to google and search the phrase \"application integration\", you will get a multitude of vendor-specific sites that try to define application integration to align with their marketing philosophy, not entirely saying what it is. some focus on heavy integration platforms, others around message brokers, while some also speak of api management solutions. all of those are elements that compose application integration and can be used or not, depending on what the requirements are and what the architecture of a particular application, system or it ecosystem is. why do we pose this question? after spending years in the field of application integration, we found that this is one of the most misunderstood and neglected areas of it. at the same time it can be one of the most complex and interesting challenges to solve within a modern it landscape. we currently find that application integration is not taught anywhere else than with the technology vendors. and as it is, all their curricula are usually limited by what they are actually selling, so some problems are either not addressed or solved in a vendor-specific way, which blurs the understanding of certain application integration patterns. they do not show trade-offs between different solutions and architectural approaches, leaving them to be done by the customer himself. we realized that there is an educational gap that needs to be bridged, so we decided to build an application integration compendium by providing perspective on different architectural styles, their trade-offs and integration patterns. a bit of history we believe that to understand any topic is foremost to understand the “why” behind it. in the case of application integration the “why” is hidden in its history and how communication developed over the years. why do we need to communicate? information has always been the key to everything humankind has done. the more information we had, the more power we could amass. nowadays it is estimated that on average we consume about 34gb of media daily (not accounting for other information, not screen-related!). most people do not do much with that information, as most of it is forgotten in the next few days. but just understanding the capacity of our brains to process and store data, kind of shows the \"why\". we are information hungry! we strive to exchange information all the time, because this is basically what we consume, whether it’s gossip, a tv drama, or actually useful knowledge! but going back to application integration. the most basic form of data exchange is a point-to-point (p2p) connection between two separate instances of some sort of software. in layman's terms this is a form of communication resembling a conversation in person between two people. this kind of communication was the first of its kind recognised in computer science, starting with the creation of arpanet and the first connection between computers in 1969 made with the ncp and later on in 1974 implementing the use of tcp. this is the protocol we rely on to this day, last updated in august 2022 as rfc 9293. the essence of application integration application integration is the strive to connect different applications into a larger ecosystem that functions as one. but what does that mean to function as one? in essence, it is the capability to provide the right data to the right place at the right time, where application integration defines how that data transfer should happen within those given parameters. as with everything that is done in it, this can be done in a multitude of different ways, some better, some worse. this is the responsibility of application integration architecture to answer how to build integrations between applications that are coherent with the overall architecture of the application or the entire it landscape. the development of application integration over the years. since 1969 the field of application integration has changed a lot, and while we still use point-to-point communication, each year brings technological advancements, with which we are able to distinguish a few application integration architectural styles that are common in the industry. we see it as important to highlight them as they roughly define what might be the focus of an application integration engineer. request - reply as mentioned above, with the creation of arpanet, request-reply is the most basic form of interoperation, commonly used to this day and utilized by all applications that want to exchange data. it is a simple, decentralized approach, which usually comes down to a single http call to an interface. this approach is commonly found in small applications or microservices environments (e.g. microservices chaining). this is the building block of all communication onwards, as all applications, even with middleware in place, rely on a request-reply relationship. benefits: fast and fairly easy to build, quick to deploy, cost effective in a small scale or short perspective, problems: complexity grows exponentially with each integration, heavy impact on application code and configuration with a medium or larger scale environment, tight coupling of applications (spaghetti architecture/big ball of mud), no reusability, low or no maintainability at a larger scale, low or no proper logging and monitoring, close to no capability to refactor or introduce changes fast, difficult or unavailable to work in a high available mode, zero trust architecture is hard to implement. file transfer as of the creation of arpanet, further software development followed with the creation of file transfer protocol in 1971. this is a direct result of the creation of telnet and netrjs as described in the rfc114. this protocol is among the first to provide indirect access to other systems. what that means is that it was the first protocol enabling abstracting of interfaces as well as decoupling of systems. while it enabled applications to exchange larger sets of data early on and is used until this day, it is usually not the best choice for interoperability. benefits: fast and fairly easy to build, quick to deploy, cost effective in a small scale or short term perspective, provides partial loose coupling between applications, problems: complexity grows exponentially with each integration, heavy impact on application code and configuration with a medium or large scale environment, may lose the benefit of decoupling alongside growing complexity, very limited reusability, low maintainability at larger scale, little or no proper logging and monitoring, low capability to refactor or introduce changes fast, difficult or incapable to work in a high available mode, zero trust architecture is hard to implement. broker application integration broker type architecture solves some key problems of the request-reply by introducing a centralized application that facilitates the integration needs in a more organized manner. this is done by managing connections and communication orchestration between different applications for them. unfortunately in certain cases this might not be enough to solve all problems, depending on what we're trying to achieve. this type of architecture is commonly used in small and medium enterprises, where the number of integrated systems is low (we usually say under twenty, but it is completely arbitrary). furthermore this approach can be divided into two distinct patterns: messaging service (e.g., jms, amqp, mqtt) - where an application message broker is introduced to facilitate all communication between systems on an asynchronous basis. this approach is common for event-driven architectures or some microservice system architectures, where individual containers communicate with each other on a publish-subscribe basis. integration broker - broker type integration platforms with dedicated tooling that provide single-purpose services tailored to the needs of each individual system. the services themselves have very limited reusability, but code reusability is enabled through code libraries and common processes. benefits: provides partial loose coupling of applications, code reusability possible, lower impact on application code and configuration, maintainable, but may be challenging at a larger scale, enables zero trust architecture to a limited extent, better error handling, problems: single point of failure (partially mitigated by multi-node ha), none or very limited services reusability, dedicated single purpose flows, limited capability to refactor or introduce changes fast, requires effort and resources to build, api-led architecture application integration api-led is an architectural concept that emerged as a refinement of service-oriented architecture (soa) that emerged in the late 1990s. while soa had its ups and downs, it evolved over the years to the current form. the original approach focused heavily on exposing business services first, with underlying layers of services to support them. api-led focuses around domain and data services, where the logic of a domain service is responsible for orchestrating the flow of data. data services expose data in a standardized manner to enable the creation of services providing different data objects or functions related to said objects (e.g. crud). although originally soa was built around soap services and xml, api-led embraced restful services as easily. before the emergence of cloud-native technologies, this approach struggled with similar problems as the broker architectures in terms of availability and performance. so far this is the most successful approach for supporting large scale enterprise application integration, especially in organizations that are not technology driven. benefits: provides loose coupling, major reusability (code, apis, services), supports zero trust architecture, inline with microservice system architecture, highly scalable, both horizontally and vertically, very low impact on application code and configuration, high capability to react to changes, mitigates it landscape complexity, problems: requires effort and resources to build, requires an investment in technology, licenses, additional tooling, with first integrations it may seem to increase complexity and cost (lowers with each reuse), requires highly trained integration architects to be successful, so what is application integration? in short, this is a quite complex field that addresses the aspects of planning, designing and implementation of the ways all technology we create, exchange information. with this short article we would like to invite you to explore the topic of application integration with us and dive into the complex details of different styles, approaches and architectural trade-offs that come with modern day challenges of an it landscape."
  },
  {
    "title": "Data Integration vs Application Integration",
    "raw": "data integration vs application integration the issue of “integration” working in any industry brings all sorts of terms and abbreviations that are specific to that particular industry or even specific to a narrow field of it. during our work day we more often use job-specific jargon than we think, and that jargon when used outside of the industry context may be a cause of a lot of misunderstandings or even serious problems. that is exactly the case with data integration and application integration. both of these terms use the word “integration”, which is commonly used by specialists in those two fields alone without the preceding word pointing to their respective terms. why is that a problem? people outside of that field mostly hear the word “integration” instead of the full term and because of that they tend to think that this is one and the same thing. to be honest even specialists in those fields make that mistake when they are not aware of the existence of the other field in it. an application integration specialist might think: “oh, data integration, yes, we move data from place to place, hence we integrate it! that is the same thing!”. this way they create a little bit of chaos and misunderstanding, that may sometimes result in later organizational problems, like a lower budget, no willingness to invest in technology (because we already did invest so much in this area - the other area, because one was mistaken for the other), etc.. what is data integration? so what actually is data integration and how does it differ from application integration? while researching the topic to give a complete answer, we stumbled upon a very descriptive analogy: “consider a room where different puzzle pieces are scattered all around, each with a picture on it. now, what do you do if you want to see the complete picture? you bring all those pieces together, connect them, and complete the puzzle, right?” data integration is the process of combining and harmonizing data from multiple sources (multiple pieces from a puzzle) to provide a unified view (the complete puzzle). at least this is the short and simple definition. but what does that actually mean? the focus of data integration is to make data, which is scattered among various systems, accessible, transformed, and deliverable in a consistent and meaningful manner across the whole organization for business intelligence (bi) as in reporting and data analysis, or use by other applications by utilizing the master data management functionalities. this typically involves data cleansing, transformation and synchronization. data integration is a crucial component in both traditional data warehouses and modern cloud-native solutions like lakehouses, facilitating seamless access and utilization of data across the organization. this means that the creators of data integration tools also provide certain capabilities to move data from sources and to consumers as this is often the requirement from customers who do not have their own application integration capabilities. this often involves setting up api connections, both hosting and calling, and directly accessing databases via odbc or jdbc connections. what is application integration then? as we already defined in the article “what is application integration?” - it is the strive to connect different applications into a larger ecosystem that functions as one and by that we mean it is the capability to provide the right data to the right place at the right time, where application integration defines how that data transfer should happen within those given parameters to achieve the near-real-time alignment across different applications. application integration tools, despite the main focus on connecting systems, are also capable of doing sophisticated data integration other than formatting or structuring. while these capabilities are usually very handy in solving specific problems, our advice is simple - please do not do data integration in the application integration layer! putting business logic over application integration platforms is usually not a good idea. unless you are working closely with a good data architect and you are capable of documenting all of the transformations properly with the documentation being easily searchable and accessible, this is usually a problem as application integration platforms are not considered to be business systems. unfortunately, when data integration is done on the application integration tools, it is usually done as a workaround to a problem that needs to be fixed later. as our experience shows, it is almost never fixed and remains in place, causing all sorts of maintenance issues, additional operational costs, further development problems, and serious production failures, with new releases that did not take this additional logic as a dependency to be tested. the trade-offs taking the above as it is, there is an overlap between data integration and application integration. to a certain extent it is true, as a lot of the data integration tools provide application integration capabilities that enable the access and delivery of data. on the other hand application integration technologies often also have specific data integration capabilities available. in that way they contribute to additional confusion as to what is the scope of this field. in the case of using data integration tools to facilitate application integration, as with all of the architecture questions, the answer to this problem would always be “it depends”. several factors could be decisive in whether or not the tool should handle application integration: organization size - if the organization is small, most of the time there is no budget to build a robust enterprise architecture with fancy features and proper division of responsibilities. in those cases “done is better than perfect”, as implementing a highly sophisticated architecture may not contribute to company growth and would be too costly at this point, it maturity - low maturity might contribute to additional complexity and make maintaining the it landscape troublesome. in large environments, adding another component may seem harmless, but it will further contribute to overall complexity that is unstructured and hard to maintain, also called “spaghetti architecture” or a “big ball of mud”, making it stiff and costly to maintain. in those cases, a clear division of responsibilities would be beneficial, preexisting application integration platforms and capabilities - if there are preexisting application integration tools available, this requires a validation, as too do they enable transport capabilities that are required by the data integration use cases (batch, on-demand, event-driven). if the capabilities are sufficient, they should be used, rather than the ones provided by the data integration tools, use cases of the data integration tools - depending on the use cases, whether it is an operational data hub, a business intelligence tool, or ad hoc usage by business applications and processes, the capabilities may differ with applied tools, as also some easy transformations, like formatting or structuring, may be done over the application integration and will be sufficient. architectural oversight as far as it’s true that every company has a different structure, roles and especially the names for them, it doesn't really matter in terms of the scope of responsibilities. these can be divided into the responsibilities of application integration and data integration, which in a perfect world would fall under the respective responsibilities of an integration architect and a data architect. while, like with tools, a company may have both, none or one or the other of the architectural roles, there are a few helpful guidelines to establish proper oversight. these can be summarized by asking four key questions: what, where, when and how. what? there needs to be a common understanding of data objects and how they are supposed to be used, like what are the edge cases for specific data points and rules for filtering them into specific scopes. what data objects are we working with? what is the business meaning of those data objects? what is the context of their use in a business process? what is the common meaning of these data objects in a specific business domain? what is the scope of the data object actually consumed? where? the data objects in a complex environment need to have a clear origin and there needs to be a clear understanding as to who (what system) is the consumer of these data objects. where is the data created and stored? where (what system) should the data be used? is there a data multi-mastery conflict? is the data split over several systems and need to be merged? when? knowing when along with what and where the data is used gives a full understanding of it in the business landscape, enabling the organization to provide proper data transport in a timely or even real-time manner based on actual needs. when is the data used by the business? is the data freshness an important feature? when should the data be moved and what triggers transport? when does it add value to the respective business domain? how? this is the underlying technical process of transporting data between systems, which involves data transformation (simple formatting over the application integration layer, or heavy changes over a data integration tool), orchestration as well as enrichment. this is when tools, protocols and formats should be discussed. can the data be classified as an event? is it pulled on demand? should it be a batch transfer? what is the orchestration and enrichment needed? so what is the difference? as stated, the difference between the fields of data integration and application integration can be easily distinguished at a logical level. the key problem lies in the fact that the tools used for both compensate in some way for the lack of capabilities of the other field. this makes it very difficult to clearly divide those capabilities in a live environment, as it requires a specific architectural skill set, as well as a high level of maturity. as the company and its it landscape grows, it is worth understanding this logical division and onboarding proper tools for each field before the capability gap becomes a burden for the business."
  },
  {
    "title": "Modern Application Integration Principles",
    "raw": "modern application integration principles improving the maturity working in corporate environments often entails dealing with technical debt that arose over the years of the company building up its it, merging with other companies, acquiring software, saas solutions, etc.. technical systems are often at the very end of this list, as they have lower priority in the eyes of the business, as they do not provide direct business value, at least not one that will be easily understood. application integration is unfortunately one of those technical systems that are often neglected, especially if the business is not technology driven (so not e.g. telecommunications, technology vendors, saas solution vendors). to improve maturity is to strive to improve applications, infrastructure, and networking, so in general a lot more than just front-end applications that the business uses. while improving it maturity is essentially a very broad topic, here we wish to focus on one of the first steps in improving the maturity of the application integration field. establishing application integration principles if we were to take a dive into architecture or strategy documents in different companies, we would probably find that the way principles are defined varies not only between companies, but also between these documents. to make our approach here a bit cleaner, let’s first try to define what a principle is. according to the merriam-webster dictionary it is: principle 1 a : a comprehensive and fundamental law, doctrine, or assumption b (1) : a rule or code of conduct (2) : habitual devotion to right principles a man of principle c : the laws or facts of nature underlying the working of an artificial device taking the first definition, we wish to propose to you some fundamental guidelines as principles to follow on your application integration journey for improvement. let’s dive in! business alignment principles focus on business value while application integration platforms are technical systems, the focus of their use should be on solving real business problems and contributing to the overall company goals. the key element here is to ask “why?”, so that making integrations for the integration's sake can be avoided. there needs to be a clearly defined goal to achieve with outlined benefits that the integration will deliver. alongside that, there is a need to measure the success of the integrations, which can be achieved by adding the right performance indicators (kpis) to track it. for example if the goal is to improve order processing, average order processing time might be a good statistic to track. with all this outlined before implementation, it is easier to properly prioritize integrations that will support the organization's growth and later on demonstrate the return on investment (roi). questions to consider: why are we applying application integration here? what problem are we solving? what can we measure to see the problem clearly and later measure the success? what is the business priority of this integration for now and the future? how does that translate to roi? alignment with business processes for application integration platforms to succeed, it is crucial for the specialists working on them to have a high-level overview of what the business process is, as that defines how different parts of the company interact with each other. this in turn enables us to identify data exchange points between business applications that support those processes. this approach can help with augmenting existing workflows, further automating manual tasks and data exchanges where it is applicable. on top of that it can also help identify and break down information silos within departments to additionally enhance collaboration. business process alignment helps build meaningful integrations that are more likely to be adopted by other departments that require the same or similar data sets. questions to consider: what business process are we supporting? what are the data exchange points (both manual and automated)? what manual tasks can be automated? what are the data sources? how can these integrations be reused? technical principles standardization standardization is a strive to limit the amount of work needed to create, deploy and operate it solutions. it can be applied on many levels, from architectural patterns, through the application of specific solutions, down to the code level. it saves time over development, reduces errors and makes troubleshooting easier. within the field of application integration this can be done by: applying proper integration patterns - to apply standard development practices, identify components that need to be built or extended, assess workload, using industry standard protocols - using common and well known protocols such as soap, rest, jms, amqp, grpc, etc., agreeing on data formats - to ensure seamless data exchange between applications, regardless of their internal data structures, using common code libraries - using code wrappers dramatically shortens development time, especially for connectivity, logging, monitoring, error handling and security, considering integration tools - having a chosen integration tool (e.g. middleware, messaging service) can streamline the development process and reduce the learning curve for developers. loose coupling introducing and facilitating loose coupling enables the applications to have minimal (or minimized) dependencies on each other. this in turn allows them to be developed, deployed and scaled independently without impacting the entire ecosystem. this improves fault tolerance and makes maintenance easier. loose coupling can be supported by: facilitating asynchronous communication (event or message-driven) using queues or pubsub mechanisms, using api gateways as a means of single point of entry, separating the consumer applications from api providers using abstraction layers that hide the complexity of accessing data, or provide, tailored on a need-to-know basis, service apis to consumers, reusability reusability focuses on creating application integration components and services in a way that their functionality can be reused. this reduces the development time and cost of future integrations as well as promotes consistency across your it landscape. this can be supported by: microservices architecture - functionality broken down into small and independent services makes them easily reusable, but requires governance over microservice chaining, as that might as well be a pitfall, orchestration driven service oriented architecture - for larger landscapes, where a large variety of applications and systems are found and microservices architecture is not applicable to all, it is worth considering abstracting those applications with tailored services, integration patterns - use well-established integration patterns alongside proven technological components like api gateways, event brokers, api or developers portal - where deployed apis are discoverable and can be reviewed, so that developers can first look for an existing api to reuse instead of building a new one. plan to scale and perform application integration services are prone to handle growing volumes of data and user demands. this means that they need special care in terms of performance as they facilitate communication between multiple applications. this is especially true for any e-commerce (or any other) businesses that experience peak sales periods, leading to heightened traffic and surges that need to be handled without compromising performance. this can be supported by: choosing the right integration pattern - it is worth considering if the communication should be decoupled by using asynchronous communication or streaming protocols for high-volume data exchanges, designing for horizontal scaling - alongside vertical scaling, designing for horizontal scaling is crucial, to create a fault-tolerant environment quickly adapting to peak traffic by utilizing automation. this also requires applications to be designed as compact as possible, possibly in a microservice architecture, to provide additional outscaling capabilities, optimizing performance - gathering and analyzing performance data to identify bottlenecks or other crucial points that can be improved in order to ensure smooth data flow. security security is a crucial aspect of all communication and data exchange, so implementing tailored and robust security measures is essential. sensitive information needs to be protected from unauthorized access, breaches, and manipulation. this can be supported by: authentication and authorization - implementing mechanisms to verify calling system identity and control access to data, data encryption - encrypting data in transit and at rest to shield it from unauthorized access and use, enabling “least privilege access” - this can be done by abstracting services and providing data need-to-know tailored apis, enabling centralized logging and monitoring - this supports anomaly detection and provides data for regular security audits that can tighten the security. operational principles logging and monitoring logging and monitoring of all events occurring over application integration components (integration platform, as well as clients and providers) are crucial to have a comprehensive overview of what is happening in the it landscape. early detection of errors and anomalies can prevent data inconsistencies. this can be supported by: metadata - provide proper metadata for each communication, such as various unique identifiers, timestamp, data origin, selected http headers, etc., as this provides crucial information to analyze any errors or anomalies, log aggregation tools - all logs, payloads (anonymised as needed) and metadata need to meet in a single aggregation tool to process it and make it available to be searched and viewed on demand, monitoring tools - use industry standard monitoring tools and dashboards to visualize log data and create custom dashboards with specific alert triggers to enable proactive operations. governance and ownership defining clear governance to oversee the entire api and integration life cycle, starting with planning and development, up to deployment and ongoing production maintenance. this ensures consistent practices between projects, promotes collaboration between different teams and avoids the uncontrolled sprawl of integrations between applications. adding clear ownership of each integration business and technical-wise to specific teams or individuals further ensures accountability for performance and maintenance. this can be further supported by: api contracts - adopting api first approach to application integration development. establishing a contract helps ensure that apis have consistency and are made to be reusable, alongside allowing parallel development, fast mock creation and testing, design guidelines - standardizing api design will ensure that apis built within the organization will be consistent, versioning and api lifecycle management - is enabling to constantly improve services offered both internally and externally in an organized manner, at the same time improving the stability of the it ecosystem, api discovery - allowing the apis to be discoverable in the ecosystem is crucial with the ever growing number of interfaces deployed. providing additional tooling to track dependencies of api consumption will be invaluable to analyze future impact, testing proactive testing throughout the integration lifecycle helps identify and rectify issues early on, preventing problems from cascading into production environments where they can disrupt business operations. often omitted or not adequately supported, it is a crucial part of the development lifecycle that improves the reliability not only of the integration solutions, but also of the applications used by business. this can be supported by: api testing - while in design, using the api-first approach, a mock service api can be provided to test with interface consumers to ensure the definitions, scope of data, or formatting are correct. unit testing - individual integration components and services can be unit tested to ensure they function as expected in isolation. system integration testing - focuses on verifying how different components of the solution interact and exchange data. this ensures a smooth flow of information across the entire integration. (end to end) user acceptance testing - tests the overall functionality of the business solution from a business perspective. this validates if the integration delivers the intended business value. other non-functional testing - that evaluates solution characteristics such as performance (stress testing, load testing), scalability, and security (pen-testing). furthermore, testing is strongly supported by standardization, as standardized components and interfaces are easier to test, and logging and monitoring, as gathering statistics and observing data flows during testing provides valuable insights. this can be further improved by: defining a testing strategy - plan your testing approach based on the complexity of the overall solution and the potential risks involved. automating testing - repetitive tests can be automated to save time and ensure consistency. this is especially useful for regression testing. however, manual testing remains essential for exploratory testing and validating business scenarios. continuous integration/continuous delivery (ci/cd) - integrating testing into your ci/cd pipeline to provide continuous feedback on the health of your integrations. documentation hardly any it specialists enjoy writing documentation. keeping a clear and up-to-date documentation for all integration flows is something crucial to an application integration platform success. this documentation should involve: technical specifications - these should involve use cases, edge cases, validation rules, volumetric data or predictions, orchestration logic, api definitions, or any custom made logic solution and data flow diagrams - these should show: logical views for solution components, bpmn defining the business processes, sequence diagrams defining the flow of information, low level design pattern diagrams for reference architecture, operational procedures for day to day operations and emergency: build and deploy procedures, logging and monitoring, root cause analysis, disaster recovery, service-level agreement (sla) proper documentation facilitates good knowledge transfer, simplifies onboarding for new team members, and ensures that there are smooth handovers between development and operations. when the integrations are well documented, they enable others to better understand, troubleshoot and scale not only the application integration layer, but also the rest of the it landscape. wrapping up the principles while the list we present above may seem a bit generic, it is important to start somewhere. these principles might not be specific enough for each company, but we believe that they cover a broad basis and provide guidelines for what should be followed to achieve a success with application integration."
  },
  {
    "title": "System vs Ecosystem Architectural Styles",
    "raw": "system vs ecosystem architectural styles many flavors of architecture when we think about architecture, we usually think of building a system. there are no right answers to how to build, most of the time we use our experience and expertise, or we look towards more advanced techniques like trade-off analysis to choose “the least worst” solution. luckily there are shortcuts that can help make decisions over what type of architectural style (or mix of those) you could use to build your system based on the architectural capabilities rather than functional behavior.   ip: neal ford, architecture foundations - styles and patterns  while it is beneficial to understand and know how to properly use and mix those architectural styles, when working with their capabilities and architectural characteristics, they focus mostly on a single domain system, monolithic or distributed alike. when we are working on enterprise application integration, the focus is usually on cross-domain functionality and enabling the data to be used in different contexts, often outside of their domain of origin (e.g. customer data from a customer domain may be needed by the order domain to fulfill orders or an anonymised version would be very useful to the marketing domain for market trend research). ecosystem abstraction layer cross-domain interoperability, while must be supported on a system level by exposing apis, encapsulating logic, should be addressed on a different abstraction level - ecosystem (or landscape).  if we consider a system to be defined as a set of elements or parts that are coherently organized and interconnected in a pattern or structure that produces a characteristic set of behaviors, often classified as its function or purpose, commonly focused around specific business needs, then what would be an ecosystem?  given that an ecosystem would be a higher abstraction layer from the system layer, this would mean that the aforementioned system becomes a part of the ecosystem alongside other systems with varied behaviours and functions. all of these systems considered as units, facilitating business needs, make up a large part of the business, company or enterprise themselves, creating its it landscape or as we name it, its ecosystem. now if we consider that the key aspect of the system is serving its functional purpose to facilitate business needs, then the main aspect of the ecosystem is how those systems interact with each other and cooperate to support the company as a whole.  this in itself brings different architectural styles that can be leveraged to ease the burdens of interoperability, amongst many other troubles.    looking from a perspective of an integration architect, that knows a few integration technologies, we can identify four distinct architectural styles, that can be used in various scenarios: point-to-point, event-driven architecture, integration broker, api-led architecture,  point-to-point an implicit architecture, where it is taken by usually without proper consideration simply by creating code. it is very common with small ecosystems of startups and is a very useful approach for quick wins. unfortunately with each new connection the complexity grows, adding to the number items on the maintenance list (e.g. credentials for each system, firewall rules, duplicate system access logic) and creating more and more coupling within the ecosystem as all of the logic is contained within the domain systems themselves. if the communication is standardized (which is rare) and proxied via an api management solution, this could be still valid for medium sized organizations.  the biggest pitfall of a point-to-point approach is that often it is not governed properly, leading to something called the spaghetti architecture, which is a state where most organizations struggle with their interoperability capabilities and seek help to resolve their maintenance issues, development bottlenecks or lack of data visibility. event-driven architecture event-driven architecture is a broker architecture, where an event-broker (or a message broker) is utilized to facilitate communication between domain systems, usually in an asynchronous fashion. in essence it is used similarly on a system level to the extent that some of the architectural patterns can be used exactly the same way.  the key to event-driven is to utilize events that are defined as facts that have occurred in the domain systems (e.g. a customer created an account, a new product bundle configuration was created, order was processed and sent). those events are transmitted through queues and/or topics, which are the transport layer that facilitates the communication. this architecture relies on business, error handling and orchestration or choreography logic to be contained within the domain systems and the event broker remains as an infrastructural system. integration broker integration brokers in comparison to point-to-point and event-driven architecture have an additional key value - the capability to create communication logic outside of the domain systems. this in turn gives this architectural style the capability to create an abstraction layer between domain systems and provide additional decoupling capabilities both with synchronous and asynchronous communication modes. this makes it a viable option for medium to large organizations. the fact that in this architecture we use a central mediator that facilitates transport logic and orchestration enables it also to provide more visibility and auditability within the ecosystem itself.  the key downside of this architecture is that while you can create many development standards, patterns and reuse, it does not have any architectural patterns as each integration flow is a custom workflow that supports a business process. while it is really hard to create flows that are entirely reusable, it is still possible in a specific set of circumstances (e.g. identical systems, with identical apis within different territories). api-led architecture api-led architecture, being a cousin to orchestration-driven service oriented architecture (odsoa), is the most mature and robust approach to enterprise application integration, enabling it to provide a great insight into the it landscape as well as individual systems and the data they provide or consume. the super power of this architecture is the capability to provide abstraction layers, that decouple the systems to an extent that in a mature it environment you can replace a whole domain system without impacting other systems in that it landscape.  the major pitfall of this architecture is that it is a lot more complex than its counterparts and requires a lot more effort and resources to implement at first. that said, the cost lowers over time as more and more data and services are exposed within the ecosystem creating reusability on a functional level, rather than only code. to be able to work with such an architecture devops capabilities are essential as with its complexity it will become hard to manage over time without proper automation to make the development, testing and deployment processes streamlined. architecture comparison it is very hard to compare those architectures between various implementations of many companies, as switching between them would require reimplementing a multitude of business systems to communicate differently based on the underlying business processes corresponding to the architectural style. that is why, while not impossible, it is not realistic to compare these architectural styles implementations within a standardized set of boundaries (a.k.a. quantitative analysis). it is however possible to provide a qualitative analysis, based on an overview of architectural characteristics of those architectural styles, which can be derived from professional experience, facilitating subject matter discussions, hosting focus groups and long analysis of those topics. that said, we would like to introduce such a comparison, which we created based on our experience and independent research. a more detailed description of the characteristics and the score for each architectural style will be provided in separate articles!   <<disclaimer>> before you jump into analyzing this table please note that while these are a result of our research, the actual outcomes in implementation may vary depending on several factors (e.g. organization and it maturity, used technology, ecosystem composition, business processes). this table should not be treated as a source of definitive truth, but rather as generalized guidelines by which you may choose an architectural style for ecosystem wide interoperability.  <</disclaimer>>  architectural characteristic name point-to-point spaghetti architecture event-driven architecture broker (mediator topology) api-led architecture if you wish to learn more about architectural styles for system architecture, you can do so by reading this book or attending a training/webinar run by mark richards and neal ford."
  },
  {
    "title": "Qualitative Analysis of Ecosystem Architectural Styles",
    "raw": "qualitative analysis of ecosystem architectural styles where do we begin? as we were preparing to write some of the articles and outlined our collective knowledge about enterprise application integration (eai), we identified many gaps in our understanding of the field we work in. some were assumptions made over time, others simply lack of working experience in certain areas, simply by a lack of customers that wanted to do something in a specific way. usually those gaps were easy to fill in, by discussion, a bit of research or reaching out to other specialists for an architectural sparring. one of the things we did not consider earlier was a proper division of architectural styles that could describe the various approaches eai specialists have towards solving interoperability issues within organizations. we tried to name those, but the names were always associated with some kind of technology and, to an unhealthy extent, their marketing practices and the jargon created by them. and that makes sense especially that no one is teaching eai in a technology agnostic way. so when we sat down to describe the architecture, it was in constant flux, with each new aspect of it creating doubt, attempts to find a better name or questioning if perhaps there is another architectural style that we’re missing. those struggles in the end lead us to create a first version of a comparison between architectural styles for it ecosystems. the bumpy way to set the tone we will not write here about the comparison itself, but rather about the process leading up to it. it was not an easy one to deal with. context switch when we usually think about architecture we mostly focus on system architecture (or design). that means we’re usually dealing with a single business domain and its subdomains. while when working on interoperability topics on the scale of an it ecosystem, the focus is a lot wider and cross-domain. if we consider this as a main factor when describing the architectural styles, this means that we need to figure out how we understand architectural characteristics that describe them. redefining architectural characteristics we took a moment to decide which architectural characteristics might be relevant for an it ecosystem wide. the key problem here is that there are numerous architectural characteristics and most people have their own definition of all of them. sometimes they do not match, not without a reason! to make them usable it is very hard to apply a generic definition, as a lot of them will be context specific, so applying something like iso25010 is not exactly doable. this itself was a process that required quite a few sessions to discuss the problem, with time in between to rethink what this all means. that said, let’s have a look at the definitions we have come up with for architectural characteristics that we have chosen for the ecosystem abstraction layer: cost related cost is an important factor in all it endeavors, as it often sets a specific direction as to what is feasible. at first we considered a single characteristic of cost to show in a simple way what implementing a specific architectural style would mean in terms of finance. while discussing it, we decided to split the cost into three distinct categories as that reflects better what might be the consequences of choosing a particular architectural style. development cost is understood as the cost of developing new integrations in the ecosystem, including the cost of changes needed to be made within business applications. this also includes the cost of testing new integrations and potential later changes. operational cost is understood as the cost of maintaining the interoperability between business systems within a specific architectural style. this includes the cost of maintaining, operating and monitoring the chosen eai tooling (integration platform, api management, event broker, devops tools, etc.) as well as the cost of providing a root cause analysis (rca) and bug fixing. architectural changes cost is understood as the cost of making significant changes to the ecosystem like replacing a whole application (e.g. moving from salesforce to m365 crm) or changing the data model of a communication and its impact. in essence it is about the degree of coupling between the eai tools and business systems connected through it. architectural and design time when discussing architectural characteristics around the ecosystem abstraction layer we divided them into two groups. starting with the architectural and design time considerations which relate to more static aspects of an it ecosystem and catering to interoperability, which relate to architecture and later design of specific solutions. abstraction is understood as the capability to hide complexity, logic and implementation details behind a well-defined interface, where high abstraction promotes loose coupling, that is mostly low level data coupling with protocol agnostic interoperability. this makes it easier to build systems in different technologies and makes it possible to evolve or even replace them independently. contract resilience is understood as the capability of a system (here mostly an integration platform) to adapt to changes in interface definitions, data models and data formats of the systems it interacts with. high contract resilience means that changes, even considered to be breaking in a point to point communication, might not impact the end consumer system. breaking changes are hidden and mitigated in the abstraction layers, e.g. a change adding mandatory fields in a response with a system, while reflected in api-led architecture in an adapter and service layers, might remain unmapped in the channel layer, not to change the customers contract). this is often critical to manage for ensuring continued interoperability in the face of system upgrades or changes in business requirements. simplicity is understood as the strive to minimize the number of components and/or interactions between them within an ecosystem, which in turn reduces the cognitive load of understanding its dependencies. by focusing on simplicity it is possible to better support maintainability and keep the ecosystem's architecture easier to understand and change. composability is understood as the capability of an architectural style to facilitate reuse, combining and recombining of data, code, libraries up to whole services or applications to provide more business value and new functionality, as well as the degree to which it is easy to do. focusing on composability makes the creation of flexible solutions easier by leveraging the existing functionalities across the ecosystem. extensibility is understood as the capability to add new features, systems, integrations without impacting or with limited impact when extending the existing functionality. this is not only understood as architectural extensibility where structures like messaging topics enable to add new consumers easily, but also the ease at which new systems can be integrated into the ecosystem (e.g. being protocol agnostic, providing api abstraction). it is the measure of the ecosystem's ability to grow while maintaining interoperability between business systems and domains. operational the second group of architectural characteristics that we have considered within our research into ecosystem architectural styles is tackling the dynamic aspects of what happens between it systems, so in essence topics surrounding the implementation and use of it systems and the interaction with integration platforms. testability is understood as the capability to easily and sufficiently test the business processes that rely on interoperability to complete successfully. this involves a wide range of tests, including regression testing of existing features and testing of new functionalities, starting with unit testing the integrated systems and integration flows, through end-to-end connectivity testing, user acceptance testing and up to performance and load testing. scalability is understood as the capability of the ecosystem to support business systems and handle increased load or volume of traffic without a significant performance degradation. this is critical for ensuring that the ecosystem can accommodate further growth and increased use of shared services. performance is understood as the ecosystem's capacity to handle high-volume data exchange with minimal latency. this is essential for ensuring that integrated systems can respond promptly and effectively, thereby supporting efficient business processes and a positive user experience. security is understood as the ecosystem's capability to prevent unauthorized access and data breaches by providing sufficient countermeasures. this can be done by addressing least privileged access, proper encryption, adhering to zero trust, etc.. it is essential for maintaining data integrity and confidentiality as data flows across system boundaries. observability is understood as the ecosystem’s capability to provide metadata and logs that enables the operations team to understand and observe the ecosystem’s behavior in real-time to support troubleshooting, operational and security anomaly detection and identifying performance bottlenecks. auditability is understood as the capability of the ecosystem to provide evidence, track and audit the ecosystem's activity for compliance and security purposes, by creating visibility into data flows and interactions between various domain systems. the next step with the context based definitions of architectural characteristics we were able to have proper discussions about the qualities of specific architectural styles for ecosystems. but first we need to define those architectural styles to be able to compare them. at first we defined this simply as: point to point - a simple architectural style for interoperability, where there is no need for mediators between systems. while some of you might question whether it is even an architectural style, we categorized this as an implicit style, where the architecture is assumed as it is developed. event-driven architecture - an ecosystem representation of a common system design architecture, governed mostly by the same rules. orchestration driven service-oriented architecture - an architectural style that is known to many integration architects and developers that spent years in the industry, as many practices of modern integration platforms derived from soa. as we discussed these architectural styles and their implementations we have encountered over the years, we realized that this list was somewhat wrong and did not match correctly with our experience and what we see happening in the industry today. we were missing at least one style in the comparison. digging further into the topic, we realized that what we consider service-oriented architecture, has evolved quite much over the years and can no longer be called that. as a result we have created a new list: point to point - as defined above, broker architectures: event-driven architecture - an architectural style that supports asynchronous communication by providing a technical broker to facilitate communication between systems, integration broker architecture - an architectural style that provides workflow like capabilities for application integration flows, enabling easily both synchronous and asynchronous communication (when paired with a message or event broker), api-led architecture - a somewhat distant relative to service-oriented architecture that provides capabilities to compose and reuse services built in three different abstraction layers of an integration platform. it enables both synchronous and asynchronous capabilities (when paired with a message or event broker), additionally we decided to add “spaghetti architecture” to the comparison, just to show how neglecting interoperability issues and architectural governance on an ecosystem abstraction level can change architectural characteristics of the ecosystem. this part was a long thought exercise that started with drafting an initial proposal of a comparison and then submitting it to several reviews with multiple independent experts in the field of eai or system architecture that we reached out to via linkedin, conference networking or previous work engagements. each review was followed by a sparring session, where the understanding of those architectural characteristics was further refined, a common dictionary established and the qualitative scoring on each characteristic for each architectural style was discussed, challenged and reassigned based on arguments provided by each side of the discussion. the changes made to the comparison were then communicated to other reviewers, further discussed and acknowledged. conclusion as a result of this long process of discussions, arguments and rethinking what architecture is on a level of an ecosystem abstraction we have finally completed a first version of the architectural styles qualitative analysis."
  },
  {
    "title": "Synchronous vs Asynchronous",
    "raw": "synchronous vs asynchronous? lifeblood of modern communication while communication was always the key to success for humankind, since 1969 and the creation of arpanet, communication has become something very different, quicker and more robust with every single decade, ever speeding up. the forecasts for 2025 are that we will produce 147 zb of data over the year, this means about 407200000 tb a day. to put that in perspective, youtube alone currently uses approximately 440000 tb daily, and your message on whatsapp is under the size of 5kb = 5120 characters (1 tb = 1073741824 kb, so around 214.7 million messages). we rely on data being moved around between systems and users. we consume more and more of it, and for most users this is just viewing things in a browser window on the screen. but deeper in the depths of the it systems, architects and developers need to make the decisions on how to move the relevant data between applications, for those users to see. communication mode while there are many ways to communicate between systems, using apis, integration platforms, message brokers, file transfers, etc., in the implementation details it comes down to a choice between two modes of communication: synchronous and asynchronous. let’s try to define those in simple terms, starting with a proper definition: synchronous 1 : happening, existing, or arising at precisely the same time 2 : recurring or operating at exactly the same periods [...] looking at those two definitions, if we try to find a real-life analog to a communication that is synchronous, the thing that comes to mind is a dialog. a situation where there are at least two parties involved in an exchange of information, where saying something usually is followed by waiting and actively listening to a response. asynchronous 1 : not simultaneous or concurrent in time : not synchronous [...] if this communication is defined as “not concurrent in time”, this means that the two participants do not communicate at the same time, so while one transmits, this is where the communication ends for the producer, and then the consumer becomes active. if the message is not picked up immediately, it is either lost or stored somewhere. a real-life analog to an asynchronous communication would be listening to a radio broadcast or exchanging messages over mail. blocking vs non-blocking communication one of the key concepts to understand when dealing with communication modes and at the same time the key differentiator between synchronous and asynchronous communication is understanding whether the communication is blocking or non-blocking. synchronous is blocking what does it mean that a communication is “blocking”? as defined above about synchronous communication, it is happening for all participants at the same time, where we have a sequence of requests and responses. the key aspect is that the party requesting is waiting for the response, so the resources used to make the call (ram, thread, etc.) are locked and the process will not move on until it gets a response or times out. the key consideration here is that the blocking time is directly dependent on the called application and the time that it needs to process the request to produce a response. this can be also applied to a chain dependency of multiple services calling each other (e.g. microservice chaining), so the time needed is a sum of processing times of those services or applications. while processing times may differ between implementations, the important consideration is how long the calling system can wait for the response (license limitations, process execution time, user experience, etc.) blocking communication is beneficial when feedback is needed as fast as possible. asynchronous is non-blocking on the other hand we have the “non-blocking” communication, which, by elimination, is asynchronous. based on what we already know about asynchronous communication, the participants are not usually active in the data exchange at the same time. they are often separated by a message broker, file server or other persistence, which provides a separation between the provider and the consumer making it possible for them to behave this way. while sending a message to a message broker is also a synchronous operation as the message broker responds with an acknowledgement of getting the message, the application sending that message does not wait for the consuming application to process the message. meaning that with asynchronous messaging the time to send out a message and get an acknowledgement is reduced, so this may lead to a significant improvement in processing speed, but the trade-off is the lack of immediate response. data volume when choosing the communication mode it is important to consider payload sizes and hope they might impact communication performance as well as application responsiveness. synchronous mode when considering a synchronous communication mode it is important to note that by making a synchronous call we are creating coupling, resulting in a dependency impacting performance and responsiveness. the larger the payload or processing time needed to produce it the bigger the impact on latency, that might be resulting in timeouts if not handled properly. looking at the ranges of data volume the impact will be as follows: messages below 10 kb or simple data retrieval will usually have a negligible impact on performance, messages between 10 kb and 100 kb or minor processing, like simple search, might cause problems if timeouts are not managed properly, messages above 100 kb or complex processing, e.g. patterns matching, while it will still work when communication parameters on both sides are properly managed, the latency might not be acceptable for most use cases, causing various responsiveness and performance issues. asynchronous mode asynchronous communication allows a lot more flexibility in handling payload sizes or processing time. because it provides decoupling, there is no direct dependency, which mitigates all impact on latency, performance and responsiveness that comes with synchronous communication. each system communicating will be able to fine tune this separately. larger payloads (files, batch transfers) will also benefit from asynchronous communication as the decoupling will allow longer processing times by creating a buffer for the consumer. use cases both synchronous and asynchronous communication has lots of applicable uses. it is impossible to mention all of them, as every single piece of equipment nowadays communicates, but we can group them into specific categories: sync list common use cases: real-time interactions, transactional communication (related to acid), situations where immediate feedback is essential, like data retrieval if data has to be presented to the user, async list common use cases: sending notifications or alerts background processing of tasks data synchronization between systems (eventual consistency) situations where decoupling and scalability are important, handling long processing times in one of the systems so which communication mode should you choose? well, to be honest we cannot really tell out. the key is to do a proper analysis of what the requirements are and choose based on those findings. synchronous and asynchronous communications differ and have different trade-offs to consider. the key aspects to analyze should be: is an immediate response needed? what is the payload size? what is the required responsiveness and tolerated latency? keep in mind that there are many more aspects related to choosing a communication mode, but they are also tied to specific architectural styles and patterns (e.g. there will be different issues between synchronous and asynchronous call within an api-led architecture than event-driven architecture) and we will be touching upon those as we explore those topics!"
  },
  {
    "title": "From Point-to-point to Spaghetti Architecture",
    "raw": "from point-to-point to spaghetti architecture starting with some basics while we’d like to discuss enterprise application integration it is important to start with the most basic approach to data exchange and why is it crucial to understand it fully before jumping into more complex architectural styles that support ecosystem interoperability. point-to-point as an architectural approach is exactly that. this is the most commonly known approach used by all developers and architects. so let’s take the time to explore the benefits and pitfalls of this architectural style. what is point-to-point (p2p)? to put this simply, point-to-point should be understood as a system or an application directly calling a different systems or applications api understood broadly. it can be a jdbc procedure call, an invocation of a rest or soap api, etc.. commonly found in every kind of environment, it is impossible to avoid, as it serves as a basis for all communication. the lack of a mediator forces all of the aspects of communication upon the business systems. those become responsible for the technical implementation of the communication, its security, governance over data and data models, etc.. implicit architecture since point-to-point, as a dominant form of interoperability in an ecosystem interoperability focus is mostly common within young, small or not tech driven organizations (one could say those with low it maturity), it is often created as a side effect of lack of architectural governance. in essence, it happens by accident, a sheer need to integrate applications that were introduced into the ecosystem without interoperability in mind, for example: “ok, so we have this custom application which we built that helps us manage assigning staff to specific client projects, but it would be so much easier if we would not have to input employee details by hand but get them from our workday instance!”. that way a p2p connection is created with every similar need and the interoperability is not governed but driven by at-the-time business needs. the road to chaos point-to-point, among other things, has a big pitfall that often, when unmanaged, as described above, leads to severe complexity that can even cripple business. this is often referred to as spaghetti architecture, because of the large quantity of tangled up interactions that, when charted out, often look more like a dish of spaghetti, rather than a diagram showing actual communication. we take this specific “architecture” as a reference point of what we do not want in our ecosystem and to be able to explain how this complexity can be resolved by various approaches, architectural styles and patterns. image source qualitative analysis before point-to-point leads us to complexity that could kill the business, it is worthwhile to explore what may be its cause and what other pitfalls may be awaiting us when implementing this architectural style. we can avoid some of the pitfalls by understanding what are the key architectural characteristics and how they are relevant in this setting. for that we will be using a comparison table that was produced through a qualitative analysis of architectural styles taking several architectural characteristics into account. if you would like to learn more about this analysis or read how we define those characteristics, you can do so by reading this article, where we explain how this comparison was created. architectural characteristic name point-to-point spaghetti architecture event-driven architecture broker (mediator topology) api-led architecture development cost \uD83D\uDCB2 \uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 operational cost \uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 architectural changes cost \uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2 abstraction ⭐ ⭐ ⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐⭐ contract resilience ⭐ ⭐ ⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ simplicity ⭐⭐⭐⭐⭐ ⭐ ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐ composability ⭐ ⭐ ⭐⭐⭐ ⭐⭐ ⭐⭐⭐⭐⭐ extensibility ⭐ ⭐ ⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ testability ⭐⭐⭐⭐⭐ ⭐ ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐ scalability ⭐⭐ ⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ performance ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐ security ⭐⭐⭐ ⭐ ⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ observability ⭐⭐ ⭐ ⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐⭐ auditability ⭐⭐ ⭐ ⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ cost analysis let’s start with the costs of point-to-point architectural style. there is no middleware used in this architectural style and it relies on direct communication between two applications, which means that development cost as well as operational cost are pushed onto the specific systems that wish to exchange data. what that means is that in this scenario we do not have any additional licensing costs, no specialized development or support team responsible for application integration. this lands p2p with a score of a single \uD83D\uDCB2 in the development and operational costs. architectural change cost is the cost that will vary the most depending on the size of the ecosystem and the number of interactions within it. the higher the number of systems and applications and the dynamic dependencies between them, the higher the architectural change cost will be. but if we assume a positive scenario that there actually is some governance (e.g. use of standardized, industry-specific protocols) over the ecosystem and its interoperability, that will limit the number of interactions between systems and as needed switch to a different architectural style, then it will not be allowed to grow to become a spaghetti architecture. in that case it will be feasible to introduce architectural changes with ease. this in turn gives p2p a single \uD83D\uDCB2 score in this category. architectural and design time analysis the first group of architectural characteristics that we can take a look at when analyzing architectural styles describes the qualities of the blueprint of the architecture. if we’d take a closer look at what makes point-to-point really good, we’d find that it is first of all its simplicity (5⭐) . this means that when implementing this architectural style, it will be quite simple to understand and test, as there is no added complexity of a mediator between the business systems. this comes with a specific corollary that this architectural style becomes more complex the bigger it grows, which means that if left ungoverned it will become the spaghetti architecture. the architectural downsides of this architectural style come from the same source as the benefits. having systems or applications interacting directly with each other means that we face tight coupling between them. the level of coupling that we’d usually like is “data coupling”, where at this point we’re facing not only “stamp coupling”, as in being bound by a specific data model, but also “external coupling” that forces the calling application into using a specific communication protocol. this in turn means that architectural characteristics such as abstraction (⭐) and contract resilience (⭐) will suffer greatly. this in turn impacts extensibility (⭐), because it is very hard to add new features, functionality and systems, when each time an integration is needed, the system engineers need to learn a substantial piece of a different system and the way it’s implemented. lastly, due to all of the previous downsides, composability (⭐) as an architectural characteristic also suffers, because as far as it is doable to reuse an api exposed by a specific business application, composing new functionality from existing components is pushed into the responsibility of the calling systems, which means that all those ned functionalities are encapsulated within those systems and not reusable by other applications. operational analysis the second group of architectural characteristics we can take a look at focuses around operational aspects of the ecosystem. the first and biggest benefit of a point-to-point architectural style is its testability (5⭐). due to the high simplicity of the ecosystem and the fact that the responsibility of testing lies solely with the business applications, testing the communication, analyzing the results and being able to report outcomes is fairly quick. unfortunately this ties in again to the aforementioned corollary, where with the growth of this architectural style, the complexity heavily impacts testability, which again is reflected over the spaghetti architecture qualitative analysis. the second operational aspect of this style that works well is performance (4⭐). this again relates to the lack of mediators, so as the communication is simpler, the latencies for communication are dependent only on the response time of the called systems. this can be managed and optimized, so the performance of the ecosystem is overall high. the reason that this is not the highest score is that this style usually does not enable efficient asynchronous communication.. the operational architectural characteristics that fall behind in the trade-off in this architectural style are observability (2⭐) and auditability (2⭐). these heavily rely on how the business systems are implemented and what metadata they allow to be extracted out of them. in essence if we’re dealing with legacy custom systems, they might be very hard to refactor to provide sufficient amounts of metadata (logs, correlation ids, etc.). a similar situation might happen with software as a service (saas) solutions, where customizing those to provide the needed information might be a costly endeavor, severely impacting those characteristics. furthermore scalability (2⭐) as an architectural characteristic is also low, because communication done over a synchronous protocol, that is the cornerstone of point-to-point, does not support horizontal scalability sufficiently, especially due to the fact that it requires additional tools to be introduced to support features such as load-balancing. that hinders the capability of the ecosystem to have automated scaling, which is also done differently with each business system. lastly, security (3⭐) in this architectural style is scored as average due to the fact that usually there is little to no overarching governance. this means that implementing least privileged access or following zero trust guidelines might be hard to achieve if not impossible. conclusions point-to-point is and will remain a valid architectural style for the ecosystem level. when implementing this, one should remember that it is not an architecture defined for extensive growth. it is valid for start-ups, small projects or data processing pipelines, where 2 or more systems can be treated as one from the perspective of the \"rest of the world\" in context of a given process. without proper governance it might quickly turn into a spaghetti architecture, leading to unwanted complexity and negative impact on business processes and their execution."
  },
  {
    "title": "Degrees of coupling in IT ecosystems",
    "raw": "degrees of coupling in it ecosystems eai and coupling one of the key problems enterprise application integration (eai) is addressing in various ways is coupling. while we explore how we can address it by applying specific architectural styles and patterns in other articles, we’d like to explore what is coupling and define, from a variety of types, which coupling types are relevant in ecosystem interoperability topics. what is coupling? to take a closer look at coupling, let’s start by looking at a few definitions:  in software engineering, coupling is the degree of interdependence between software modules; a measure of how closely connected two routines or modules are; the strength of the relationships between modules. coupling is not binary but it is multi-dimensional. wikipedia  coupling describes the independent variability of connected systems, i.e., whether a change in system a affects system b. if it does, a and b are coupled. gregor hohpe  if a change in one system, service or component, defined broadly, might facilitate a change in another system, service or component, they are coupled. mark richards  if we consider the above definitions, there are two specific statements we can derive from them, showing the dual nature of coupling:  no system is free from coupling, as there is always some sort of a dependency, e.g. to the operating system, infrastructure or other components, modules or applications. it is a matter of understanding the strength, distance and volatility of coupling.  therefore a system, on its own, cannot be decoupled from its environment. coupling is binary by design, meaning it is either intentional or implicit. components and applications are either coupled to each other or not, meaning they either have or do not have a dependency to one another (e.g. two systems exchanging data with each other have a dependency, whereas a mobile game on your phone and your hr system at your workplace most likely have no dependencies between each other). in most cases, those dependencies can be derived from architectural blueprints, architectural decision records, documentation, or other knowledge sources. however, the challenge arises when those dependencies are hidden or indirect, so we cannot rule out their existence, especially if we have limited knowledge of the (eco)system at hand.  therefore the system can be coupled or decoupled with another system.  in essence coupling cannot be treated as an on-off switch, it has multiple dimensions and degrees along those. furthermore coupling is contextual, which means that as always it depends on several factors whether or not a specific type or degree of coupling is wanted. so in essence any time you see a line between two boxes in a diagram depicting something about architecture or system design, that is some sort of a coupling that can be described. different types and characteristics of coupling there are various types of coupling, each with distinct dimensions and strengths. discussing all of the coupling possibilities would be a very long and difficult process worthy of a book. fortunately that work has already been done by vlad khononov in balancing coupling in software design. as we are going to dive into the topic we will be focusing only on those that bear significant relevance for interoperability on an ecosystem abstraction level.   as we are diving into architectural styles and patterns in eai we can witness all types and forms of coupling. some of them are very much wanted, others implicit and manageable, while some will be considered undesirable in specific conditions. let’s take a moment here to classify them: types of coupling while there are various types of coupling, we discuss them on different abstraction levels. some may be seen in the fine details of designs, others on a much broader scale. we can clearly differentiate between two distinct groups of coupling types based on where they derive from: architectural (static) coupling - describes dependencies that are directly deriving from the design in place. contract - described by the agreed scope of data that will be transferred between involved parties. while contracts can be one-sided or negotiated between parties, all involved are coupled this way, data format and type - deriving from specific contracts between modules and applications where data models are described in conjunction with respective formats, semantic - the generalized understanding and meaning of transferred data, both business and metadata, on the level of users, business processes, purpose and usage,  interaction - is the communication done in the synchronous or asynchronous fashion, conversation - the specifics of how the communication happens (e.g. pagination, caching, retry policies, technical and business error handling), operational (dynamic) coupling - describes dependencies that result from operating the systems and applications, and the behavior patterns that come with how the users use them. temporal - the sequence of events, how the logic is executed, whether or not there are race conditions, what is the processing time, are there other processes dependent on another process being finished (might lead to pathological coupling), interaction - extending the static coupling, this relates to the implementation of the interaction using, or being forced to use a specific protocol and how the system is dependent on it, technology - the inner workings of a specific technology used to implement the system, the way it works, how it interacts with the underlying operating system and infrastructure, location - the geographic location of where the systems are deployed, where are they available, what is the network configuration between them, semantic - extending the architectural aspect of this coupling, the meaning of an instance of data being exchanged and how that data influences the behavior of the implementation through data validation, transformation and error handling, coupling direction while discussing coupling it is important to also understand the direction of coupling, meaning which component is the one providing a dependency, and which one is consuming it. this can be later used to provide two specific measures: efferent coupling - a measure of how many components the analyzed component is dependent on, afferent - a measure of how many components are dependent on the analyzed component, measuring efferent and afferent coupling in the ecosystem is a useful tool to spot potential bottlenecks, components that might need to be split functionally or operated with more care, e.g. by applying automated scaling.  coupling strength one of the three main characteristics of coupling is its strength. over the years we were accustomed to having somewhat meaningless terms like “decoupled” or “loosely coupled”, which could have been interpreted in various ways depending on the context of the conversation or the individual interpreting them. that’s why a more precise classification is required. strength in this case would be classified as one of the following ordered from weakest to strongest: contract - components are contract-coupled if they rely on communicating through a data model that is integration specific. the contract outlines the terms and conditions of collaboration and is usually defined as a communication protocol between two systems.  model - happens when the data model related to a business domain and its data model is shared by multiple applications/systems, e.g. internal data model either used for communication or reused in the processes of more than one system. functional - two modules are functionally coupled if their functionalities are interrelated, which means that if the business requirements change and so does functionality in an application, the coupled application will most likely be affected, temporal (sequential) coupling - when logic needs to be executed in a specific order (may be also known as pathological coupling), transactional coupling - when several operations have to be carried out as a single unit of work, thus a transaction, symmetric coupling - happens when two or more applications implement the same functionality independently, giving the same goal by means of different implementations. when the requirements for the shared behaviour change, all applications need to reimplement the logic at the same time at risk of introducing bugs (e.g. mismatch in data cohesion), intrusive - instead of communicating through public interfaces, the client  communicates through, and thus depends on, the implementation details of the provider,  when it comes to eai, the usual and wanted coupling strength is contract coupling which is supported by the capability of integration tools and platforms to provide abstraction. model coupling may at times occur if the data models are not managed properly and leak through the abstraction layers to contracts. moving on to functional coupling, none of those types occur solely in the integration layers, as they derive from the business process logic, usually (hopefully) outside of the integration platform, which in turn only facilitates these coupling forms. coupling distance while it is possible to create monolithic software that is just a single block of statements with a lot of ifs, that is not the reality we are currently facing. over time various types of encapsulation were introduced to provide boundaries. in essence, coupling distance is bound to how closely related are the dependencies. we can distinguish distance levels against which we can measure coupling: statements, methods, objects, namespace or package, libraries, services, systems or applications,  there are two key, linear qualities we should track related to this: cost of change over distance - the greater the distance between components that have to change together, the higher the effort, and with that the cost of the shared change, e.g. changing a small statement in a method is far less costly than changing two systems that are coupled with each other with a contract coupling influenced by new requirements and changes to the data models. the cost is usually proportionate to the distance, lifecycle coupling - all software is involved in some sort of a lifecycle that may have multiple stages, ranging from requirements gathering, through design, implementation, testing, deployment and finally maintenance and updates. the lifecycle coupling is usually inversely proportional to the distance, coupling volatility the last characteristic that is crucial in understanding coupling, and especially the risks involved with certain tighter coupling instances, is volatility. this is a bit of a compound characteristic of two particular qualities: frequency of changes - how often a coupled component changes within the software development lifecycle due to new features, floating requirements, bugs and fixes, customization, etc., impact - how much of a ripple effect a change is going to cause, and by that any cascading additional changes that will influence other components and teams working on them whether by chance or deliberately, volatility reshapes the focus of investigating coupling to discuss its influence on the software in time, as well as how this software may and will evolve over time. focusing on volatility invokes a discussion about whether or not certain instances of very tight coupling are acceptable, or can we allow ourselves to cut-corners and simplify design (introducing more coupling),  where for example a solution might be only temporary. conclusions coupling is a multifaceted concept that is quite difficult to grasp fully, and it is an intrinsic part of any it system. while it is impossible to completely eliminate coupling, understanding its various types and degrees is crucial for designing and maintaining efficient, adaptable, and maintainable systems. by carefully considering factors such as architectural characteristics, data models and contracts, interaction between components, operational considerations, business logic, as well as coupling strength, distance, and volatility, architects and developers can make informed decisions to minimize unnecessary dependencies and promote system resilience."
  },
  {
    "title": "Event-Driven Architecture for IT ecosystems",
    "raw": "trying to avoid the italian cuisine problems modern businesses rely on a diverse ecosystem of applications, including custom-built systems and cloud-based services. integrating these disparate systems can be challenging, often leading to tightly coupled (spaghetti) integrations that are difficult to maintain and scale. event-driven architecture (eda) offers a more flexible approach to interoperability. by leveraging events as the foundation for communication, eda enables systems to exchange data more loosely, improving maintainability and facilitating easier adaptation to changing business requirements. at the beginning there was messaging there are various types of objects and information that pass through message brokers. to understand event-driven architecture (eda) it is crucial to be able to navigate easily between them: message - is the core object of communication, which can be literally anything. following the examples given by jacqui read at ddd europe 2024 in the talk “every event everywhere all at once”, this might as well be “high five! well done!”. a message can have more narrowly refined subtypes that serve specific interoperability function: command - is a subtype of a message where we tell the other side of the communication to do something, like “create customer”, this literally means that the data set transferred between the systems is sent with the intent to invoke an action on the receiving end, state - is another subtype, with a passive message, transferring a state of something, telling us for example “ticket 123qwe is in progress”, this is often reflected by a very light payload, sent periodically, most commonly containing an identifier, described state and some metadata, event - is another subtype of message that states a fact that occurred. this means that the message sent between the systems contains metadata and a payload (or identifier of one) that describe something that happened business or operations wise. but since we’ll be focusing on it a little bit more, we’ll get back to it in a moment, given the above, we can notice that a very common mistake happens, that there is a tendency to name something eda, where it is not actually event-driven, so it uses message types that are not events. to make this clear and before we take a deep dive into eda, we need to define what is an event and further explore the other types of “objects” that can go through a message broker. events vs the world let’s start with a generic definition. as we already established, the root of all things going through a message broker is a message. it is the most simple option and it has the least boundaries of what it can be. simply put a string as payload and that is your message! but events are not just some random messages. they are specialized in terms of the contents and meaning, so let’s look at a few definitions: event 1 a : something that happens : occurrence b : a noteworthy happening [...] essentially in programming an event is the fact that something happened. which means it would be associated with verbs in the past tense like: created, started, completed, updated, failed, etc.. this is reflected in the messages metadata and payload. for example, if you use topic hierarchy the metadata would be hidden in the name of the topic, e.g.: /eshop/order/created/v1/{orderid}, and the payload would be the details of that order, even if it is just an id of a record for later polling. as mentioned earlier, there are other types of messages that are common to it ecosystems. for comparison, the implementation philosophy of soap was commands and responses to them. the messages were sent to an api that was focused around command operations based on imperative verbs, like get, set, update, which resulted in names like e.g. getcustomer, updateorder, setstatus. similarly in rest, while the api represents the resource, the imperative verbs of http methods (get, post, put, patch, delete) represent the command nature of the invoked operation. commands imply that the calling system is in control of the transactional behavior of the api provider. another option is to use messages to transfer state, where it can be described as statuses like those found on a common kanban board (e.g. to do, in progress, done). and lastly messages can transfer whole binaries of documents, which is a nice feature, but lands more in the spectrum of mass transfer topics, rather than real-time application integration. what is event-driven architecture? as we now know what an event is, we can define what event-driven architecture is. the most basic understanding would be that it is an architecture that detects, reacts to and processes events. it is a distributed architecture that is composed of event producers and event consumers, which are the components (applications, systems, modules of applications) that contain business logic. the communication between them, as mentioned, happens usually via a message broker, so a dedicated technical application that facilitates asynchronous message exchange. context: integration vs. distributed systems as we have defined roughly what eda is, there is one more distinction to make. context matters! when using eda as an architectural style, there are different considerations an architect must take into account when designing a distributed system and when designing application integration. while with a single system this is usually a single team that has a high level of control over development and the domain, for integration that would turn into dealing with cross-domain connections, coordinating between several different development teams and system architects. this brings additional complexity and certainly will influence several aspects of implementation. furthermore it is worth noting that in distributed systems asynchronous communication may happen without the use of a specialized message broker, by using e.g. in memory queues. qualitative analysis as we did with point-to-point, let’s explore the qualities of event-driven architecture and look into a few pitfalls that can be crucial to consider when trying to apply this architectural style. for that we will be using a comparison table that was produced through a qualitative analysis of architectural styles taking several architectural characteristics into account. if you would like to learn more about this analysis or read how we define those characteristics, you can do so by reading this article, where we explain how this comparison was created. cost analysis let’s start with cost considerations of event-driven architecture as an architectural style. with this approach to ecosystem interoperability a message or event broker is used as a mediator between different domain systems. this brings an implication of development and operational costs being higher than with point-to-point. the use of a technical broker brings additional complexity. depending on the choice of a broker, this means that all domain systems that wish to communicate need to learn the brokers protocol as well as learn to properly produce and consume events. this increases the cognitive strain on development teams. on the other side message brokers are incapable of building sophisticated processing logic as they only facilitate communication, so using them does not require a dedicated development team that would implement changes. this brings the development cost to be higher than point-to-point, resulting in \uD83D\uDCB2\uD83D\uDCB2 for eda in this category. operational cost is higher as well, as there is another component in the ecosystem, which often requires a license to run as well as a team to maintain and operate it. this also increases overall operational effort in all environments to provide analysis and fixes for any issues that might occur, as there are more components that need to be looked into. this gives the operational costs a score of \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2. architectural change cost, even if it is an unpredictable variable, is fairly low (\uD83D\uDCB2\uD83D\uDCB2). this is due to the fact that a message broker is providing a separation layer between domain systems, splitting a dynamic quantum, that would be common for a point-to-point connection, into two or more quanta, depending on the architectural pattern used. this makes the domain systems loosely coupled between each other, so in turn easier to modify or entirely replaced. architectural and design time analysis let’s first take a look at the design time qualities of eda. like with point-to-point, the superpower of eda, in this area, is its simplicity (4⭐). implementing this architectural style is fairly simple in terms of architectural components. it is very clear and easy to understand. the reason the score is only 4⭐ is that this style forces all the domain systems to learn how to work with events alongside making additional p2p calls while processing them. secondly, the use of message brokers, especially patterns related to the use of topics as a communication medium, enables composability and by that also a certain level of reusability of events and related data models. however the downsides of eda as an ecosystem architectural style, listed below, bring the composability score to 3⭐. while overall the scores are higher than with p2p, there are aspects that should be looked into. if we explore the capability of eda to provide abstraction (2⭐), we can see that, like with p2p, we are bound by several types of coupling: semantic coupling being the most basic and unavoidable, contract coupling, which means being bound by the same data exchange contract afferent, as in for producers, when using queues to communicate many to one, efferent, as in for consumers, when using topics to communicate one to many or many to many, conversation coupling, if the chosen message broker does not allow to be protocol agnostic, this means there are many things to consider when putting this architectural style in a context of a specific implementation and choosing the right broker for a job. these would be the skills available in the organization or on the market, the protocol capabilities of systems that are to be integrated, etc.. then the various degrees of coupling influence the contract resilience (2⭐). if the communication is other than one to one, there is a design consideration that each change in the data model (contract) may be a breaking change, depending on how producers and consumers parse payloads and what is the data format used. this impact needs to be assessed early on, because it will influence every integration. lastly we should take a look into extensibility (2⭐), and here’s where the context of integration vs. distributed systems comes into play leading to a lower score than often expected from eda. this quality seems to be a lot stronger when designing a single domain distributed system, because it is a lot easier to extend the system with new modules or components that work within the same bounded context. when we’re dealing with ecosystem interoperability, while the same extensibility mechanisms apply (patterns to do many to one, one to many and many to many), they work cross-domain, with the cost of using various technologies, different bounded contexts or effort to adapt specific protocols. these cannot be disregarded just as implementation details and need to be considered beforehand. operational analysis moving on to the second group architectural characteristics, that describe the operational qualities of the architectural style. the foremost benefit of eda in this area is its performance (5⭐). some people might wonder why we scored it higher than p2p. there is one key difference here that is worth noting - responsiveness. synchronous communication that is the key element of p2p is always a blocking communication, which means that it impacts responsiveness, because it creates a dynamic coupling between the caller and the provider of the api. as mentioned before, using a message broker splits this quantum into two quanta, which means that the event producer does not rely on the performance of the consuming system(s) and can provide data to a group of consumers in parallel instead of calling them in sequence. this provides a performance boost, freeing up resources as soon as the communication with the broker is done. looking into further characteristics we can see that scalability (4⭐) is the next characteristic that is scored high in the comparison. this architectural style makes it a lot easier architecturally to scale-out your systems, because it does not require additional components for load balancing! using message broker queues enables the use of a competing consumers operational pattern, that by itself provides load balancing capabilities based on resource availability of consumers. depending on the message broker chosen, this may be also available with topics and configuring consumer groups. it is also possible to provide auto-scaling capabilities in the ecosystem based on monitoring of queues or topics of the message broker and the load on those communication structures. another superpower of eda that derives from its simplicity is testability (4⭐). the overall setup is fairly straightforward, as the only added component is a broker that technically facilitates communication and sometimes routing based on metadata, so it does not provide any additional logic (e.g. validation rules, content based routing, that needs to be specifically tested. looking further into operational characteristics, observability (3⭐) and auditability (2⭐) change slightly compared to p2p. while we can observe more within the ecosystem if proper monitoring is implemented to account for the message broker and its communication structures, it does not influence auditability. this is due to the fact that those characteristics still heavily depend on what can be extracted from business systems. the composition of the ecosystems and types of applications within it will have a great impact on what kind of data and metadata describing its operations can be extracted. lastly, what requires additional attention with eda as an architectural style is security (2⭐). while all of the components can be separately secured, the concern is not with those components, but rather the implications of using message brokers. if we want to leverage the use of topics, it is important to understand the consequences of “contract coupling” mentioned before. the consideration is not only around the brittleness of the contract, but also the fact that if we are broadcasting data via topics to several consumers, it means that all of them receive the same scope of data, without consideration of the actual scope used by a particular system. this means that while the usage might be proper and as designed, full scope of data will be most likely at least logged somewhere. this means that the data can be accessed by someone who is not supposed to do so, resulting in a security breach. conclusions event-driven architecture is a very useful architectural style, especially for ecosystems that are required to be highly performant and at the same time remain simple, testable and scalable. while it has its downsides, if properly governed and mitigated, it is a viable architectural option for small to medium companies that rely on real-time interactions within their business processes."
  },
  {
    "title": "Event-Driven One-way point-to-point",
    "raw": "event-driven one-way point-to-point integration patterns as we describe the architectural styles for ecosystems, for some of them we can identify distinct and repeatable architectural patterns that we can leverage to build better interoperability. as we dive into event-driven architecture, let’s take a look at the first of the integration patterns deriving from this architectural style.  pattern nameplate let’s start with a small summary, we dubbed the “pattern nameplate” (an analog to a device nameplate that you can find on any electrical device, that describes its basic characteristics).  name: one-way point-to-point communication mode: asynchronous architectural style: event-driven architecture common use cases:  decoupling from long running or unpredictable time processes - preventing blocking communication and therefore increasing responsiveness by decoupling time-consuming processes or processes that have unstable response time of backend systems from their consumers, competing consumers pattern, load balancing - distributing workload efficiently across multiple consumers for improved throughput and scalability, sequence processing (fifo) - ensuring messages are processed in the order they were received, which is crucial in certain scenarios. this requires a broker that has a fifo capability, queue based load leveling - providing the capability to smooth out the spikes in demand by temporarily storing excess events. architectural coupling:  contract coupling - the provider and consumer of the event are locked by an agreed contract, it is wanted within the bounds of a p2p communication, might become tighter, if it is extended into a broadcast or multicast pattern with additional consumers that only use a subset of the data model, data type and format coupling - the provider and consumer must have the same understanding of the data model types and format (e.g. json, xml, csv) conversation coupling - depending on the broker implementation, the consumer and provider may be locked by the protocol of the event broker, semantic coupling - unavoidable with any data exchange, operational coupling: there are distinct architectural quanta, one for the event producer, the other for event consumer, both overlapping on the event broker structure used (topic or queue).   diagram(s)  one-way point-to-point using a queue  alternative: one-way point-to-point using a topic  alternative: one-way point-to-point using a topic bridged to a queue pattern analysis one-way point-to-point, while fairly common, is not the most used pattern from the eda toolbox. it can be implemented in three ways, where each of those has distinct trade-offs. one-way p2p using a queue the simplest and most obvious form of point-to-point communication, where a queue (jms) or a direct exchange (amqp), both with persistence, is being used to facilitate communication and decouple the consumer from a provider. this essentially means that the consumer does not have to operate in the exact same window as the provider of the event. architectural considerations using this kind of structure enables the competing consumers pattern, enabling easy scaling of the consuming application, without the need for any special configuration. furthermore, the persistence of the queue enables load leveling and decoupling long running processes.  the trade-off of using a queue is that the extensibility on the consumer side is hindered, as only one-to-one and many-to-one communication is available. operational considerations this approach, in most event brokers, is easy to monitor and manage. they provide a lot of insight into the performance of the consuming system, which may prove beneficial in detecting anomalies, peaks or operational incidents, before they impact business. one-way p2p using a topic an uncommon practice for actual point-to-point communication is to facilitate it over a topic (jms) or a fanout exchange to a single subscriber (amqp). it is truly only applicable if the communication is to be extended by additional consumers. it is similar to using a queue, but only if the producer and consumer operate in the same time window, otherwise things get a little bit more complicated. architectural considerations using a topic is perfect to support extensibility, especially when considering a broadcast (fanout exchange) or multicast (topic exchange in amqp), so one-to-many communication. it provides the same level of extensibility as a queue towards multiple providers, so there is no issue in building many-to-one or many-to-many relationships by extending this pattern later on.  the trade-offs for using a topic are quite different. topics by default are not persistent, as they are the true fire-and-forget mechanisms. persistence in the form of a durable topic subscription or durable message (if such is available in a message broker), needs to be deliberately configured per consumer, which means that load leveling is not done by default. furthermore, using topics does not allow the competing consumers pattern by default and again it needs to be deliberately configured using consumer groups (again, if such option is available in the chosen message broker). this brings more complexity to this communication. operational considerations not all message brokers support monitoring a topic per subscriber, so while using a topic for a p2p communication is not an issue in terms of monitoring, it may be problematic if the communication is extended. if a durable subscription is enabled, then insight into the consumer performance, communication anomalies and detecting peaks is also available, as the number of messages awaiting pickup will be easily trackable.  one-way p2p using a topic bridged to a queue a third and more common option is to do p2p communication with a topic that is bridged (routed) to a queue. this option is a default operational option with message brokers implementing amqp (fanout or topic exchange routed to a queue) as their native protocol, although it is also possible with jms implementations, it requires additional configuration. architectural considerations using a topic bridged to a queue enables the benefits of using both structures. publishing the events to the topic enables extensibility for future uses as it is fairly easy to add additional queues to which data can be routed in a broadcast or multicast. additionally, consumers listening on a dedicated queue can utilize the competing consumers pattern by that scale easier.   the key trade-off here is that the communication becomes quite a bit more complex, and requires more attention to details in setup. operational considerations due to the routing there is less emphasis on monitoring the topic, because most information about consumption will be available over the dedicated queues. the downside is that it requires more attention to set up and maintain as more broker structures are involved in the communication.  additional considerations using a message broker as a mediator in communication means that the application producing the event does not have (or at least should not have) any knowledge of who the consumer of that event is. this also means that there is no mechanism within this communication for the producer to know if the event was delivered to the consuming party. while this communication may seem very simple, this implicitly means that the error handling in such communication needs to be handled by a third party that does not participate in it. this ranges from manual handling to automated workflows managing data cleanup and compensating updates, all of which depend on the business process and its requirements. conclusions the one-way point-to-point patterns are a useful thing to have in the integration toolbox that, in combination with other eda patterns as well as broker or api-led architecture, provides an easy and quick way to provide meaningful, real-time data."
  },
  {
    "title": "EDA Broadcast and Multicast",
    "raw": "event-driven broadcast and multicast a common pattern when you dive into the world of event-driven architecture, the phrase you hear most often is “pub-sub” or “publish-subscribe”, which seems to be the most commonly used data distribution pattern in this architecture. and for a lot of cases this will be very true, especially when we'd take a look at mqtt, where the only available structure on the broker is a topic. in essence, this pattern could cover all possible cases for the number of communication participants: one-to-one, one-to-many, many-to-one, and many-to-many. pattern nameplate name: broadcast and multicast communication mode: asynchronous architectural style: event-driven architecture common use cases:  distribution of data object events (create, update, delete), for example new product configuration, or an update done on a customer profile, architectural coupling:  contract coupling - the provider and consumers of the event are locked by an agreed contract schema, while it is wanted within the bounds of a p2p communication, might prove problematic with a broadcast or multicast pattern and multiple consumers that only use a subset of the payload described by the contract. this, if left unmanaged, may lead to potential data security problems and cause contract brittleness, data type and format coupling - the provider and consumers must have the same understanding of the data model types and format (e.g. json, xml, csv) conversation coupling - depending on the broker implementation, the consumer and provider may be locked by the protocol of the event broker, semantic coupling - unavoidable with any data exchange, operational coupling: there are two or more distinct architectural quanta, one for each event producer, and one for each event consumer, both overlapping on the event broker structure used (topic or queue).  diagram(s)  asynchronous broadcast/multicast using a topic  alternative: asynchronous broadcast/multicast using a topic bridged to queues  pattern analysis asynchronous broadcast and multicast are very common patterns, known also as publish-subscribe or pubsub. they are the most used pattern in the eda toolbox and not without a reason.  asynchronous broadcast/multicast using a topic using topics for broadcasting events is a very old pattern originating from jms1.0 (dated 5th of october 1988). while new protocols and analogous structures like exchanges in amqp emerged over time, and the standards developed further, the core idea for this type of asynchronous communication remains unchanged. the main use case for this pattern is to distribute data to many consumers at the same time decoupling them from the producer(s). it can be used like a queue for one-to-one communication, but by default it lacks persistence, which means that the producer(s) and consumers need to operate in the same time frame for the communication to be successful. that can be mitigated by setting up a durable subscription. architectural considerations using a topic (in jms) or a fanout exchange (in amqp) is a good way to support extensibility for data distribution, as it supports the topology of one-to-many and many-to-many communication. topics provide an additional benefit of having a selective subscription, meaning that the communication does not always have to be to all of the subscribers. this can be achieved by utilizing the selector in jms, or topic exchange routing key in amqp with a topic exchange (or other mechanisms like header/system exchanges) for each subscriber based on metadata passed through the message headers.  the trade-offs for using a topic are quite different than with a queue. topics by default do not support persistence, as they are the true fire-and-forget mechanisms. persistence in the form of a durable topic subscription (if such is available in a message broker), needs to be deliberately configured per consumer, which means that load leveling is not done by default. furthermore, using topics does not allow the competing consumers pattern by default and again it needs to be deliberately configured using consumer groups (again, if such option is available in the chosen message broker). this brings more complexity to this communication in the form of configuration that needs to be managed and maintained. operational considerations not all message brokers support monitoring a topic per subscriber, so while using a topic for a p2p communication is not an issue in terms of monitoring, it may be problematic if the communication is extended. if a durable subscription is enabled, then insight into the consumer performance, communication anomalies, and detecting peaks is also available to the extent of the implementation, as the number of messages awaiting pickup will be more easily trackable.  asynchronous broadcast/multicast using a topic bridged to a queue a second, less common option in jms, option for broadcast and multicast is using topics bridged (routed) to queues. this option is a default operational option with message brokers implementing amqp as their native protocol, although it is also possible with jms implementations, it requires additional configuration. architectural considerations using a topic bridged to a queue enables the benefits of using both queues and topics. publishing the events to the topic enables extensibility as it is fairly easy to add additional queues to which data can be routed in a broadcast or multicast. consumers listening on a dedicated queue can utilize the competing consumers pattern and with that enhance scalability and performance of consuming systems. if we wish to utilize the filtering within the subscription, this is done either by configuring a selector over the bridge per queue (jms) or using routing keys with topic exchanges (amqp). some broker technologies enable naming of structures that resembles a rest uri path, enabling filtering based on variables in the names and using wildcards.  the key trade-off here is that the communication becomes quite a bit more complex, and requires more attention to details in setup. luckily most of that can be automated to a degree. operational considerations due to the routing there is less emphasis on monitoring the topic, because most information about consumption will be available over the dedicated queues. the downside might be that it requires more attention to set up and maintain as more broker structures are involved in the communication. a crucial element that might be beneficial with jms servers is that in this scenario topic durables are not needed, so that reinforces maintainability as well as simplicity. additional considerations using a message broker as a mediator in communication means that the application producing the event does not have (or at least should not have) any knowledge of who the consumers of the broadcast event are . this also means that there is no mechanism within this communication for the producer to know if the event was delivered to the consuming parties. this implicitly means that the error handling in such communication needs to be handled by a third party that does not participate in it. this ranges from manual handling to automated workflows managing data cleanup and compensating updates, all of which depend on the business process and its requirements. while this pattern is immensely useful it needs to be thought out with this consideration in mind.  using the broadcast / multicast pattern may damage the security of your ecosystem. it is not due to the fact that topics or exchanges are insecure, but rather as a consequence of using them. while the producer and consumer are contract-coupled, when distributing events over a topic to multiple consumers, they all consume exactly the same message in terms of data scope, format and data model. there is no in transit adaptation (that is part of a different architectural style), so even if the consumer uses only a small subset of the data provided in the event, the full scope is delivered. this means that all consumers need to pay attention to what happens to that data in their system (logging, writing to storage, etc.), because if left unchecked and the security of the application is compromised, then that additional data, that came from the event, may be leaked. conclusions in conclusion, the broadcast and multicast pattern is a fundamental component of event-driven architecture, offering a flexible and scalable approach to data distribution. by leveraging topics or topic-to-queue bridges or proper exchanges, systems can effectively disseminate events to multiple consumers, creating looser coupling between components and enhancing system resilience. however it is worthwhile exploring all of the considerations, as some variants, depending on the implementation of applications or the chosen message broker, might be worse in terms of maintainability or operability of solutions. it is also important to consider the security implications of broadcasting sensitive data. this is something that would require additional careful considerations. by understanding the trade-offs and best practices associated with this pattern, architects can effectively design and implement event-driven systems that are both efficient and secure. and lastly it is crucial to understand that while we’re writing here about topics and exchanges, these are common names used outside of the field of application integration, and are also common in closely related data integration, where they have a bit different behavior and use cases."
  },
  {
    "title": "Event-Driven Callbacks",
    "raw": "event-driven callbacks a spark for discussion callbacks are the asynchronous analogs of the on-demand call. it is as simple and as complex as that. they are used when those synchronous capabilities to retrieve or process data cannot adhere to the requirements of responsiveness and performance. structurally they are a sequential composition of two basic eda patterns, e.g. broadcast and point-to-point or two point-to-point, etc.   callbacks are one of those patterns that might be a bit troublesome to form a philosophical point of view. they are fairly straightforward technically speaking, but it is debatable whether or not they fall under the category of event-driven. the key question to explore here is that if we are sending a message and we are expecting a response, perhaps not an instantaneous one, rather over time or in an undetermined time, is that still event-driven? if we consider the callback alone, without the triggering subscription or event, then it seems to always be an event of sorts, marking the completion of a processing task, an occurrence in a system, etc. let us consider this when exploring this pattern.  pattern nameplate name: callback communication mode: asynchronous architectural style: event-driven architecture common use cases:  data quality assurance based on api contracts, real-time subscribed data delivery as a result of an earlier data subscription, confirmation of event processing, decoupling processing with long or non deterministic processing time, architectural coupling:  contract coupling - the provider and consumer of the event are locked by an agreed data model, it is wanted within the bounds of a p2p communication, might not be, if it is extended into a broadcast or multicast pattern with additional consumers that only use a subset of the data model. furthermore, there are at least two contract couplings present in this pattern, depending on the variant (e.g. if dedicated callback queues are used) data type and format coupling - the provider and consumer must have the same understanding of the data model types and format (e.g. json, xml, csv) conversation coupling - depending on the broker implementation, the consumer and provider may be locked by the protocol of the event broker, semantic coupling - unavoidable with any data exchange, operational coupling: there are two or more distinct architectural quanta, one for each event producer, one for each event consumer, both overlapping on the event broker structure used (topic or queue), temporal coupling - for a callback to occur successfully, there must be first a successful event/command/message that results in said callback, related to: one-way point-to-point, broadcast and multicast, diagrams pattern diagrams  simple callback   callback to a broadcast with a single contract queue  callback to a broadcast with dedicated queues behavioral diagram  pattern analysis callback patterns at times seem like not the most common patterns in interoperability on the ecosystem level, and sometimes their limited or common use can be considered industry specific. in either case, they may prove extremely useful, both from the real-time data delivery and real-time operations points of view. they are the asynchronous equivalent of an on-demand call in some use cases, but can also serve several operational functions, e.g. delivery confirmation, statuses. it is also important to note that while using synchronous communication it is implied that there will be a response, issuing a callback is optional and left to the discretion of the callback producer and based on the business process. simple callback the simplest form of a callback is an exchange between two systems, where the callback producer first receives a message, state, event or a command from the callback consumer that triggers processing and the creation of a callback event. this is a fairly easy way to decouple two systems, by providing a separation in the form of a message broker between them.  architectural considerations while any messaging structure can be used to implement this pattern, let’s consider implementing it over queues only. this will be a dedicated flow, tailored to the specific exchange need. it has limited extensibility as it may be turned into a service fronted with queues as its api implementation, changing the topology from one-to-one-to-one to many-to-one-to-one. this means that there may be several callback consumers that produced a trigger for the producer to create a callback. in that case additional configuration needs to happen over the callback queue to enable proper filtering based on metadata (e.g. message headers, correlators, topic/queue taxonomy) so that the right consumer receives the right response. it is similar to a one-way point-to-point over a queue, because this is this exact pattern used twice in sequence.  operational considerations utilizing this pattern will provide easy scalability if needed for each party consuming messages from them, especially if there might be multiple threads producing callbacks to various triggers coming in from the callback consumer. this pattern, while may have varied performance due to the communication characteristics, processing time of the callback producer, which involves domain complexity, it is useful for supporting responsiveness of the callback consumer as the process is not confined to a single thread that becomes blocked awaiting the callback.  callback to a broadcast with a single contract queue a different callback pattern, following one-to-many-to-one topology, that has more of an operational use, chaining a broadcast/multicast and an one-way point-to-point patterns in sequence. one of the key use cases for this pattern is mitigating the operational lack of delivery or processing confirmation, which makes error handling and it operations a bit more difficult. this might at times be crucial for specific business areas, e.g. product configuration, where the business needs to be sure that all sales channels and order processing have the same product definitions (e.g. bundles, promotions). in that case a lack of callback to the data master confirming successful processing or a callback signalling processing errors will give more operational agility. this unfortunately comes at a trade-off that the data master must be aware of all consumers that need to send an operational callback. these callback messages could be considered to be events, but might as well be state. architectural considerations given the broadcast nature of a callback trigger, this pattern will support extensibility. with a callback originating from multiple producers to a single queue the key consideration is that all of those producers are bound to a single contract managed by the callback consumer. this needs to be considered when designing, as this contract coupling may be a problem with high volatility of requirements, as any change to the scope of data will impact all of the communication participants.  operational considerations from an operational perspective this kind of callback using the queue topology of many-to-one might be troublesome to monitor, as there are many applications writing to a single queue. that would require more effort from the operations teams in terms of monitoring the message broker for which producers actually wrote to the queue. callback to a broadcast with dedicated queues callback to a broadcast with dedicated queues is probably the rarest of the callback patterns. not only does it require the most effort to set up, but also facilitates more refined and deliberate processes. this key use case is that all the callback producers react to an event published and that triggers their processing resulting with a callback that is specific to what the functionality of each of those event processors is. if each processor delivers different data as a callback to a trigger in this scenario, this means that each one will have a different schema for said data, to properly separate objects and avoid contract brittleness. this also means that ownership of the schema and each callback queue lies within the scope of the respective callback producer as they are governing the data transferred. architectural considerations first thing that comes to mind is that this setup is complex, and while it may seem simple for each callback producer, it is a lot more complicated for the consumer of all those callbacks as now that application needs to implement processing logic to consume multiple endpoints as well as various contracts. additionally if we’d  consider replacing the dedicated queue with a topic or topic bridged to a queue (changing the topology from one-to-many-to-one, to one-to-many-to-many), we can create extensibility and reusability of the callback produced. this later can be used in additional contexts for whichever application this data might bear relevance. this in turn contributes to even better real-time communication within the ecosystem. operational considerations turning again to the operational perspective, this variation of the callback pattern will be easier to maintain and operate as there is a clear separation for each callback producer, so it will be easier to spot if for some reason there is a communication breakdown. otherwise, operationally they do not differ from one-way point-to-point or broadcast/multicast patterns, which are the basis for this communication. conclusions callback is a useful pattern that enables the organization and the data transfers between applications to be more agile and run in real-time, with the business applications acting on events as they happen rather than over time polled with a delay. while it may present some additional complexity and operational challenges, the benefit outweighs the cost.   additionally, here we can see that it is not straightforward whether all messages we send are actual events. it is worth considering what is the process as a whole as in some specific cases we will be dealing with commands and states that will be triggering the event-driven behaviour."
  },
  {
    "title": "Event-Driven Waiter Pattern",
    "raw": "event-driven waiter pattern processing, please do not wait... as we dive deeper into event-driven architecture (eda), there is a specific scenario that deserves attention on its own. as we already established that eda patterns can be used to loosen the coupling between different applications and systems so they can operate in a more independent manner. the waiter pattern builds on top of that capability allowing the applications that produce data over a long span of time to deliver them in an organized and standardized way.  pattern nameplate name: waiter pattern communication mode: asynchronous architectural style: event-driven architecture common use cases:  real-time requested data delivery in chunks as they are produced as a result of an earlier command or event, delivery of larger amount of data in parts, parallel complex processing chunked based on a single event/command. architectural coupling:  contract coupling - the provider and consumer of the event are locked by an agreed data model, it is tolerable within the bounds of a p2p communication, might not be, if it is extended into a broadcast or multicast pattern with additional consumers that only use a subset of the data model. furthermore, there are at least two contract couplings present in this pattern, depending on the number of callback producers, data type and format coupling - the provider and consumer must have the same understanding of the data model types and format (e.g. json, xml, csv) conversation coupling - depending on the broker implementation, the consumer and provider may be locked by the protocol of the event broker, semantic coupling - unavoidable with any data exchange, operational coupling: there are two or more distinct architectural quanta, one for each event producer, one for each event consumer, both overlapping on the event broker structure used (topic or queue). temporal coupling - for a subscription callback to occur successfully, there must be first a successful event/command/message that results in said callback. related to: callback pattern diagram(s) behavioral diagrams  waiter pattern behaviour pattern analysis the core function of the waiter pattern is to provide value with produced data as soon as possible. depending on the business scenario, and in this case quite a specific one, this data will be delivered at least once, meaning that this pattern behaves like a callback in its simplest form. the real value of it is in the more advanced scenario, where the callback producer’s processing time is very long - minutes, up to hours, but the callback data can be partitioned into usable chunks and sent as separate callbacks. this might seem like a subscription to events like webhooks, but there is one key difference - the processing and publication of callbacks is temporally coupled to the requesting party. without the initial trigger message (command or event), no data will be produced.   an example of such a mechanism would be a callback function for a web crawler, which is charged with crawling through news sites in search of specific keywords, like news about companies. usually these news are copied from site to site with very little addition, so the job of such crawlers is to identify a theme of an article and aggregate all news on the same theme, then deliver a summary and sources. if you span that across a time frame of for example two years, that is a lot of data to cover and it will take a lot of time. packaging each theme into a single deliverable callback brings value faster to the business as their actions are not blocked entirely by waiting for the processing to finish. and sometimes partial results might be sufficient to finish work, such as risk assessment. architectural considerations given the scope of use cases for the waiter pattern, essentially all callback patterns are viable, as they largely depend on the number of event processors producing callbacks and whether or not they all produce data in the same data model. in that sense architectural considerations are the same as described for callbacks.  what can be an additional consideration to be had, is that since the number of callbacks will be usually more than one per event triggering the processing, this requires mindful handling of persistence and errors, especially when running parallel jobs. as there might be a connectivity problem (as per the first fallacy of distributed computing), the number of messages pending being sent from the callback producer or delivered and temporarily stored on the message broker, will gradually increase. this in turn might pose a risk of overwhelming the message broker and disabling the event based communication for all involved, making it important to be managed on an architectural level first. operational considerations turning to operational aspects of the waiter pattern, other than the aforementioned inheritance from callbacks, communication metadata might play a more important role. correlation identifiers may be especially useful, not only to track and correlate log data between various instances of callbacks from a single processing job, but also to provide robust statistics and reporting on services and data usage. this is especially crucial if the callbacks are turned into broadcasts enabling other consumers, than the original requestor, to subscribe to that data, as this may provide an easy way to track all consumption. conclusions waiter pattern as a specific case of a callback pattern is a very useful tool in an integration architects toolbelt as it provides additional capabilities to balance long processing times clashing with low time to market, that seems to be a very common business driver of the information age. it enhances the agility of data consumers to provide value to the end user in a timely manner.  however it is important to consider the error handling, persistence and operational aspects of this pattern, especially in high volume scenarios, as, if not addressed, they may lead to critical failures, severely impacting communication. this needs to be defined and implemented in cohesion with the business process, as these things will be case specific. luckily if managed properly, this pattern can provide significant value to any organization."
  },
  {
    "title": "Broker Architecture",
    "raw": "when complexity calls there comes a certain point in an ecosystem's growth, when due to its expansion and business growth, operating and maintaining point-to-point, or event-driven interoperability becomes a nightmare of tangled dependencies for each system involved in the communication. all of this is the result of complexity that is not only specific to the business domain, which dictates functional requirements for each of those systems, but also due to the communication overhead that forces the systems to manage connections, various api contracts, data access logic, or credentials. all of this builds up to a hefty load of work, shifting the balance of time consumed on operations and development. this can further lead to bottlenecks in project development or complete stop of further growth as the team will be busy with solving production issues or locked in a fight between development and operations. this is where we can find help by looking into a law postulated in the mid-1980s.  “every application must have an inherent amount of irreducible complexity. the only question is who will have to deal with it.” law of conversion of complexity (tesler’s law)  while larry tesler was mostly concerned over the interactions between the software and its users, that is why his law is mostly known in the ux discipline, there is a very important lesson we can learn from it in terms of interoperability as interaction between two different systems. we can move most of the communication complexity away from the domain applications. it will not be reduced, but managed elsewhere.  “complexity is not bad. it’s confusion that’s bad. forget about simplicity; long live well-managed complexity.”  don norman, author of the design of everyday things.  complexity managed by mediation if we assume that complexity can only be moved, not reduced, then we can certainly move the complexity of communication and reduce it to the bare minimum for all participants. the easiest way to do so is to introduce a broker (a.k.a. a mediator).  broker 1\t: someone who acts as an intermediary [...] 3\t: someone who sells or distributes something  mediator 1\ta : one that mediates \tespecially : one that mediates between parties at variance [...]  if we look at those definitions from a perspective of it ecosystems, an integration broker in a broker architecture acts as an intermediary between two parties that have differing requirements. the key word here is ‘differing’, as this is where the complexity hides. communication is fairly simple if everyone speaks the same language and has the exact same definition of every single word. this translates to applications being developed in the same coding languages, having data models, represented by data in a specific format like json or xml and lastly managing the semantic coupling on the level of data. unfortunately this kind of situation is rarely the case in the modern it landscape, and this is where an integration broker and broker architecture come into play. what is broker architecture broker architecture is an ecosystem architectural style that introduces an infrastructural component, the integration broker, into the it ecosystem, that is responsible for handling communication complexity, that is:  systems using different communication protocols,  various contract data models,  mismatched data formats,  orchestration, observability, abstraction or extensibility capabilities. it inherits certain qualities from the previous architectural styles and builds new ones on top of those already existing. the use of point-to-point communication now shifts from system calling each other to invoking services exposed by the integration broker that orchestrates communication. on top of that we gain new capabilities, such as the ability to initiate the communication completely externally of the business systems by means of schedulers or change data capture mechanisms. lastly the integration flows in an integration broker have a workflow nature that is usually tailored to the business process requirements which it needs to support. qualitative analysis as we did with point-to-point and event-driven architecture, let’s explore the qualities of broker architecture and look into a few pitfalls that can be crucial to consider when trying to apply this architectural style. for that we will be using a comparison table that was produced through a qualitative analysis of architectural styles taking several architectural characteristics into account. if you would like to learn more about this analysis or read how we define those characteristics, you can do so by reading this article, where we explain how this comparison was created.  architectural characteristic name point-to-point spaghetti architecture event-driven architecture broker (mediator topology) api-led architecture development cost \uD83D\uDCB2 \uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 operational cost \uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 architectural changes cost \uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2 abstraction ⭐ ⭐ ⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐⭐ contract resilience ⭐ ⭐ ⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ simplicity ⭐⭐⭐⭐⭐ ⭐ ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐ composability ⭐ ⭐ ⭐⭐⭐ ⭐⭐ ⭐⭐⭐⭐⭐ extensibility ⭐ ⭐ ⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ testability ⭐⭐⭐⭐⭐ ⭐ ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐ scalability ⭐⭐ ⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ performance ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐ security ⭐⭐⭐ ⭐ ⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ observability ⭐⭐ ⭐ ⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐⭐ auditability ⭐⭐ ⭐ ⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐  cost analysis considering how broker architecture changes the landscape in comparison to point-to-point or event-driven architecture, it is clear that the cost overall will be higher in comparison. as we stated at the beginning of this article, we are moving complexity away from the domain systems, which means, we are moving development and operational cost to an integration broker. this comes with a need to hire or retrain developers to have the right skill onboard. next to that, the business systems development team can work protocol agnostic, as the integration broker is capable of providing a protocol that is best suited to the business system needs and technology used to build it. this limits the sprawl of development costs to an extent and lands broker architecture at 2\uD83D\uDCB2.    operational costs in broker architecture are similar to eda (3\uD83D\uDCB2), mostly due to the effect of moved complexity. there is the cost of licensing, which can influence the overall cost as it will vary between different technology providers and their licensing models that depend on several factors (e.g. usage, deployment mode). the operational effort on environments and operating the integration broker itself can be partially compensated within the technology licenses if an ipaas technology is chosen.   architectural change cost is a bit higher for the broker architecture (3\uD83D\uDCB2) due to the fact that it is a completely separate system with a distinct workflow nature. all integration flows are dedicated, which means they are more coupled to the business systems and do not provide as much abstraction as is possible. this means that while changes might be entirely contained within the integration broker, they will impact the whole integration flow. the changed business system needs to be deployed with it as a single deployment unit. architectural and design time analysis moving on to architectural and design time qualities of broker architecture, we can see from the qualitative analysis that it is a well-rounded architecture having average scores in all characteristics in this category. let’s start with the one that scores the lowest - composability. the reason why it is scored with only 2⭐ is because of the integration broker workflow nature. the reusability in each integration flow is mostly limited to code, e.g. reusable wrappers, connectors, perhaps some standardized mapping functions or transformations, but it is rarely feasible to create reusable services, hence it is harder to use business systems and integration flows as composable building blocks of the ecosystem. removability, as part of composability, is a bit easier to achieve, given that all changes are encapsulated in the integration platform, but it has an operational impact on all integration flows and systems related to the system that is replaced, triggering full regression tests with each change. it is the downside of moving the communication complexity from those systems into the integration broker.   moving on to simplicity (3⭐), it is quite obvious that the landscape overall became a bit more complex, giving broker architecture a one point lower score than it was with eda. it is worth noting that this complexity is only superficial, as introducing a new system does not have to translate directly to higher complexity, as it is usually the number of interactions (operational coupling) that is the better measure. the key is the scale as broker architecture will be used in more complex environments than eda and point-to-point, so it aligns with the growth of the business complexity partially moved to the integration broker. the simplicity of this architectural style supports time to market very well, as usually the integration flows are easier to develop with the right technology and a dedicated team of developers that specialize in such tasks.   the element that sets this architectural style apart from the previous styles is the capability to provide abstraction (3⭐) of business systems contracts, data models, data access logic, credentials, etc., which, if used properly, can help lessen the workload on development teams, so they can focus on functional requirements and not try to solve communication riddles. this is directly tied to contract resilience (3⭐) that is a lot easier to achieve as a lot of the changes to contracts with downstream systems can be encapsulated within the broker and will not impact the upstream applications. lastly, with the integration brokers enabling protocol agnostic communication, there is more support for extensibility (3⭐), as it is a lot easier to add new services and systems to the ecosystem if there is less adjustment needed from them to be integrated.  operational analysis let’s now take a look at the operational characteristics of the broker architecture, where things are a little more varied. starting with testability (3⭐), we can see that it is lower proportionally to simplicity. introducing a new system that facilitates communication makes testing a bit more difficult. yet, due to the fact that all orchestration, transformation and data access logic is contained within a single integration flow per use case, the effort increase needed to test communication end-to-end is not that significant, which means that there is a fairly low impact on time to market.  moving on to characteristics describing how this architectural style operates under load of messages it will be less performant than point-to-point or eda, due to the fact that we are introducing latency in a form of an integration broker, which has its logic to execute, the overall performance (3⭐) of the ecosystem will be impacted, which is a trade-off compared to the previous architectural styles. how severe this impact will differ between implementations is based on a number of factors, like, but not limited to:  deployment mode - cloud vs on premise,  runtime operations - self-hosted runtime vs managed services, chosen technology - is it chosen to facilitate the right needs, like on-demand, event communication or batch transfers, integration flow complexity - simple p2p flows, simple orchestration or complex bpm like orchestrations (which we do not advise), the end result might noticeably differ from a qualitative analysis result as some of the factors may be improved in various ways. which leads us to a very similar situation with scalability (4⭐), as it will also be dependent on similar factors. luckily since most modern integration brokers are built in a microservices architecture or as serverless functions, they have robust scalability features, including automatic horizontal scaling if needed. this, combined with good load balancing capabilities, enables the integration flows to be separately scalable, providing availability that easily matches requirements of all systems involved in the communication.   looking further into operational characteristics we find observability (3⭐) which does not really differ from eda in regards to score, but with broker architecture it will provide a completely different set of observable data and metadata. since nearly all data is supposed to pass through the integration broker it is a great place to gather intelligence as to where data is used, as well as how often it is requested or distributed, and what is the performance of business systems in terms of interoperability. the only downside is that since each integration flow is a dedicated workflow, that metadata per system will be distributed among many flows and needs to be aggregated and classified before it can be used. tying it to auditability (4⭐), when observability is properly managed, the integration broker becomes a valuable source of information about the ecosystem and often a source of truth on general it operations and the consumption of data. these are very important aspects when there is a need for proof for root cause analysis (rca) or audits. if combined with managed observability from business systems, aggregated to a single logging and monitoring, or an analytics platform, it can give a very wide overview of all processes. lastly we have security (4⭐), which is scored considerably higher in the qualitative analysis. the sole fact that a mediator is used, limiting the access to other systems from any business application, is a big step towards securing the ecosystem. if a system and its data are compromised, that limits the access to that particular system, without giving any footholds to other systems that are bound with the breached one by communications. combined with observability and real-time traffic and metadata analytics, broker architecture can enable anomaly detection, helping to automate and boost the speed of reaction to potential security breaches. if a breach is identified, the compromised system can be isolated swiftly by stopping specific integration flows that are inbound or outbound to that system, further limiting the damage.  conclusions tesler's law highlights the inevitable presence of complexity in any software system. broker architecture offers a pragmatic approach to managing this complexity by strategically shifting it away from individual applications and towards a dedicated integration layer. by centralizing communication logic, data transformation, and protocol handling within the broker, this approach reduces the cognitive load on application developers and simplifies the development and maintenance of individual systems. this not only enhances developer productivity but also improves business systems maintainability, reduces the risk of errors, and ultimately enables organizations to adapt more effectively to the ever-changing demands of the modern business landscape. while the introduction of a broker introduces a new layer of complexity, it is a managed complexity, allowing for better control, observability, and scalability of the overall system."
  },
  {
    "title": "Pattern of an antipattern",
    "raw": "in the sea of patterns working in it we hear a lot of jargon, phrases that have a specific meaning often only in this industry. while those terms are fairly common, and we hear them a lot, everyone has their own definitions as to what they actually mean. a large part of the role of an it specialist, an architect especially, is to clarify those definitions and foster ubiquitous language in conversations within it and when talking with any stakeholders.  one of such words that constantly escapes a proper definition in common use is an antipattern. we hear it all the time in meetings, discussions over code and changes made to systems. most commonly it is used as a description of something that should not be done that particular way, meaning it’s been done badly.  an uncharted island the word antipattern in all of its various spellings is not found in common language as it is limited to software engineering, project management, or business processes.  andrew koenig, who coined the term ‘antipattern’ back in 1995 in the “journal of object-oriented programming, vol 8”, defined the the term as:  an antipattern is just like a pattern, except that instead of a solution it gives something that looks superficially like a solution but isn’t one.  a lot has changed since 1995, so the definition also shifted a bit here and there:  an antipattern is a technique that is intended to solve a problem but that often leads to other problems. an antipattern is practiced widely in different ways, but with a thread of commonality.  - sql antipatterns volume 1 bill karwin antipatterns are common approaches to recurring problems that ultimately prove to be ineffective.  - rails antipatterns: best practice ruby on rails refactoring chad pytel, tammer saleh an antipattern is a repeatable process that produces negative results.  - fundamentals of software architecture, 2nd edition neal ford, mark richards the quest for the treasure all of those definitions above, despite being slightly different, have a common theme: the antipattern is not the solution to the problem or might lead to additional problems that greatly outweigh the benefits of the solution. someone might generalize, and we believe many do, that antipatterns are simply bad and you should avoid them.  like all things that began to have a life of its own, antipatterns have a grain of truth in them, as they must have been used somewhere and worked benefiting the overall solution at a certain point. otherwise they would not have the characteristics of commonality attached to them in all of the definitions. to understand the nature of antipatterns it is crucial to understand their context and how they came to be and there might be several various scenarios for their origins:  outdated gold - a common solution or pattern from a different time, as there are still a lot of solutions or ideas circulating in the industry that were born and implemented a long time before the first microservices and large scale distributed systems. trying to apply some of those solutions, e.g. trying to create a file cache, might prove highly ineffective in distributed solutions, while it worked perfectly fine with monolithic systems hosted on bare metal. there might be different and better ways to do things or the business drivers that created them already shifted too far for the original idea, for them to support the business in a meaningful way, it worked so it must be universal (false equivalence fallacy) - there are cases where a specific solution worked for a specific use case and it was known to be of great benefit to the business. there are bound to be some that will try to replicate the exact or very similar solution in the context of their own company without first building an understanding of why that solution worked in the first place and what was the context of the original solution's use and creation. this can lead to completely mismatched solutions, implemented just because someone mistakenly thought that they’re solving the same problem, we have always done it this way (status quo bias) - a tendency to preserve the ways of working, used patterns, if not from fear of change and loss, then from rigidness of the change process and the overwhelming effort needed to produce a stable change (e.g. acceptance processes, board reviews, sign-offs, security assessments). as deviating from the norm is risky, this often results in inaction, leaving us with patterns that may be outdated and inadequate. additionally, there is the aspect of feeling overwhelmed by the number or complexity of available options, opting towards the status quo helps to avoid the stress of decision making. others are already doing it! (bandwagon effect) - a phenomenon where a certain approach, behavior, technology is adopted simply because others are doing so. it is even greater if the solution is presented by a well known company that is considered to be some sort of an industry leader (e.g. netflix, google, salesforce), where the presentation often has no goal of an actual knowledge transfer, but more of a marketing aspect. by that all trade-offs, operational and organizational problems, costs are hidden behind the story tailored to bring attention and revenue to the company. all of these villain origin stories have one single action that was missing - a trade-off analysis, that would check if those solutions actually match the requirements and context of the ecosystem they are to be implemented in. but that also leads to another important conclusion showing the nature of an antipattern - it is still a pattern and it will surely be applicable in a specific, probably niche, scenario! the assumption that it is always bad is simply false. as with everything: it depends.  ‘x’ that marks the spot if we consider all of the above we can actually refine the definition of an antipattern. from our perspective it should look something like this:  an antipattern is a common solution to a problem that is not the right fit in a specified context or a solution that might lead to additional problems that greatly outweigh its benefits.  let's delve deeper into this definition and explore all the clues on the map of patterns, exploring the implications:  common solution: antipatterns aren't difficult to comprehend or unusual; they're often widely recognized and readily adopted solutions. this popularity can stem from their initial effectiveness in certain scenarios, their simplicity, or their promotion by influential figures or companies. problem-solution mismatch: the core issue with antipatterns lies in their incorrect use. a solution that works well in one context can be disastrous in another. this mismatch can arise from differing requirements, constraints, or unforeseen circumstances. contextual sensitivity: the effectiveness of a solution is highly dependent on the context in which it's applied. factors such as the problem domain, the available resources, the organizational culture, and the time constraints can all influence the suitability of the solution. hidden costs: antipatterns often come with hidden costs that aren't immediately apparent. these costs can include increased complexity, reduced maintainability, performance degradation, or unintended consequences that ripple through the ecosystem. benefit-cost imbalance: even when a solution provides some benefits, it can still be considered an antipattern if the associated various costs significantly outweigh those benefits. this highlights the importance of considering the long-term implications of a solution. conclusions the concept of antipatterns, while often misunderstood, plays a crucial role in the ever-evolving landscape of software engineering and it architecture. by recognizing the common origins of antipatterns and understanding their contextual nature, we can navigate the complexities of solution design and implementation more effectively. the key lies in striking a balance between leveraging established patterns and adapting them to the specific requirements and constraints of each unique scenario. ultimately, by fostering a culture of critical thinking and informed decision-making, where solutions are not blindly adopted but thoughtfully evaluated and tailored to achieve optimal outcomes, we can avoid costly and problematic patterns which we would later name antipatterns."
  },
  {
    "title": "Event-Driven Polling",
    "raw": "event-driven polling not enough context there are many controversial patterns in the world of it architecture, some are even considered antipatterns, which by most people are flagged as “avoid at all cost”, or “always bad”. so like with any other architectural style, event-driven architecture (eda) also has a pattern that brings controversy to the table.  while initially this article was supposed to be named an antipattern, we decided not to name it so, due to the fact that this is still a valid pattern, but one that is difficult to place in the right context where it should be used. to cover that problem area we wrote a separate article that provides a bit more explanation on the matter.  instead of discussing how problematic this pattern might be we decided to present it to you as any pattern and provide the right context to understand it and be able to use it with success. pattern nameplate name: polling / anemic events communication mode: hybrid architectural style: event-driven architecture common use cases:  distribution of data object events (create, update, delete) to a large number of subscribers with a varied scope of payload of the same object (e.g. regulatory variations per territory), strict, centrally governed security control of access to data, architectural coupling:  contract coupling - the event producer and the event consumers are not only coupled contractually by the definition and granularity of the event, but are also coupled by the contract of the api used for polling the payload for each event,  data type and format coupling - the provider and consumer must have the same understanding of the data model types and format (e.g. json, xml, csv) conversation coupling - depending on the broker implementation, the consumer and provider may be locked by the protocol of the event broker, semantic coupling - unavoidable with any data exchange, operational coupling: temporal coupling - for the data to be actually transferred over a point-to-point api call, an event providing a record identifier must occur first and be successfully delivered to the consumer system. related to: broadcast and multicast pattern, diagram(s) pattern diagrams  base distribution of an anemic event  payload polling  behavioral diagram  polling behaviour pattern analysis the event-driven polling pattern, also known as the \"anemic events\" pattern, presents a unique approach to data distribution within an eda. unlike a typical broadcast or multicast pattern, this method employs a two-step process. first, an event is published using a broadcast pattern, containing only an identifier, and sometimes minimal metadata (e.g. schema identifier for parts of larger data objects). second, consumers must make an explicit, on-demand api call to retrieve the full data payload. this characteristic defines the core behavior of the pattern and significantly impacts its applicability.  this two-step nature leads to potential inefficiencies. scheduled polling is noted to be 98% ineffective (returning no results), and while event-driven polling reduces this significantly, there are still cases where consumers retrieve data that is irrelevant to them. this occurs when changes are trivial or when a consumer does not require the specific data that has been updated. despite these inefficiencies, the pattern offers valuable benefits, particularly in scenarios where security and controlled access to data are paramount. architectural considerations when considering the event-driven polling pattern it is important to note that it is more complex compared to other eda patterns due to the required two-step interaction. this comes with certain trade-offs. moving the payload delivery mechanism to an on-demand api call, introduces a level of abstraction, that is the key benefit of using this pattern. the abstraction aspect enhances the control over accessing data, where depending on the needs, each consumer can have a varied response to the anemic event. this in turn enhances security, as each api call can be authenticated and authorized. for example, using graphql or a rest api with credential-based data access allows for granular control over which systems can access specific data attributes. this pattern, like the related broadcast and multicast patterns, is quite extensible, as new consumers can be added relatively easily, with the overhead of an on-demand api call.  however, this introduces a complexity shift, where it is the responsibility of the consumers to implement data access logic needed to retrieve and process the payloads. this means that we are multiplying the number of implementations of said logic over several systems. furthermore, if we introduce control mechanisms on the producer side, that narrow the scope of data, we introduce a tighter coupling between the producer and the consumers, as the producer now needs more knowledge about each of the consuming systems (configurable by design or acquired in runtime). the potential extensibility of this pattern adds to the complexity equation, as each new consumer will introduce that repeated implementation as well as at least new configuration on the provider side.  considering data models and contracts, while the anemic event contract itself is resilient, the on-demand api contract is more brittle, as changes to the api will most likely affect all consumers. adding a new consumer might introduce a requirement to change the api to accommodate business needs, that in turn might impact all other consumers.  operational considerations operationally, the event-driven polling pattern introduces some challenges as a consequence of adopting this pattern.  testability is complicated by the two-step process, requiring an extended test coverage for event production, delivery, payload retrieval, and end-to-end integration. taking into account the possibility of adjusting the payload scope per customer, that makes the scope of test cases quite large and complex, with many variants, while the test cases for a broadcast or multicast are a lot simpler.  performance is also affected due to the introduction of additional latency between event publication and the actual payload retrieval. this two-step process inherently limits performance compared to simpler event-driven patterns. furthermore, whenever communication requires more steps to finalize the transfer of data, that introduces additional choke points or points of failure that can have a severe negative impact. it is in the responsibility of the event consumers to properly and timely handle processing of the anemic event, so in essence, react without delays. additionally, content filtering per consumer will also impact the performance of the provider, as querying data and applying filtration, that is based on custom logic, will increase the time needed to produce the response. with many queries done concurrently, this may have a noticeable impact on the producer system and the api behavior.  another thing to consider from an operational point of view is the aspect of temporal coupling that may influence processing, especially if it is required for the events to be processed in a sequential manner. this kind of scenario will need to be carefully addressed through design per case as to what would be the behaviour if the data changes before the previous event was polled.  lastly, when it comes to downsides of event-driven polling, observability and auditability become more challenging due to the added complexity, but with a centralized data master issuing events and managing payload access, these can be somewhat mitigated. the key aspect is to implement sufficient observability over the consumer side, to ensure proper processing of events and later their payload, which is especially important when dealing with sensitive or restricted data.  on the other hand, security, especially the safety of data, is improved through controlled access to payload data, which can be crucial for regulatory compliance, such as gdpr. it is especially beneficial when the sheer number of varying consumers completely disables the capability to provide usable and safe payload over a broadcast or multicast. conclusions the event-driven polling pattern, while complex, is a valid and useful pattern for niche applications. its primary strengths lie in enhanced security and controlled data distribution, making it suitable for scenarios where different consumers require varying attributes from the same data object. in such cases, the benefits of controlled access and simplified producer logic outweigh the increased complexity for consumers and potential inefficiencies. while it can be misused or applied without good reason, leading to various challenges, it is not an anti-pattern in itself. when used appropriately, it is a valuable tool for architects and developers in specific contexts."
  },
  {
    "title": "EDA Content-based filtering",
    "raw": "event-driven content-based routing sorting through the messages when working with event-driven architecture on an ecosystem level, efficient message routing is an essential feature. typically, routing decisions are based on metadata included with the message, so a structure like a jms header, routing key in amqp0.9 or variables in topic name in amqp1.0. but what if the producer does not provide these metadata? the common sense is to go to that particular system and make changes to it so that it supplements the communication with additional information. well then, what if that is not possible? this might lead to a number of communication inefficiencies and consumers processing data they don’t need to process.  to address this, content-based routing could be used, where a dedicated application parses the message payload to determine sufficient metadata for routing. while this approach can solve immediate problems, it often introduces significant complexities and potential pitfalls. pattern nameplate name: content-based routing communication mode: asynchronous architectural style: event-driven architecture common use cases:  providing routing metadata, when an event producer is incapable of producing them, refinement of coarse grained event for redistribution (create, update, delete), for example new product configuration, or an update done on a customer profile, architectural coupling:  contract coupling - the provider, router and the consumers of the event are locked by an agreed contract, while it is wanted within the bounds of a p2p communication, might prove problematic with a broadcast or multicast pattern and multiple consumers that only use a subset of the payload described by the contract. this, if left unmanaged, may lead to potential data security problems and cause contract brittleness, data type and format coupling - the provider and the router, as well as the router and the consumer must have the same understanding of the data model types and format (e.g. json, xml, csv). conversation coupling - depending on the broker implementation, the consumer and provider may be locked by the protocol of the event broker. furthermore, the router creates additional coupling in this regard as all consumers will now be dependent on the metadata provided by the router to the data exchange, semantic coupling - unavoidable with any data exchange, operational coupling: semantic coupling - each payload is parsed by the router to determine routing metadata as needed, so each instance of the payload will influence how the communication will proceed. temporal coupling - depending on how the router is built and if it supports parallel processing, this may introduce additional problems, like race conditions between events of the same type, but also consumers will now depend on the validity of the routing logic introduced in this component, related to: broadcast and multicast pattern, diagram(s)  content-based routing pattern pattern analysis content-based routing is a solution that is most likely classified as a workaround, rather than a permanent fix, and might be highly influenced by business needs, which drive to solve problems quickly, often without considering implications. let’s take a closer look at the situation and what it means to implement this pattern.   initially the producer exposes data over a topic, to which there are several subscribers and all of them receive the same set of data, as described by the broadcast pattern. a new need is introduced to change how data is distributed to be more selective. unfortunately, in this case, there is no metadata available for routing. this makes the task at hand impossible within the existing setup, as native message broker capabilities cannot be used to provide routing. there might be a number of reasons why there is no metadata, e.g. a legacy system that is not modifiable anymore due to lack of knowledge or skills in the organization, a third party application that we have no control over or simply the cost and effort needed to do so is too high.  this results in a situation where messages are broadcast to all subscribers, regardless of their actual interest in the content. introducing a content-based router addresses this need. this application intercepts messages, analyzes their payload, produces metadata based on that payload to finally republish the message to another topic for redistribution. the message broker routes them to the appropriate destinations based on the metadata created from the message content, hence the name: content-based router.  while this approach may seem straightforward, it introduces several challenges. the routing application, often treated as mere infrastructure, tends to be neglected. this in turn leads to poor maintenance, insufficient resource allocation, and the potential for it to become a performance bottleneck. additionally, business logic is often introduced into this application, further complicating its operation and increasing the risk of errors and failures blocking the communication. while by default the router is stateless, and metadata is created only from the current payload contents. if business logic is introduced, that often brings some sort of state to this application (e.g. dictionary reference table for translating values, payload cache), that complicates the process and makes maintenance even harder as well as the application itself becomes a higher risk factor in communication. architectural considerations the content-based routing pattern may have a significant impact on the structure of the solution. here are some key considerations:  we are introducing a new component with a high degree of coupling. the router becomes a central point of dependency, tightly coupled to the producer. this can lead to a brittle system, where changes in the producer system might lead to a communication stop. the content router must understand the message content to route effectively. let’s consider a scenario where finally the legacy system is being replaced, but we’re aiming at a quick win, so all communication channels remain the same. small adjustments are made to the data model of the payload on the consumer sides for the communication to work. and it does not, because the content-based router was most likely forgotten and the data models it uses were not updated. hopefully that happened only in a test environment.  it’s not all bad though, as content-based routing enhances extensibility by enabling more refined subscriptions. the additional plus of this situation, which is especially relevant for cloud environments, is that we limit the number of calls, as that is what is generating costs. however, this comes at a damage to simplicity. the additional routing application adds complexity to the system, making it harder to understand, test, maintain, and troubleshoot. operational considerations from an operational perspective, content-based routing introduces several challenges that cannot be ignored when introducing this pattern into the communication flow.  first of all, this pattern introduces additional latency. the fact that we place an additional component in between the event producer and event consumers, which is responsible for parsing the payload, extracting relevant information, repackaging the payload and redistributing it with additional metadata attached. all of these are additional steps that take time, which means it will have an impact on performance. secondly, we have to deal with the maintenance overhead! dedicated application responsible for producing routing metadata requires ongoing maintenance and operational support. this includes ensuring it has sufficient resources, monitoring its performance. this means that we need to be aware of any issues that may arise, before they become a problem! neglecting this can lead to performance bottlenecks and communication failures, which can be pretty costly for a business. as mentioned above, introducing an additional application increases the complexity of the overall solution. this makes it more challenging to test and debug, as there are more moving parts and potential points of failure. observability is also impacted, as it becomes harder to track the flow of messages. if the router is a workaround, that was created to solve an immediate problem, it will likely have no proper monitoring, so quite soon after, no one will know what is happening to it. this leads to another aspect of this pattern that needs to be addressed - it may become a single point of failure! and that is not for a single integration flow, but as it redistributes the events, especially those that came as coarse grained, to more than one topic with added metadata, it impacts multiple business processes. if it goes down, message routing is disrupted, potentially bringing a broad spectrum of communication to a halt. lastly, if the message payload contains sensitive information it may have been previously encrypted by the producer. for the routing to work, the router would need to decrypt the payload for analysis. we may end up in a situation, where the communication that was previously considered secure, now leaks data through logs that were not properly masked and persist payload in an unparsed, yet human-readable form. this introduces a potential security risk, especially if the application is not properly maintained, updated, monitored, or secured.  conclusions content-based routing in event-driven architectures is generally considered an anti-pattern. while it can provide a temporary solution for routing messages when producers lack metadata, it introduces significant risks and complexities. the dedicated routing application often becomes a maintenance burden, a performance bottleneck, and a single point of failure.  whenever possible, the long-term solution is to refactor legacy producers to include the necessary metadata for routing. this eliminates the need for a dedicated routing application and simplifies the overall architecture. so while content-based routing might seem like a perfect quick fix, its drawbacks often outweigh its benefits, making it a good practice to avoid or remove as quickly as possible in most scenarios."
  },
  {
    "title": "API-Led Architecture",
    "raw": "api-led architecture something old, something new as we take deeper dives into ecosystem architectural styles, we can see that we already solved some problems using event-driven architecture and broker architecture, but more challenges come with scale. as we keep introducing new complexity and dependencies with new business systems, those architectures start to become over cumbersome, with a large number of repetitions. now we are facing a challenge that could be named \"dedicated vs composable integration flows” which would be an architectural equivalent of the “dry code vs wet code” in development.  let’s apply some architectural thinking and analyze the trade-offs for each option: dedicated integration flow: in dedicated integrations, the level of reusability is fairly limited, mostly to reusable libraries, specific snippets of code. the integration flows themselves are closely knitted to the business needs and are a reflection of the underlying business process. this means that quite often access logic to specific systems is duplicated, with small variations to the query parameters or scope of data. this brings a benefit of less abstract code and being able to precisely fit to the business requirements. this comes at the cost of having a bigger codebase to maintain and operate, which can lead to development bottlenecks as the focus will shift from development to operations and maintenance. composable integration flow: in composable integrations, the integration flow is split into microservices, facilitating a specific part of the communication. in that case we’re not only reusing code libraries, but whole integration services. this means that while the logic does not get duplicated as much, it usually has to be designed either in a generic way, to cover a broader spectrum of possible needs or on an iterative basis to extend the services as needed to reach that broad spectrum. while this comes at the benefit of having a smaller code base, that is easier to scale and manage as separate services, the cost is that it becomes a bit more abstract and harder to understand. this requires proper governance and additional tools to govern all available services and make them discoverable and searchable, otherwise the duplication will not happen at code level, but rather on the level of whole services.  ecosystem architecture styles scope how does that relate to ecosystem architectural styles?? if we take a look at eda and broker architecture, there is a large number of repetitions within those architectural styles. with eda it is due to the effort on communication being distributed among various producers and consumers and encapsulated within those systems. with integration brokers, the balance shifts a little bit closer to removing duplication by providing a finite level of abstraction. this is due to the workflow-like nature of integration flows, where, while there is reusability on the code level, it makes the flows a bit repetitive as they might be required to be implemented over and over again for different systems with some variance. both of those architectural styles can be placed on a spectrum between dedicated and composable approaches, yet closer to dedicated.  something borrowed going into the direction of the composable approach we can find api-led architecture, which is an architectural style that is a descendant from service-oriented architecture’s (soa) enterprise service bus (esb). while there are many similarities, the core concept moved so far away from the original that it deserved a name of its own. this architectural approach provides a three-layered separation of concerns within an integration platform that was present in the industry at the beginning of 2010s. mulesoft refined the concept further and coined the term “api-led connectivity” around 2016. a large part of what api-led architecture is, was refined and coined as a standard by that technology vendor, despite the fact that you can implement this architectural style in a number of different technologies.  what is api-led architecture? api-led architecture is an ecosystem architectural style that builds atop the previous architectural styles expanding their capabilities and adding a more standardized and reusable approach to interoperability. the key aspects are: separation of concerns - the communication between business systems is separated by three layers of integration applications, each with different responsibilities. separation of access - each business system has a dedicated set of integration applications,  adapter for traffic towards a system - exposing standardized data services (crud-based) to the rest of the ecosystem, channel for outbound traffic - exposing dedicated services to systems, otherwise this architectural style reinforces the integration platform aspects inherited from broker architecture: protocol agnostic approach to communication, mediating between various contract data models and formats, provide orchestration logic, provide observability, abstraction and extensibility capabilities, three abstraction layers one of the most important aspects of api-led architecture is the capability to provide abstraction layers separating the business systems from each other. those layers have distinct responsibilities that they deal with as they abstract different aspects of the communication: channel layer - an abstraction layer composed of integration apps and their apis exposed directly to the consumers of the integration platform, with each one of them represented by its own channel application is responsible for: exposing to the consumer dedicated services that they need, translating from the integration contract to the internal platform service contract, limiting the service response if the payload scope of data is too extensive, narrowing it down only to the needed set, provide additional communication metadata, that could be based on the payload, to enable proper message routing in case of publishing events, caching responses to frequent consumer requests (if applicable), creating a secure dedicated channel, that will only be used by a single business system, providing observability metadata on service usage of the consumer system, abstract the service or data service contract to mitigate the impact of some service changes on the customer, composition layer - an abstraction layer composed of integration apps and their standardized apis responsible for composing complex services usually grouped around a specific set of business processes or capabilities (e.g. customer, or order related), exposing standardized reusable services to the channel layer, encapsulating the complexity of service and data orchestration between various data services, caching responses to frequent service requests for all consumers (if applicable), providing observability metadata on the integration platform service usage, adapter layer - an abstraction layer composed of integration apps and their standardized apis exposed for the internal use of integration platform. applications within this layer hold data services of all provider systems, with each one of them represented by its own adapter application, which is responsible for: exposing a standardized reusable data services to the channel and composition layers, encapsulating the complexity of data access logic, translating between the integration platform internal service contract and the provider system contract model for the messages passing between the integration platform and the system, limiting event-based communication payload scope if the data is too extensive, narrowing it down only to the needed set, caching responses to frequent data service requests for all consumers (if applicable), creating a secure dedicated adapter to funnel all traffic to the system, providing observability metadata on data usage of the provider system,   qualitative analysis as we did with previous architectural styles, let’s explore the qualities of api-led architecture and look into a few pitfalls that can be crucial to consider when trying to apply this architectural style. for that we will be using a comparison table that was produced through a qualitative analysis of architectural styles taking several architectural characteristics into account. if you would like to learn more about this analysis or read how we define those characteristics, you can do so by reading this article, where we explain how this comparison was created. cost analysis api-led architecture is always a significant investment. at least this is the case in the first few years, when this new concept of a mediator will still be debated, challenged and its governance will be in a state of flux till it stabilizes. the development cost (3\uD83D\uDCB2) at first encompasses the creation of all of the base infrastructure for each business system, that is: their respective integration apps, as well as those for processing, figuring out the logic on how to expose and standardize data, etc.. over time this cost lowers as a lot of data services and composed services are already there, which means they can be leveraged to compose new services and expose them as needed to new business applications, gradually lowering the development effort, time to market and cost. the cost score here is an average of the initial cost and the later lowered value.   operational cost (4\uD83D\uDCB2) in api-led architecture is usually high. this is due to several factors: licensing cost and model - technologies tailored to facilitate this architectural style are expensive and have various licensing models that monetize their use differently on premise and in cloud, number of components - the number of deployable components and their size may be large in the later stages of the platform’s life, so that will be a factor increasing the cost with growing ecosystem complexity, standardized logging, monitoring and analytics - api-led based integration platforms can produce a significant amount of logs that need to be processed, stored and made searchable, this means additional costs, especially in cloud environments, where data transfer is the costly part, the distributed nature of the platform means that, while it is a lot more readable, without support from proper tooling it will likely take a lot of effort and knowledge to perform a proper root cause analysis (rca) for any incidents, so the initial operational investment into the right tools is crucial.   the real value of api-led architecture lies in the architectural change cost (2\uD83D\uDCB2) as it is one of the super powers of this architectural style. as a result of architecting for the right level of abstractions, whole systems can be replaced with limited impact on the ecosystem, as most of it will be encapsulated within the integration platform and never propagated to the business systems, unless there is a reason for that. architectural and design time analysis from the perspective of the qualitative analysis it may seem like api-led architecture is always the best choice for an ecosystem architectural style. so before we jump into the great qualities of it, let’s discuss the less beneficial scores. starting off with simplicity (3⭐), which is scored similarly to broker architecture, but for a completely different reason. if we look at integration brokers, this is a collection of easy workflow-like integration flows, usually highly custom and tightly knit with the business process, which gives it that particular score. with api-led architecture, we have a single integration flow distributed across several integration apps starting at at least two. the complexity of this style seems a lot higher by that alone. fortunately due to the way abstraction layers are organized and the fact that integration applications are assigned in every layer, the overall architecture is fairly more readable, than a collection of integration flows built in a broker. because of that the score cannot really be lower, and at the same time the sheer scale of this kind of platform does not allow for it to be higher.  moving on to qualities that were scored higher in the qualitative analysis, let’s start with the capability to provide abstraction (5⭐), as it is the key benefit of this platform that to an extent sets the score higher for other architectural characteristics as well. as mentioned above api-led architecture has three distinct abstraction layers, which enables encapsulation of logic and a clear division of responsibility within the code. this highly impacts contract resilience (4⭐) and composability (5⭐), as any impactful changes (e.g. a change of a contract with the system) can be contained as close to that system as possible, limiting the bubbling upstream and forcing changes on other business systems. with this encapsulation of data access logic and service orchestration, api-led architecture enables architects to mix and match, to create new services to facilitate business needs. which fosters a reusable mindset. lastly, the encapsulation of logic in abstraction layers enables a certain ease in adding new consumers to the platform, boosting its extensibility (5⭐), making each future change somewhat easier. operational analysis moving on to the operational qualities of this architectural style, we can see that the scores are a little bit more polarized. looking at the superpowers of api-led, we can see that it is a highly scalable approach with scalability scoring at 5⭐. this is another aspect deriving from the separation of concerns and the abstraction layers, where due to the microservice nature of the integration platform every single element is separately scalable. this enables a great degree of fine tuning flexibility, giving a lot of control over resource consumption, especially with high reuse of all services. however the fact that we have so many layers and applications composing the integration platform negatively impacts testability (2⭐) and performance (2⭐). there simply are too many moving parts, which bars us from performing tests really fast, especially if end-to-end testing reveals some bugs - analysis may take some time. performance wise, the chaining of microservices introduces more latency than it was with the broker architecture, making this architectural style slightly slower than the competition.   looking at a different aspect of observability (5⭐), the abstraction layers and dedicated integration apps may provide a significant amount of data and metadata about the health and operations of the whole ecosystem, often leading to situations that the integration platform operations may spot problems in connected business systems, before their operations teams see them themselves. this level of granularity and fine details, combined with proper metadata, supports auditability (5⭐) in a fantastic way, enabling gathering of significant data that can be later used for audit trails, tracing or simply rca.   lastly, as a result of the abstraction capabilities and integration apps granularity, the security (5⭐) aspects of the whole ecosystem gain a great benefit, as not only metadata of all communication is available for anomaly analysis, there is more room for swift reactions as well. if a system is compromised for some reason, the abstraction layers halt the progress of the intrusion to a degree. if needed, all communications with that breached application can be cut by disabling the whole integration applications responsible for facilitating communication or if the danger is lesser only specific operations tied to the compromised parts.  conclusions the evolution of integration architectures mirrors the growing complexity of modern it ecosystems. api-led architecture emerges as a pivotal advancement, addressing the shortcomings of earlier approaches like event-driven architecture and broker architecture. by embracing a three-layered abstraction (channel, processing, and adapter), this style not only enhances efficiency and agility within integration processes but also empowers organizations to gain deeper insights into data flows and system dependencies. this improved understanding can serve as a catalyst for strategic business decisions and drive innovation across the enterprise. while the initial investment in implementing api-led architecture is significant, the long-term benefits in terms of scalability, observability, and security, coupled with the potential to reduce architectural change costs, position it as a compelling option for organizations seeking to modernize their integration landscapes."
  },
  {
    "title": "API-Led Synchronous point-to-point",
    "raw": "fixated on the grain of sand in the landscape of api-led architecture, the synchronous point-to-point pattern might initially appear as a roundabout route for a simple exchange. one could question the need for mediating applications within an integration platform when a direct system-to-system call seems more straightforward. however, to view it this way is to miss the fundamental purpose of this pattern. it isn't just about connecting two points; it's about constructing a robust, adaptable, and reusable infrastructure that transcends individual interactions. with this article we will explore this pattern shifting the perspective from the grain of sand to a wider picture. pattern nameplate name: synchronous point-to-point communication mode: synchronous architectural style: api-led architecture common use cases:  interactive data retrieval - e.g. partial match for forms on a frontend app. for better understanding of usage, for monetization or utilization of a paid service (one funnel to keep track of all usage), data on demand - retrieving data that already \"exists\" within a system, for operational use, enrichment etc., hiding core systems behind integration layers, validating input data to mitigate security issues and potential breaches,\t\t\t data model and format translation between not aligned systems, potential reuse of adapter data services for other cases, data quality assurance based on api contracts, architectural coupling:  contract coupling - both, the service consumer and the data provider are bound by contract coupling with the integration platform, but not bound by said coupling to each other, semantic coupling - unavoidable with any data exchange, operational coupling: all communication happens within a single architectural quantum that envelops both systems and the integration platform, diagram(s)  synchronous point-to-point  synchronous point-to-point, data service reuse  pattern analysis synchronous point-to-point is the most common integration pattern in api-led architecture, making up the majority of the communication within the platform. at first glance, this might seem excessive for such a simple communication, and this would probably be true if we look at this pattern from a perspective of a single instance of such communication. however, the true value of this pattern becomes apparent when we analyze the implications of its use from a wider perspective, considering its reusability and the broader context of api-led architecture. unlike traditional point-to-point, this pattern introduces two intermediary layers: the channel layer and the adapter layer. these layers, while adding complexity, provide benefits that may not be obvious. the channel application tailors the api to the consumer's specific needs, while the adapter layer standardizes access to the provider system. this setup, though initially appearing as overengineered, becomes highly beneficial when the data service is reused by multiple consumers or for building composed services. architectural considerations when discussing the point-to-point pattern in api-led architecture, it is crucial to look beyond the immediate communication at hand and consider the platform as a whole. when using this architectural style, we're not only solving an immediate communication problem. we're also addressing several other problems that were not addressed in the earlier ecosystem architectural styles.  in terms of architectural qualities let's start a deeper dive with the capability to create abstractions. the channel and adapter layers serve this function in this architectural style, to an extent shielding the communication participants from contract changes in the consumed apis and ensuring better contract resilience. this means that changes on the provider side have a minimized impact on the overall ecosystem. furthermore, as a result of having abstraction layers, it is possible to make the communication protocol and data format agnostic for all applications invoking the platform, meaning that the integration platform can expose all sorts of protocols and format, and does not force any of those systems to use anything specific, leaving the business developer teams to decide what they are comfortable with.   another key aspect from an architectural perspective is that the technical partitioning of the integration flow creates the capability to treat the adapter applications and their services as reusable and composable building blocks that enable easier data access and developing new integrations and services as needed at a lower cost of time and effort. this further contributes to a reduction in redundancy, as adding a new consumer requires only to create their dedicated service in the respective channel application. this partitioning also contributes to the security by architectural design, as each channel enables to trim down the response scope to exactly the data needed by each api consumer.   another added value of this pattern is that if needed it is easily extensible. if for any reason, the set of data available in the provider system would not be sufficient anymore to facilitate business needs, this pattern can be adjusted and extended to by a composed service, where additional data is introduced through orchestration and enrichment. operational considerations looking at the operational qualities of this pattern, there is one aspect that will influence other characteristics quite significantly. that is increased complexity, which is introduced by the structure of the integration platform. integration applications may be considered as potential points of failure that need to be properly covered by operations and maintenance. all communication passing through them also extends the length of root cause analysis, when trying to track errors. additionally, testability of the integration is also impacted, as there are more parts to it that require testing. the number of tests that have to be performed increases, which also influences time to market. however, the technical partitioning of the integration platform and the readability of its structure makes this aspect navigable. another operational characteristic that is influenced by the complexity is performance. while this might not be a significant impact, as most integration platforms are optimized for high performance, the additional components will introduce additional latency, which may be a risk factor if the applications are not properly maintained or will have excessive logic. luckily this pattern brings a lot of operational benefits as well. one of the key aspects that influences the performance and availability of the integration services is the robust capability to scale each integration application and its services, both vertically and horizontally. scaling can be done separately for the channel applications and for the adapter applications, which supports the reusability and composability aspect greatly. as the data service in an adapter can be reused by other consumers and used as a building block for composed services, it will experience a different level of traffic, hence it needs to be sufficiently scaled with the underlying it connects to. furthermore, this pattern structure supports security, as all service consumers can be easily cut off from data sources if compromised. same goes for any data provider, if the data becomes corrupted or the system compromised it can be easily excluded from the communication. this means that only the services provided by that specific system will be unavailable, not the entire integration platform or ecosystem. other integration applications will function normally.  lastly, a very important operational quality is that this pattern enables a lot better observability, which can provide significant information on how data is consumed in the ecosystem. having a clear separation of apis designated for consumption by various systems, we can easily track which system consumes what and when. this in turn gives a very robust set of data to support auditability. conclusions at first glance, this pattern might be criticized for its seemingly excessive use of resources to facilitate simple communication. however, when viewed within the broader context of api-led architecture and the reusability of specific services and abstraction layers, the benefits become clear. this pattern is highly viable and provides significant value in complex environments. the abstraction layers enhance contract resilience and protocol agnosticism, while the reusability and composability lead to enhanced scalability and security. although it might introduce a slight performance overhead and increased complexity, the advantages in terms of ecosystem stability and extensibility make it a valuable approach."
  },
  {
    "title": "API-Led Synchronous Service",
    "raw": "moving complexity in the world of application integration, managing complexity is an unending challenge. as we navigate the labyrinth of data flows and system interactions, we must acknowledge that complexity, especially that of communication, is inherent and unavoidable. the core issue lies not in eliminating it altogether, but rather in strategically assigning the responsibility of handling it. this is perfectly captured by tesler’s law: “every application must have an inherent amount of irreducible complexity. the only question is who will have to deal with it.” law of conservation of complexity, larry tesler as we dive into synchronous services in api-led architecture we can see that this pattern is a crucial strategy for addressing communication complexity and moving it away from business systems through orchestrating complex data flows in distributed environments. unlike direct point-to-point integrations, this pattern leverages the qualities of an api-led integration platform to manage and orchestrate data traffic and transformation between multiple business systems. this approach simplifies the communication logic required on the consumer side to a minimum. at the same time it promotes reusability of composed and data services. in environments where data sources are fragmented and fulfilling a specific use case requires orchestrating multiple calls, the synchronous service pattern proves invaluable. pattern nameplate name: synchronous service communication mode: synchronous architectural style: api-led architecture common use cases: data quality assurance based on api contracts data on demand - data enrichment to make a data retrieval, where the caller does not have sufficient data to make a p2p call hiding core systems behind integration layers, validating input data to mitigate security issues and potential breaches potential reuse of composed services for other cases architectural coupling: contract coupling - both, the service consumer and the data provider are bound by contract coupling with the integration platform, but not bound by said coupling to each other, semantic coupling - unavoidable with any data exchange, operational coupling: temporal - the fact that this pattern utilizes a composed service, which is responsible for orchestration of various calls, means that we are deliberately introducing temporal dependency, where the execution of each subsequent orchestrated call usually depends on the successful completion of the previous call. this coupling is there by design, and is usually unavoidable as it derives from business logic and communication complexity moved from the service consumer system to the integration platform. all communication happens within a single architectural quantum that includes all involved systems (service consumer and providers - can be more than 2), and the integration platform, related to: synchronous point-to-point diagrams pattern diagram synchronous service behavioral diagram example sequence diagram for synchronous service pattern analysis the synchronous service pattern is an extension of the synchronous point-to-point pattern, characterized by its use of a composed service residing within the integration platform's composition layer. this service is responsible for orchestrating calls to multiple data services, enriching data, and transforming it to meet the needs of the consumer. by centralizing orchestration within a composition layer, this pattern abstracts away the complexities of individual system interactions, allowing consumer systems to focus on their core business logic. this method contrasts sharply with traditional point-to-point integrations, where each consumer system would need to manage its own complex communication logic, including orchestrating calls, managing data transformations, and handling diverse data models. orchestration typically occurs in sequence, though parallel calls may be possible for data gathering scenarios. unlike simple pass-through services, the composed service adds significant value by managing complexity, service state, and providing a standardized service api to consumers. the fact that the logic is centralized also helps solving error handling issues, especially if the communication needs to be transactional between all provider systems. the composed service, since it holds all communication state, is where transaction error handling, rollback or compensating updates logic can be placed. architectural considerations architecturally, the synchronous service pattern leverages the abstraction capabilities of api-led architecture to a great extent. data services abstract the logic of accessing data in provider systems, providing standardized access points. composed services then abstract the need for multiple calls to these data services, offering a single, standardized api to consumers. the composability of this pattern is a significant advantage, as composed services and data services can be reused as building blocks for various integration needs. this reusability reduces development effort and time, as new services can be created by leveraging existing components. this multi-layered abstraction builds robust contract resilience, as changes in provider systems can be mitigated within the integration platform without impacting consumers. additionally, the use of dedicated services in channel applications allows for tailoring responses to specific consumer needs, further enhancing flexibility and security. extensibility and removability are also significant benefits, as services, as well as consumer and provider systems, can be easily added, removed, or replaced without affecting the entire ecosystem. while the pattern itself is not inherently simple, the technical partitioning of the integration flow enhances readability, making it easier to understand and troubleshoot compared to alternatives in other architectural styles. operational considerations operationally, the synchronous service pattern presents both challenges and benefits. performance can be a concern due to the sequential invocation of data services, which can introduce latency. if the underlying data services are not invoked in parallel, then the service latency becomes the sum of latency over all integration services (which luckily is usually fairly low), as well as the provider systems. if the providers are not performant, this can lead to communication bottlenecks and further down the line to timeouts. on the other hand, the overall scalability and high availability of the pattern are significant advantages. each application on the integration platform can be scaled individually, making them performant even if they are significantly reused. testability is more challenging compared to other architectural styles due to the complexity of orchestrating multiple services. however, the structured approach of technical partitioning aids in root cause analysis, making it easier to identify and resolve issues. this further enhances maintainability as we can easily pin point the problematic areas. security, observability, and auditability are enhanced due to the technical partitioning. the distributed nature of the platform means that proper tooling is essential for effective monitoring and root cause analysis. robust logging and metadata tracking provide valuable insights into data consumption and system behavior, supporting auditability. granular monitoring and control of data flow are possible, and the ability to isolate communication for specific systems further enhances security. lastly, composability, and the modular approach that comes with it, simplifies and enhances maintainability. however, that can be severely impacted by transactionality which can be a challenge, when multiple provider systems require transactional behavior. in such cases, the composed service must manage transactionality and rollback mechanisms, adding complexity to error handling. conclusions the synchronous service pattern is a valuable tool in the api-led architecture toolkit, particularly in complex, distributed environments. it provides a structured approach to orchestrating data flows, simplifying consumer logic, and enhancing overall system maintainability. while it introduces some complexity and potential performance overhead, the benefits in terms of contract resilience, composability, and extensibility outweigh these drawbacks. the pattern’s ability to abstract away complexities, provide reusable building blocks, and enhance security and observability make it a robust choice for modern integration needs. as organizations continue to evolve their it ecosystems, the synchronous service pattern offers a flexible and scalable solution for managing intricate data interactions, ensuring that data is delivered efficiently and securely to all consumers."
  },
  {
    "title": "Bridging Worlds: EAI and DDD for seamless interoperability",
    "raw": "complexity finds us again in the intricate landscape of modern enterprise systems, the challenge of integrating diverse applications and data sources is a constant. often, these systems evolve independently, leading to a complex web of interactions that can be difficult to manage and maintain. we can address the technical challenges of communication with proper integration patterns, good practices and architectural styles. unfortunately there are aspects of communication, like understanding data, business behaviour and language in various contexts, or mapping out dependencies, that also need to be addressed. this is where the purely technical approach fails. bridging bounded contexts to navigate this complexity effectively, we can turn to domain-driven design (ddd), which offers a powerful concept: \"bounded contexts\". they represent distinct areas of a business domain, each with its own unique model and \"ubiquitous language\"*. this means that the meaning of a particular term or concept can vary depending on the specific bounded context in which it's used. this in turn creates a need for maintaining clear boundaries between these contexts as a crucial part of managing complexity and ensuring that each part of the (eco)system remains coherent. however, the existence of these distinct bounded contexts naturally leads to a need for integration. business processes often span multiple contexts, requiring data and services to be shared and coordinated. this is where enterprise application integration (eai) comes into play. here, we’d like to dive in and explore how ddd and eai intersect to facilitate communication between bounded contexts. *as we dive into this article, we’re clashing two different jargons, ubiquitous languages of ddd and eai. for a side by side comparison and definitions, see the glossary section at the end of this article. understanding bounded contexts the importance of bounded contexts lies in their ability to manage complexity. by clearly defining boundaries, we prevent the mixing and confusion of concepts from different parts of the business. for example, a \"customer\" in the sales context might have different attributes and behaviors than a \"customer\" in the support context. within the sales context, \"customer\" might refer to a potential buyer with information like lead source and purchase history, while in the support context, \"customer\" might refer to an existing user with details on service tickets and support interactions. clear boundaries maintaining these clear boundaries and distinct models within each context is crucial. it ensures that each part of the (eco)system remains coherent and focused on its specific purpose. the ubiquitous language within a bounded context is vital for effective communication among domain experts and developers. it fosters a shared understanding and reduces ambiguity. however, it's essential to recognize that the ubiquitous language can differ between contexts. what one term means in one context may have a different meaning or be entirely absent in another. while bounded contexts provide clarity and manage complexity, they also necessitate mechanisms for integration. as business processes often span multiple contexts, requiring data and services to be shared and coordinated. this leads us to the need for effective integration strategies. essentially, bounded contexts are like specialized departments within a company; they each have their area of expertise, but they must collaborate to achieve the company's overall goals. the customer-supplier relationship in eai in the context of enterprise application integration (eai) and bridging bounded contexts, the \"customer-supplier\" relationship is a fundamental pattern. while there are other types of relationships, like partnership, or shared kernel, they do not really exist on the ecosystem layer, as they occur on a smaller scale. that means they usually are present in systems, something designed and developed in a closer knit environment (e.g. dev teams of the same company, or even the same development team). the situation we face on an ecosystem level is that the systems are built not only by completely different teams, but quite often also by different companies (e.g. saas solutions, custom software made by consulting companies). most of the time they do not necessarily share the exact same purpose. think of service providers, they usually aim for more generic service, and it is only our business process that provides context and defines which options/parameters are used. if we’d analyze the relationships between various applications in the ecosystem architecture, we’d see that the customer-supplier relationship is omnipresent. at the heart of this pattern lies the distinction between \"upstream\" (supplier) and \"downstream\" (customer) systems. the upstream system is the one that provides services, data, or functionality. think of it as the source of truth or the system that owns a particular piece of information. that leaves the downstream system to be the one that consumes or utilizes these services or data. it relies on the upstream system to fulfill its needs. consumer-supplier relationship in ddd in eai terminology, these concepts often translate to \"provider\" and \"consumer.\" the provider system exposes an interface or api that other systems can use, while the consumer system integrates with that interface to obtain the necessary resources or use a specific service, so consumes the interface. this relationship isn't merely technical; it often reflects real-world business relationships and power dynamics. the upstream system, as the provider, may have more control over the data format and service availability. the downstream system, as the consumer, must adapt to the provider's offerings. for example, consider an e-commerce platform that exposes an api for new orders. a fulfillment system—responsible for processing and delivering those orders—integrates with this api to retrieve order data and initiate downstream logistics processes. in this case, the e-commerce platform acts as the upstream system (supplier), owning the order data and defining the integration contract. the fulfillment system is the downstream system (customer), consuming that contract. but is that all that can be said about the customer-supplier relationship? certainly not! within this there are distinct behaviours that have further implications on how systems interoperate. the conformist relationship in the case of integrating bounded contexts, the \"conformist\" relationship represents a specific approach to how a downstream system interacts with an upstream system. in this pattern, the downstream system chooses to align its own internal model with that of the upstream system. essentially, the downstream system conforms to the data structures, service interfaces, and behaviour provided by the upstream system without significant transformation or adaptation. conformist relationship in ddd this means that the downstream system directly uses the data and services as they are offered by the upstream system. if the upstream system provides data in a specific format or through a particular api, the downstream system adopts that format and api without introducing an intermediary layer to translate or modify it. this approach can significantly reduce the development effort required for the downstream system, as it eliminates the need for complex mapping or transformation logic. however, the conformist relationship comes with certain implications. by conforming to the upstream system's model, the downstream system becomes tightly coupled to it. any changes to the upstream system's data structures or apis may require corresponding changes in the downstream system. this introduces significant risks in highly volatile (eco)systems and, as the conformist approach offers close to non protective mechanisms, can lead additional problems: high maintenance overhead - each change upstream cascades downstream, causing continuous integration and testing efforts. reduced autonomy - downstream teams lose control over their internal domain model and become reactive, forced into constant adjustment to upstream changes. increased risk of misalignment - frequent changes increase chances for integration issues, misunderstandings of changed data semantics, or misaligned business rules. despite these potential drawbacks, the conformist relationship can be a suitable choice in certain scenarios. for example, when integrating with a well-established industry standard api or a legacy system that is unlikely to change, conforming to its model may be the most practical and efficient approach. it can also be beneficial when the downstream system has limited resources or a tight deadline, as it allows for faster integration. open-host service (ohs) as we mention an industry standard, this is a good chance to introduce the \"open-host service\" (ohs) relationship, which is another crucial concept in integrating bounded contexts. in this relationship, the upstream system (provider) creates a stable, well-defined interface, often referred to as a \"published language,\" that acts as a facade. this interface shields the internal implementation details of the upstream system, providing a consistent and predictable, so standardized, way for downstream systems to interact with it. open-host service relationship in ddd essentially, an ohs acts as an abstraction layer, offering a standardized set of services and data formats, abstracting the internal structure and logic. this allows the upstream system to evolve without disrupting downstream systems that rely on the ohs interface. by loosening the coupling between the internal workings and the external interface, the ohs promotes a solution that is more resilient and extendable, making integrations more reusable and maintainable. in the context of application integration, the ohs provides a standardized interface, typically through clearly defined apis representing publicly available data and services. by establishing a clear contract, ohs defines exactly which services are available and how to access them. this stability and predictability facilitate collaboration and integration across different teams and systems. although it is worth noting that sometimes the ohs is enforced upon systems by compliance with specific standards well established in an industry, standardization like iso (e.g. iso 20022) or laws and regulations (e.g. openbanking or eu psd2 regulation with its respective country implementations). the key benefit of ohs is that it allows the upstream system (provider) to evolve its internal model without breaking downstream integrations. as long as the ohs interface remains stable, downstream systems can continue to function even if the upstream system undergoes significant changes. this provides flexibility and agility, allowing the upstream system to adapt to changing business needs without causing disruption. anti-corruption layer (acl) shifting the perspective to the consumer side, the \"anti-corruption layer\" (acl) relationship addresses the challenges that arise when a downstream system (consumer) needs to integrate with an upstream system whose model or data format is significantly different from its own. in this relationship, the downstream system creates an abstraction layer that is responsible for translating the upstream system's model into its own internal model. anti-corruption layer in ddd the primary purpose of an acl is to protect the downstream system from being \"corrupted\" by the upstream system's data or model. it acts as a buffer, insulating the downstream system from changes or inconsistencies in the upstream system. by creating a translation layer, the acl ensures that the downstream system only deals with data and services that align with its own internal logic and structure. in the context of application integration, acl enables looser coupling between the communication and the logic of the downstream system. it allows the downstream system to maintain its own domain model and business logic, even when integrating with external systems that use very different data models and formats. the downstream system becomes more resilient to changes in the upstream system. this means that the impact of frequent, often breaking, changes done to the upstream system, will be limited to the acl. the benefits of acl include increased autonomy, reduced coupling, and insulation from upstream changes. by creating a translation layer, the downstream system can protect its own integrity and maintain its own internal consistency. this further contributes to the downstream system evolvability. additionally the acl reduces the risk of unintended consequences when integrating with external systems. having ohs and acl in many complex integration scenarios, both open-host service (ohs) and anti-corruption layer (acl) relationships are used in conjunction. the upstream system can provide an ohs, offering a stable and well-defined interface, while the downstream system can use an acl to further adapt the data to its specific needs. this combination provides a flexible integration, balancing the needs of both providers and consumers. the ohs ensures that the upstream system can evolve without breaking downstream integrations, while the acl ensures that the downstream system can maintain its own internal consistency and autonomy. for example, consider a legacy system that provides data through an ohs. a modern application that needs to integrate with this legacy system can use an acl to translate the data from the ohs into its own domain model. this allows the modern application to work with the data in a way that makes sense for its own business logic, without being constrained by the legacy system's data structures. in complex integration scenarios, especially those involving integration platforms, using both, or even multiple, ohs and acl can significantly improve the overall architecture and maintainability of the (eco)system. it allows for a clear separation of concerns, promotes loose coupling, and provides flexibility for both providers and consumers. glossary as you have probably noticed by now, the language used in domain-driven design differs from the one used by us to describe application integration. to make life a little easier here’s a cheat sheet of terms in both jargons, or at least their closest related terms: ddd term eai term description bounded context system/service a distinct area of a business domain with its own unique model and ubiquitous language. in eai, this often translates to a separate system or service that provides specific functionality or data. upstream system provider/producer the system that provides services, data, or functionality to other systems. in eai, this is the \"provider\" that exposes an interface or api or produces events. downstream system consumer the system that consumes or utilizes services or data from another system. in eai, this is the \"consumer\" that integrates with the provider's interface or subscribes to events provided by a producer. ubiquitous language api contract the shared understanding of terms and concepts within a bounded context. in eai, this often translates to the api contract that defines the data structures that are exchanged between systems and the behaviour of the services. open-host service standardized interface, api, or service a stable, well-defined interface (published language) created by the upstream system to provide a consistent way for downstream systems to interact. in eai, this is akin to a well-defined api or service that acts as a facade. anti-corruption layer abstraction, transformation or mapping layer a layer created by the downstream system to translate the upstream system's model into its own internal model. in eai, this is a component that handles data transformation and mapping between different system models. conformist direct integration a relationship where the downstream system aligns its internal model with the upstream system's model without significant transformation. in eai, this often means direct integration without an intermediary layer, using the provider's api and data formats as is, e.g. integrating two databases via db-links. published language api specification the stable, well-defined interface that defines how downstream systems can interact with the upstream system in the ohs pattern. in eai, this is equivalent to the api specification or documentation that outlines the available services, data formats, and protocols. there can be multiple published languages per ohs. model translation data mapping the process of converting the upstream system's model into the downstream system's model, typically done in an acl. in eai, this involves data mapping and transformation between different data formats and structures. conclusions facing complexities of modern integration between various applications like saas solutions, custom made software and legacy systems, it is important to build an understanding of the common pitfalls that we can encounter. the open-host service (ohs) and anti-corruption layer (acl) can be crucial tools for building flexible and maintainable architectures. the combination of ohs and acl is particularly powerful, especially in complex scenarios. by leveraging ohs, providers offer a clear and stable point of interaction, while acl allows consumers to adapt and integrate data without compromising their own integrity. this approach promotes resilient ecosystem interactions, enabling systems to evolve independently while still effectively communicating and sharing data. in essence, ohs and acl are not merely technical patterns but architectural strategies that facilitate collaboration, reduce dependencies, and ensure the long-term viability of integrated systems."
  },
  {
    "title": "Understanding_Active_and_Passive_Broker",
    "raw": "there are many various approaches to application integration, as we may see by looking at least at the ecosystem architectural styles. today we’ll have a closer look at one such approach rooted in the broker architectural style, where we encounter two distinct behaviors in integration platforms and integration flows built within them: the active and passive participant. these behaviors define how communication is initiated and managed, significantly impacting the overall architecture, operational efficiency, and reusability of the integration flows. understanding the nuances of these behaviors is crucial for designing robust and scalable integration solutions. two variants of broker behavior in broker architecture, we observe two primary approaches to how integration flows would behave: active and passive. an active participant integration flow (or active broker for short) initiates and controls all communication within the integration flow. it is triggered either manually or by a scheduler (e.g., a cron job). in contrast, a passive participant integration flow (or passive broker) waits for external triggers from other systems, responding to requests and facilitating communication only when invoked. these two behaviors represent fundamentally different approaches to managing integration flows, each with its own set of advantages and disadvantages. active broker in an active broker scenario, the integration platform takes the initiative in driving communication. it actively triggers and controls all interactions within the integration flow. this means that the broker, either manually or through a scheduled mechanism like a cron job, initiates calls to various business systems. these systems simply expose their apis (e.g. soap, rest, graphql, messaging protocols) or other data transfer methods (e.g. direct database access, file transfer), while the active broker manages the entire process, including communication metadata, state, error handling, and any necessary compensation updates or rollback procedures. essentially, the active broker is the orchestrator, dictating when and how data is exchanged between systems. this also means that the broker has no knowledge if the data in the source system has actually changed. example active participant integration flow processing sequence diagram architectural considerations while at first, it may seem like this is a flow like any other, it does have some specific considerations that might be important to note out. to start with: cost. since all the business systems are only exposing data, the overall cost of architectural change, to and from this approach is fairly low for most systems. all of the exposed services, apis, structures could be easily reused if the broker integration platform would be changed to a different architectural style. the development effort concentrates on the integration platform, especially if the integrated systems have standardized apis (e.g. saas solutions). looking at the integration flow logic, it is not reusable as it is tightly coupled to the system that it is facilitating the communication for and often directly implements business processes activities. the only condition where the whole flow would be reusable is if there would be distributed identical systems (e.g. target system) and the data would be dynamically routed to endpoints with the right credentials, uris and metadata within the integration logic. the approach is fairly simple, as it moves most of the communication complexity to the integration platform. this allows the business systems to have minimal and simplified integration logic. it also gives centralized control over the whole integration process, which needs to be carefully considered from a design perspective. operational considerations from an operational perspective, alongside the qualities of the broker architectural style, there are a few more specific considerations that can be noted. starting with data freshness, which in this scenario would be a potential operational issue to watch for. since the integration flow is triggered manually or on a scheduler, this means that we are processing data that is ranging in freshness from as old as the interval time to very fresh. this means that using the active participant integration flow bears the risk of bringing data too late to the target systems. this can be somewhat mitigated by setting the scheduler to run very often (e.g. every 5 minutes). this brings us to another issue that needs to be addressed within this behaviour which is resource utilization. frequent polling by the active broker, especially in near-real-time setups, often results in a high percentage of empty calls (over 95% in some studies). this wastes resources on both the integration platform and the business systems, as they are repeatedly invoked even when no data is available. while short polling intervals are great to facilitate user experience and to have data somewhat fast in a target system. this may come at a significant cost, especially if the participants of the communication subscribe to a “pay-as-you-go” model with a cloud or saas provider. there is another side to this, especially with systems that produce a large number of records during the day, e.g. in bursts of user activity. the integration flow might serve in these cases as a rate-limiter, with strict control over how many records are pulled in one execution and when the next execution can start (usually they are not allowed to run parallel to avoid record duplication). this in turn may have a very positive impact on the target systems performance, as they will not be flooded with new records to process. they will receive new data in a rate-limited way, until all data is properly processed. lastly, on a more positive note, active broker behaviour offers more simplicity in monitoring of execution. the predictable nature of scheduled triggers makes it easy to spot anomalies and errors, as logs should appear at expected intervals. this is not limited to the integration platform itself, but also the systems that are the source of data, as they will be queried instantly after execution starts. use recommendations the active broker behavior is a valid approach, but it is not universal. it is worth considering for use cases where data is created or needs to be processed in batches and with longer intervals in between (e.g. 24h, week, every friday, month). this also implies that it should not be used for operational needs that would require real-time data, but rather for recurring, time bound actions, e.g. analytical or data integration purposes. passive broker in contrast to an active broker, a passive broker operates reactively. it doesn't initiate the integration flow (no manual or schedule-based triggers), but rather waits for external triggers from consumer systems. the passive broker exposes apis (such as rest, soap) or messaging endpoints (e.g. jms, or amqp) and listens for incoming traffic. when another business system actively makes a request, the passive broker then processes the data, invoking other systems orchestrating, enriching and broadcasting data as needed. this behaviour positions the passive broker as a service provider, responding to requests and facilitating communication only when prompted by an external trigger. example passive participant integration flow processing sequence diagram architectural considerations facilitating the communication through an active broker will be overall a bit more complex, than it is with the active counterpart. this is due to the fact that now there is a service exposed on the integration platform. a service that will be consumed by one or more systems. this means that the consumers need to build more logic on their side to consume the service, which involves understanding communication metadata, handling state to a limited extent, and supporting sufficient error handling, including errors returned from the integration platform. this requires a better and more precise governance over design, development and operations of such integration flow. the fact that the integration exposes the integration flow as a service to be consumed means that we introduce more reusability. exposing apis allows multiple business systems to consume the same integration flow if it meets their needs. however, the degree of reusability depends on how tightly tailored the flow is to specific business processes. this comes with a small warning, that all consumers will be coupled to this integration service by its contract. the trade-off to having the potential reusability is that all consumers will lose contract resilience, as any changes in the integration service contract that introduce mandatory fields need to be reflected on the consumer side. furthermore, the coupling on a contract level also may pose a security risk in terms of receiving excess amounts of data in the response from the service. this potentially may limit the reusability of the integration flow when dealing with sensitive data, e.g. personal data, which is restricted by privacy laws. if designed well, reusability can lower the development cost, as certain services wouldn’t have to be developed per consumer. unfortunately due to the introduced coupling, and depending on the volatility of changes in the it landscape it may be a problem later on, and increase development costs for all service consumers. so it needs to be carefully considered or properly mitigated by adding an abstraction layer per consumer further increasing complexity of the integration flows. from the perspective of architectural change cost, this approach is a little bit more expensive as replacing a system or the integration platform may require more effort. operational considerations looking at the operational perspective we can see that the passive broker behaviour is a lot better suited for real-time or on-demand processing, as the integration flows will be only executed as they are triggered by the consumer system. in that sense the data freshness will be very good for all real-time flows, working with events and commands, and on-demand data will be delivered as it is needed, not in intervals, minutes or hours after it is actually useful. the passive broker behaviour is also more resource efficient, as there are no empty runs based on a predefined scheduler. that ensures lower operational costs if the licensing model for the systems and/or the integration platform is pay-as-you-go. unfortunately this approach brings in a little bit of complexity to monitoring. as there are no fixed intervals that derive from a scheduler, the behaviour is less predictable, so are the logs. the importance in monitoring shifts towards communication metadata like http headers, where are the key information on the usage, as well as the consumer of the service. use recommendations the passive broker behavior, like its counterpart, has its limitations. the typical use cases for this approach would be for real-time or on-demand communication. it brings the additional value if it is possible to design services with looser coupling to the business process, turning them into standardized, reusable integration services that abstract the complexity of communication with a well defined api. conclusions both active and passive broker behaviors have their place in integration architecture. active brokers are preferred for batch processing and handling large volumes of data with specifically tailored integration flows. they offer simplicity in design and monitoring but lack real-time capabilities and reusability. passive brokers are more suitable for real-time or on-demand communication and offer the potential for reusable services. they are resource-efficient but introduce complexity in error handling and monitoring. the choice between active and passive brokers depends on the specific requirements of the integration scenario and the organization context."
  },
  {
    "title": "When Worlds Collide: EAI and DDD",
    "raw": "bridging strategy and systems as integration architects, we are navigating a constantly shifting landscape. it’s a world where rapid change, complex systems, and sprawling networks of interconnected applications are the daily bread. these systems often stretch across departments, technologies, and ownership models - making it incredibly hard to achieve smooth collaboration and seamless interoperability. that’s why enterprise application integration (eai) has been and most likely always will be a critical discipline: it allows us to exchange data reliably, automate shared processes across diverse systems, and provide much-needed visibility into the flows that connect our businesses. but let’s be honest - while traditional, tech driven, eai helps us move data, it often misses the mark when it comes to aligning with what truly matters: our business purpose. it solves the “how” of connectivity, but not always addresses the “why”. this is where domain-driven design (ddd) becomes our strategic ally. it doesn’t replace eai - it enhances it. ddd equips us with a structured way to think about software as a reflection of our business. it introduces a language to describe the intricacies of our domain, boundaries to handle complexity, and patterns to guide interactions. when we apply ddd to integration, we do more than route messages - align our teams across functions, and replace ad hoc connections with a well-defined, strategic design. the result? a more coherent, collaborative, and business-aligned integration landscape - one that focuses not just on technical aspects of systems, but how the business actually works. strategic alignment: why eai needs ddd enterprise application integration is fundamentally concerned with enabling interoperability between systems - allowing applications, services, and data sources to exchange information and function together cohesively. however, the lack of focus on eai and interoperability as a first-class citizen, followed by missing a clear understanding of what is being integrated and for what business purpose, integration efforts often become technically successful but strategically misguided. the result is a patchwork of ad hoc connections that may enable data flow, but do so in ways that are fragile, inefficient, or disconnected from business priorities. this is where ddd plays a vital strategic role. it fills the critical gap that exists between technical integration mechanics and business intent by providing tools, patterns, and language that align integration efforts with domain understanding. specifically, ddd helps teams: develop a deep understanding of the business domain by fostering close collaboration between developers and domain experts. this ensures that integration is rooted in business reality, not just technical assumptions. for example, in an integration initiative aimed at streamlining the order-to-cash process, developers might initially focus on connecting the erp system with the crm and invoicing services. however, through structured workshops with sales and finance stakeholders, it may become evident that specific customer segments require more tailored process flows like customer risk assessment, fraud detection or other regulatory checks that are not reflected in the current systems. by capturing these nuances upfront and embedding them into the integration model, the resulting architecture goes beyond technical plumbing - it reflects the actual business flow, reduces rework, and increases operational confidence across teams. establish precise system boundaries through the use of bounded contexts. these boundaries reflect natural divisions in the business domain and ensure that models do not leak across contexts, preserving integrity and clarity. for example, in an enterprise where logistics, procurement, and customer service are handled by distinct departments and applications, bounded contexts can be used to define separate integration models for each. attempting to share the same data model between the order management and warehouse systems may lead to misinterpretation of fields like 'status' or 'priority'. by enforcing clear boundaries - such as treating the warehouse system as a consumer that interprets 'priority' only in the context of fulfillment urgency - teams avoid confusion, ensure ownership clarity, and support independent evolution of their respective systems. design and govern system interactions using context mapping, which visualizes and formalizes the relationships between different systems or domains, enabling deliberate and sustainable integration strategies. for instance, in a multinational enterprise where separate systems handle inventory, procurement, and finance across regions, teams often struggle to coordinate integration due to differences in terminology, process maturity, and ownership. by applying context mapping, the enterprise can explicitly chart out which system is the provider (e.g., the central procurement platform), which are downstream consumers (e.g., regional finance tools), and what form of relationship governs their interaction (e.g., open host service or customer/supplier). this clarity allows teams to define the correct level of coupling, implement protective translation layers where needed, and avoid overengineering interfaces by aligning them with actual domain responsibilities. as a result, integration becomes more predictable, governed, and business-aware. promote a shared language through the creation of a ubiquitous language, which reduces ambiguity in communication between stakeholders and ensures that both technical and non-technical participants operate with the same mental model. in the context of enterprise application integration, imagine a scenario where the term \"order\" is used across a crm, an erp, and a warehouse management system - but each uses it to mean something slightly different: the crm might refer to a sales intention, the erp to a legally binding transaction, and the warehouse system to a picking list. without a shared language, these semantic mismatches create confusion, integration bugs, and costly misunderstandings. by establishing a ubiquitous language early in the integration design process - co-defined by domain experts and developers - teams align on what an \"order\" means in each context and ensure that integration logic respects those definitions. this not only prevents errors but also creates a communication foundation that scales with the organization. when these principles are applied within eai initiatives, the integration effort evolves from a narrow technical exercise into a strategic capability. rather than just focusing on the technical \"how\" - how to connect apis, transform data, or orchestrate data flows through the ecosystem - ddd forces a rigorous inquiry into the \"why\": why are these systems being connected? what business processes are being supported? which events, responsibilities, and boundaries define their interaction? consider the earlier example where different systems - erp, crm, warehouse - each interpret an \"order\" differently. without a clear shared language and explicit context boundaries, integration teams risk delivering solutions that inadvertently disrupt business workflows or produce inconsistent outcomes. with ddd, these differences are made explicit and respected. integration becomes purposeful: reflecting the real business distinctions, honoring domain-specific meanings, and enabling each system to interact in a way that upholds its own integrity. in this way, ddd transforms integration from reactive patchwork into a high-leverage activity - one that strengthens business agility, improves cross-team collaboration, and fosters architectural coherence across the enterprise. what ddd brings to the table organizations striving to modernize their enterprise integration strategies while staying deeply aligned with business needs will find domain-driven design not just useful - but essential. ddd does more than cleanly separate systems. it provides the structural, linguistic, organizational, and strategic foundation necessary for building integrations that are sustainable, commonly understood, and inherently aligned with business logic. the table below outlines the core strategic benefits of applying ddd to eai. these are not abstract gains - they are tangible enablers of agility, clarity, and enterprise-wide coherence: strategic benefit how ddd delivers it clarity by establishing a ubiquitous language, ddd ensures that integration teams speak the same language as business stakeholders. this shared vocabulary eliminates ambiguity in requirements and system behavior, drastically improving the quality and relevance of integration design. modularity bounded contexts help define explicit boundaries for domain logic. while boundary definition is a default requirement in broker or api-led architectures, ddd takes this further by aligning these boundaries with business meaning and ownership - not just technical partitioning. this modularity allows integration designs to respect organizational structures and domain expertise, promoting team autonomy and clarity of responsibility. more than just isolating models, bounded contexts ensure that each domain speaks its own language with precision. that clarity reduces model contamination, limits unintended dependencies, and enables domain-specific evolution. when integration touches multiple teams, systems, and semantics - as it often does in large enterprises - this rigor becomes essential to sustainable, scalable design maintainability because ddd encourages decentralized and self-contained models within each bounded context, changes in one area can be implemented with minimal risk to others. this makes evolving the system easier, faster, and less error-prone - especially important in fast-moving enterprises. however, this characteristic is also inherent in large and complex distributed ecosystems supported by integration platforms (e.g. broker, api-led architectural styles) regardless of ddd. the real added value of ddd in an eai context lies in how it structures these distributed systems for better business alignment and maintainability, not just in enabling independent changes. ddd's focus on clear boundaries and shared language facilitates more robust and understandable integrations, which is crucial in complex enterprise environments. resilience through domain events and a preference for eventual consistency, ddd promotes asynchronous communication. this results in loosely coupled systems that are more efficient, responsive, scalable, and tolerant of partial failures - qualities that are critical in modern, more and more distributed it ecosystems. context awareness ddd allows treating a given system differently depending on the context considered. this becomes of value when discussing domain problems with stakeholders. understanding the relationships between systems allows everyone to switch contextually between treating them in one instance as supplier, in other as provider, additionally acknowledging the form of the relationship by precisely describing the dependencies. incorporating ddd into your integration architecture is not a theoretical exercise. it is a pragmatic, proven approach to making interoperability a strategic asset - rather than a source of hidden complexity. organizations that adopt ddd in their integration efforts have a better capacity to respond faster to change, collaborate more effectively across teams, and build data flows and interactions that reflect - not fight - their business. the challenges at hand the convergence of ddd and eai isn’t without friction - particularly in environments with legacy systems and entrenched organizational silos. many legacy applications were not designed with modular boundaries in mind. they often expose tightly coupled, monolithic structures that blur domain responsibilities. as a result, trying to retrofit them into a clean bounded context model can be challenging. for example, an old erp system might handle both financial accounting and warehouse operations in a single database schema, making it difficult to isolate and evolve each domain independently. similarly, organizational silos create structural resistance to collaboration. different departments may maintain their own data models, processes, and terminologies, often optimized for local efficiency rather than cross-functional alignment. when integrating a crm used by sales with an invoicing system run by finance, conflicting definitions of a 'customer' or 'account' can derail integration efforts unless made explicit and resolved through shared language and clearly defined bounded contexts. these challenges are not exceptions - they are typical in large organizations. ddd does not magically eliminate them, but it offers a structured and proven approach for addressing them. by surfacing domain boundaries, enforcing language consistency, and promoting contextual integrity, ddd provides a roadmap to untangle legacy entanglements and align technical work with organizational and business reality. the effort is non-trivial, but the payoff is substantial: improved agility, cleaner integration boundaries, and systems that evolve sustainably over time. driving the change successful adoption of ddd in a business environment requires concentrated effort! it necessitates navigating existing complex system landscapes and overcoming established organizational structures. achieving the benefits of ddd, such as enhanced business alignment and system resilience, requires a deliberate and multifaceted approach that addresses not just technical practices but also cultural and collaborative aspects within the organization. iterative application start small with strategic subdomains, because trying to apply domain-driven design across an entire enterprise in a single effort is rarely feasible - especially in environments shaped by a wide range of diverse technologies, legacy platforms, and distributed ownership. an incremental approach allows organizations to experiment, validate, and grow capabilities over time while containing risk. for example, consider a global logistics company where the invoicing process spans multiple systems: a legacy billing platform, a regional erp, and a customer-facing portal. integration between these systems is brittle, with inconsistent definitions of billing events and manual reconciliation steps. instead of attempting a wholesale redesign, a team could identify the 'invoice dispute management' process as a high-impact subdomain. this area likely suffers from communication gaps, duplicated data, and poor traceability - making it a ripe candidate for ddd. by defining a bounded context around 'dispute management’ the team can establish ownership and involve domain experts to shape a ubiquitous language (e.g., \"claim reference\", \"invoice adjustment\", \"dispute window\"), and map clear relationships to provider and consumer systems. they could introduce better designed integration flows that would serve as anti-corruption layers, to isolate the legacy billing system’s model, while exposing normalized events (e.g. invoice issued) and on-demand services (e.g. get account) to the customer portal. this focused application of ddd delivers tangible business value: fewer failed handoffs, clearer accountability, and improved turnaround times for dispute resolution. once this approach to redefining interactions proves effective, it can serve as a blueprint for adjacent areas such as 'credit note issuance' or 'tax correction'. this way, ddd adoption grows iteratively - anchored in the reality of enterprise application integration, validated through working software, and scaled based on measurable success. cross-disciplinary collaboration business and tech must co-own models to ensure that integration initiatives are anchored in domain reality and not just in technical abstractions. successful enterprise application integration demands alignment across organizational boundaries. this requires forming teams that include not only software engineers and architects but also domain experts, business analysts, process owners, and representatives from key stakeholder systems. for example, in a large insurance company integrating claims processing between the customer portal, core insurance system, and payment provider, it is not sufficient for developers to simply expose or consume apis. business rules governing eligibility, fraud detection, and payout conditions are often embedded in departmental knowledge or legacy manuals. by forming a cross-functional team - comprising claims adjusters, underwriting staff, and backend engineers - the organization can collaboratively develop a domain model that faithfully captures these rules. together, they can define events like \"claim filed\", \"claim under review\", or \"payment authorized\", along with responsibilities, data structures, and boundary conditions. by this we are fostering a common understanding of the problem domain, that is aligned across various organizational levels and can be reflected in implementation with less effort. this approach ensures that both system behavior and integration logic reflect real-world scenarios, avoids costly misalignment, and builds shared ownership. without this collaborative effort, technical solutions risk becoming disconnected from business needs, leading to brittle integrations and rework downstream. in contrast, co-owned models produce efficient, adaptive integrations that are formed by how the business actually operates. organizational support ddd requires cultural as well as technical change because it fundamentally reshapes how teams think about software - not as code-first constructs, but as models of real business domains. this shift impacts roles, responsibilities, and communication patterns. without top-down endorsement and support, teams may struggle to prioritize domain modeling over urgent delivery tasks due to missing incentives aligned across departments. in the context of enterprise application integration, imagine a scenario where a company is integrating procurement, inventory, and finance systems across several global regions. each region has optimized its workflows for local efficiency, often at the expense of global coherence - a common architectural trade-off. applying ddd here demands not just technical refactoring, but organizational change: creating cross-regional modeling sessions, aligning terminology, and defining shared responsibilities across distributed teams. these efforts are difficult to facilitate without leadership support to allocate time for needed activities and recognize their strategic value, ddd risks being perceived as overhead rather than enablement. support might also include staffing dedicated domain facilitators, adjusting kpis to reward shared ownership, and funding the time needed for context mapping or ubiquitous language development. organizations that make space for these foundational activities benefit from better-aligned integration efforts, faster cross-team coordination, and systems that evolve cleanly with business growth. a strategic evolution in integration as integration architects, we see clearer than most that connecting systems is no longer a matter of technical necessity alone - it’s a strategic act. integration is the circulatory system of organizations, and if it’s misaligned, the entire enterprise risks moving in the wrong direction. throughout this article, we’ve looked at how domain-driven design (ddd) elevates enterprise application integration (eai) from tactical wiring to strategic modeling. we’ve seen how a shared ubiquitous language allows us to cut through confusion and build shared understanding across teams and systems. we’ve explored how bounded contexts help us protect the integrity of meaning in each domain - ensuring that sales, finance, logistics, and customer service each get to own their truth without stepping on each other’s toes. lastly, through context mapping, we’ve learned the importance of charting our dependencies with care, governing how systems communicate and evolve together. strategic clarity, modular design, resilience through loosening of coupling, and clear business alignment aren’t luxuries. they’re necessities if we want to build integration landscapes that can scale with our businesses and adapt to constant change. but this transformation doesn’t happen automatically. it takes work. ddd needs to be introduced iteratively, built on shared ownership across domains, and with secured backing to treat modeling as the strategic effort it is. when embedding ddd into our integration practices, we stop treating data like raw cargo and start treating it like structured thought. we move from transactional messaging to meaningful interaction. our integrations become coherent, explainable, and ready for whatever comes next. let’s not fool ourselves into thinking we can avoid complexity. we’re surrounded by it. what we can do is model it intentionally. we’ve all seen how pretending complexity isn’t there leads to brittle integrations and midnight failures. ddd gives us the vocabulary and structure to face it head-on. so here’s our call to action: let’s make complexity visible. let’s draw boundaries with intent. let’s model what matters. start with one problematic integration - a customer object with conflicting meanings, an order process that spans five systems - and model it clearly. collaborate. name things together. align expectations. from there, the benefits unfold naturally. we don’t just connect apis. we connect mental models, business logic, and decision flows. that’s what makes us integration architects - and that’s why ddd belongs in our toolkit. because enterprise application integration isn’t just about systems. it’s about connecting meaning - and shaping the future of how our organizations work."
  }
]
