[
  {
    "title": "What is Application Integration?",
    "raw": "what is application integration? this seems to be a very easy question, ain't it? but if you go to google and search the phrase \"application integration\", you will get a multitude of vendor-specific sites that try to define application integration to align with their marketing philosophy, not entirely saying what it is. some focus on heavy integration platforms, others around message brokers, while some also speak of api management solutions. all of those are elements that compose application integration and can be used or not, depending on what the requirements are and what the architecture of a particular application, system or it ecosystem is. why do we pose this question? after spending years in the field of application integration, we found that this is one of the most misunderstood and neglected areas of it. at the same time it can be one of the most complex and interesting challenges to solve within a modern it landscape. we currently find that application integration is not taught anywhere else than with the technology vendors. and as it is, all their curricula are usually limited by what they are actually selling, so some problems are either not addressed or solved in a vendor-specific way, which blurs the understanding of certain application integration patterns. they do not show trade-offs between different solutions and architectural approaches, leaving them to be done by the customer himself. we realized that there is an educational gap that needs to be bridged, so we decided to build an application integration compendium by providing perspective on different architectural styles, their trade-offs and integration patterns. a bit of history we believe that to understand any topic is foremost to understand the “why” behind it. in the case of application integration the “why” is hidden in its history and how communication developed over the years. why do we need to communicate? information has always been the key to everything humankind has done. the more information we had, the more power we could amass. nowadays it is estimated that on average we consume about 34gb of media daily (not accounting for other information, not screen-related!). most people do not do much with that information, as most of it is forgotten in the next few days. but just understanding the capacity of our brains to process and store data, kind of shows the \"why\". we are information hungry! we strive to exchange information all the time, because this is basically what we consume, whether it’s gossip, a tv drama, or actually useful knowledge! but going back to application integration. the most basic form of data exchange is a point-to-point (p2p) connection between two separate instances of some sort of software. in layman's terms this is a form of communication resembling a conversation in person between two people. this kind of communication was the first of its kind recognised in computer science, starting with the creation of arpanet and the first connection between computers in 1969 made with the ncp and later on in 1974 implementing the use of tcp. this is the protocol we rely on to this day, last updated in august 2022 as rfc 9293. the essence of application integration application integration is the strive to connect different applications into a larger ecosystem that functions as one. but what does that mean to function as one? in essence, it is the capability to provide the right data to the right place at the right time, where application integration defines how that data transfer should happen within those given parameters. as with everything that is done in it, this can be done in a multitude of different ways, some better, some worse. this is the responsibility of application integration architecture to answer how to build integrations between applications that are coherent with the overall architecture of the application or the entire it landscape. the development of application integration over the years. since 1969 the field of application integration has changed a lot, and while we still use point-to-point communication, each year brings technological advancements, with which we are able to distinguish a few application integration architectural styles that are common in the industry. we see it as important to highlight them as they roughly define what might be the focus of an application integration engineer. request - reply as mentioned above, with the creation of arpanet, request-reply is the most basic form of interoperation, commonly used to this day and utilized by all applications that want to exchange data. it is a simple, decentralized approach, which usually comes down to a single http call to an interface. this approach is commonly found in small applications or microservices environments (e.g. microservices chaining). this is the building block of all communication onwards, as all applications, even with middleware in place, rely on a request-reply relationship. benefits: fast and fairly easy to build, quick to deploy, cost effective in a small scale or short perspective, problems: complexity grows exponentially with each integration, heavy impact on application code and configuration with a medium or larger scale environment, tight coupling of applications (spaghetti architecture/big ball of mud), no reusability, low or no maintainability at a larger scale, low or no proper logging and monitoring, close to no capability to refactor or introduce changes fast, difficult or unavailable to work in a high available mode, zero trust architecture is hard to implement. file transfer as of the creation of arpanet, further software development followed with the creation of file transfer protocol in 1971. this is a direct result of the creation of telnet and netrjs as described in the rfc114. this protocol is among the first to provide indirect access to other systems. what that means is that it was the first protocol enabling abstracting of interfaces as well as decoupling of systems. while it enabled applications to exchange larger sets of data early on and is used until this day, it is usually not the best choice for interoperability. benefits: fast and fairly easy to build, quick to deploy, cost effective in a small scale or short term perspective, provides partial loose coupling between applications, problems: complexity grows exponentially with each integration, heavy impact on application code and configuration with a medium or large scale environment, may lose the benefit of decoupling alongside growing complexity, very limited reusability, low maintainability at larger scale, little or no proper logging and monitoring, low capability to refactor or introduce changes fast, difficult or incapable to work in a high available mode, zero trust architecture is hard to implement. broker application integration broker type architecture solves some key problems of the request-reply by introducing a centralized application that facilitates the integration needs in a more organized manner. this is done by managing connections and communication orchestration between different applications for them. unfortunately in certain cases this might not be enough to solve all problems, depending on what we're trying to achieve. this type of architecture is commonly used in small and medium enterprises, where the number of integrated systems is low (we usually say under twenty, but it is completely arbitrary). furthermore this approach can be divided into two distinct patterns: messaging service (e.g., jms, amqp, mqtt) - where an application message broker is introduced to facilitate all communication between systems on an asynchronous basis. this approach is common for event-driven architectures or some microservice system architectures, where individual containers communicate with each other on a publish-subscribe basis. integration broker - broker type integration platforms with dedicated tooling that provide single-purpose services tailored to the needs of each individual system. the services themselves have very limited reusability, but code reusability is enabled through code libraries and common processes. benefits: provides partial loose coupling of applications, code reusability possible, lower impact on application code and configuration, maintainable, but may be challenging at a larger scale, enables zero trust architecture to a limited extent, better error handling, problems: single point of failure (partially mitigated by multi-node ha), none or very limited services reusability, dedicated single purpose flows, limited capability to refactor or introduce changes fast, requires effort and resources to build, api-led architecture application integration api-led is an architectural concept that emerged as a refinement of service-oriented architecture (soa) that emerged in the late 1990s. while soa had its ups and downs, it evolved over the years to the current form. the original approach focused heavily on exposing business services first, with underlying layers of services to support them. api-led focuses around domain and data services, where the logic of a domain service is responsible for orchestrating the flow of data. data services expose data in a standardized manner to enable the creation of services providing different data objects or functions related to said objects (e.g. crud). although originally soa was built around soap services and xml, api-led embraced restful services as easily. before the emergence of cloud-native technologies, this approach struggled with similar problems as the broker architectures in terms of availability and performance. so far this is the most successful approach for supporting large scale enterprise application integration, especially in organizations that are not technology driven. benefits: provides loose coupling, major reusability (code, apis, services), supports zero trust architecture, inline with microservice system architecture, highly scalable, both horizontally and vertically, very low impact on application code and configuration, high capability to react to changes, mitigates it landscape complexity, problems: requires effort and resources to build, requires an investment in technology, licenses, additional tooling, with first integrations it may seem to increase complexity and cost (lowers with each reuse), requires highly trained integration architects to be successful, so what is application integration? in short, this is a quite complex field that addresses the aspects of planning, designing and implementation of the ways all technology we create, exchange information. with this short article we would like to invite you to explore the topic of application integration with us and dive into the complex details of different styles, approaches and architectural trade-offs that come with modern day challenges of an it landscape."
  },
  {
    "title": "Data Integration vs Application Integration",
    "raw": "data integration vs application integration the issue of “integration” working in any industry brings all sorts of terms and abbreviations that are specific to that particular industry or even specific to a narrow field of it. during our work day we more often use job-specific jargon than we think, and that jargon when used outside of the industry context may be a cause of a lot of misunderstandings or even serious problems. that is exactly the case with data integration and application integration. both of these terms use the word “integration”, which is commonly used by specialists in those two fields alone without the preceding word pointing to their respective terms. why is that a problem? people outside of that field mostly hear the word “integration” instead of the full term and because of that they tend to think that this is one and the same thing. to be honest even specialists in those fields make that mistake when they are not aware of the existence of the other field in it. an application integration specialist might think: “oh, data integration, yes, we move data from place to place, hence we integrate it! that is the same thing!”. this way they create a little bit of chaos and misunderstanding, that may sometimes result in later organizational problems, like a lower budget, no willingness to invest in technology (because we already did invest so much in this area - the other area, because one was mistaken for the other), etc.. what is data integration? so what actually is data integration and how does it differ from application integration? while researching the topic to give a complete answer, we stumbled upon a very descriptive analogy: “consider a room where different puzzle pieces are scattered all around, each with a picture on it. now, what do you do if you want to see the complete picture? you bring all those pieces together, connect them, and complete the puzzle, right?” data integration is the process of combining and harmonizing data from multiple sources (multiple pieces from a puzzle) to provide a unified view (the complete puzzle). at least this is the short and simple definition. but what does that actually mean? the focus of data integration is to make data, which is scattered among various systems, accessible, transformed, and deliverable in a consistent and meaningful manner across the whole organization for business intelligence (bi) as in reporting and data analysis, or use by other applications by utilizing the master data management functionalities. this typically involves data cleansing, transformation and synchronization. data integration is a crucial component in both traditional data warehouses and modern cloud-native solutions like lakehouses, facilitating seamless access and utilization of data across the organization. this means that the creators of data integration tools also provide certain capabilities to move data from sources and to consumers as this is often the requirement from customers who do not have their own application integration capabilities. this often involves setting up api connections, both hosting and calling, and directly accessing databases via odbc or jdbc connections. what is application integration then? as we already defined in the article “what is application integration?” - it is the strive to connect different applications into a larger ecosystem that functions as one and by that we mean it is the capability to provide the right data to the right place at the right time, where application integration defines how that data transfer should happen within those given parameters to achieve the near-real-time alignment across different applications. application integration tools, despite the main focus on connecting systems, are also capable of doing sophisticated data integration other than formatting or structuring. while these capabilities are usually very handy in solving specific problems, our advice is simple - please do not do data integration in the application integration layer! putting business logic over application integration platforms is usually not a good idea. unless you are working closely with a good data architect and you are capable of documenting all of the transformations properly with the documentation being easily searchable and accessible, this is usually a problem as application integration platforms are not considered to be business systems. unfortunately, when data integration is done on the application integration tools, it is usually done as a workaround to a problem that needs to be fixed later. as our experience shows, it is almost never fixed and remains in place, causing all sorts of maintenance issues, additional operational costs, further development problems, and serious production failures, with new releases that did not take this additional logic as a dependency to be tested. the trade-offs taking the above as it is, there is an overlap between data integration and application integration. to a certain extent it is true, as a lot of the data integration tools provide application integration capabilities that enable the access and delivery of data. on the other hand application integration technologies often also have specific data integration capabilities available. in that way they contribute to additional confusion as to what is the scope of this field. in the case of using data integration tools to facilitate application integration, as with all of the architecture questions, the answer to this problem would always be “it depends”. several factors could be decisive in whether or not the tool should handle application integration: organization size - if the organization is small, most of the time there is no budget to build a robust enterprise architecture with fancy features and proper division of responsibilities. in those cases “done is better than perfect”, as implementing a highly sophisticated architecture may not contribute to company growth and would be too costly at this point, it maturity - low maturity might contribute to additional complexity and make maintaining the it landscape troublesome. in large environments, adding another component may seem harmless, but it will further contribute to overall complexity that is unstructured and hard to maintain, also called “spaghetti architecture” or a “big ball of mud”, making it stiff and costly to maintain. in those cases, a clear division of responsibilities would be beneficial, preexisting application integration platforms and capabilities - if there are preexisting application integration tools available, this requires a validation, as too do they enable transport capabilities that are required by the data integration use cases (batch, on-demand, event-driven). if the capabilities are sufficient, they should be used, rather than the ones provided by the data integration tools, use cases of the data integration tools - depending on the use cases, whether it is an operational data hub, a business intelligence tool, or ad hoc usage by business applications and processes, the capabilities may differ with applied tools, as also some easy transformations, like formatting or structuring, may be done over the application integration and will be sufficient. architectural oversight as far as it’s true that every company has a different structure, roles and especially the names for them, it doesn't really matter in terms of the scope of responsibilities. these can be divided into the responsibilities of application integration and data integration, which in a perfect world would fall under the respective responsibilities of an integration architect and a data architect. while, like with tools, a company may have both, none or one or the other of the architectural roles, there are a few helpful guidelines to establish proper oversight. these can be summarized by asking four key questions: what, where, when and how. what? there needs to be a common understanding of data objects and how they are supposed to be used, like what are the edge cases for specific data points and rules for filtering them into specific scopes. what data objects are we working with? what is the business meaning of those data objects? what is the context of their use in a business process? what is the common meaning of these data objects in a specific business domain? what is the scope of the data object actually consumed? where? the data objects in a complex environment need to have a clear origin and there needs to be a clear understanding as to who (what system) is the consumer of these data objects. where is the data created and stored? where (what system) should the data be used? is there a data multi-mastery conflict? is the data split over several systems and need to be merged? when? knowing when along with what and where the data is used gives a full understanding of it in the business landscape, enabling the organization to provide proper data transport in a timely or even real-time manner based on actual needs. when is the data used by the business? is the data freshness an important feature? when should the data be moved and what triggers transport? when does it add value to the respective business domain? how? this is the underlying technical process of transporting data between systems, which involves data transformation (simple formatting over the application integration layer, or heavy changes over a data integration tool), orchestration as well as enrichment. this is when tools, protocols and formats should be discussed. can the data be classified as an event? is it pulled on demand? should it be a batch transfer? what is the orchestration and enrichment needed? so what is the difference? as stated, the difference between the fields of data integration and application integration can be easily distinguished at a logical level. the key problem lies in the fact that the tools used for both compensate in some way for the lack of capabilities of the other field. this makes it very difficult to clearly divide those capabilities in a live environment, as it requires a specific architectural skill set, as well as a high level of maturity. as the company and its it landscape grows, it is worth understanding this logical division and onboarding proper tools for each field before the capability gap becomes a burden for the business."
  },
  {
    "title": "Modern Application Integration Principles",
    "raw": "modern application integration principles improving the maturity working in corporate environments often entails dealing with technical debt that arose over the years of the company building up its it, merging with other companies, acquiring software, saas solutions, etc.. technical systems are often at the very end of this list, as they have lower priority in the eyes of the business, as they do not provide direct business value, at least not one that will be easily understood. application integration is unfortunately one of those technical systems that are often neglected, especially if the business is not technology driven (so not e.g. telecommunications, technology vendors, saas solution vendors). to improve maturity is to strive to improve applications, infrastructure, and networking, so in general a lot more than just front-end applications that the business uses. while improving it maturity is essentially a very broad topic, here we wish to focus on one of the first steps in improving the maturity of the application integration field. establishing application integration principles if we were to take a dive into architecture or strategy documents in different companies, we would probably find that the way principles are defined varies not only between companies, but also between these documents. to make our approach here a bit cleaner, let’s first try to define what a principle is. according to the merriam-webster dictionary it is: principle 1 a : a comprehensive and fundamental law, doctrine, or assumption b (1) : a rule or code of conduct (2) : habitual devotion to right principles a man of principle c : the laws or facts of nature underlying the working of an artificial device taking the first definition, we wish to propose to you some fundamental guidelines as principles to follow on your application integration journey for improvement. let’s dive in! business alignment principles focus on business value while application integration platforms are technical systems, the focus of their use should be on solving real business problems and contributing to the overall company goals. the key element here is to ask “why?”, so that making integrations for the integration's sake can be avoided. there needs to be a clearly defined goal to achieve with outlined benefits that the integration will deliver. alongside that, there is a need to measure the success of the integrations, which can be achieved by adding the right performance indicators (kpis) to track it. for example if the goal is to improve order processing, average order processing time might be a good statistic to track. with all this outlined before implementation, it is easier to properly prioritize integrations that will support the organization's growth and later on demonstrate the return on investment (roi). questions to consider: why are we applying application integration here? what problem are we solving? what can we measure to see the problem clearly and later measure the success? what is the business priority of this integration for now and the future? how does that translate to roi? alignment with business processes for application integration platforms to succeed, it is crucial for the specialists working on them to have a high-level overview of what the business process is, as that defines how different parts of the company interact with each other. this in turn enables us to identify data exchange points between business applications that support those processes. this approach can help with augmenting existing workflows, further automating manual tasks and data exchanges where it is applicable. on top of that it can also help identify and break down information silos within departments to additionally enhance collaboration. business process alignment helps build meaningful integrations that are more likely to be adopted by other departments that require the same or similar data sets. questions to consider: what business process are we supporting? what are the data exchange points (both manual and automated)? what manual tasks can be automated? what are the data sources? how can these integrations be reused? technical principles standardization standardization is a strive to limit the amount of work needed to create, deploy and operate it solutions. it can be applied on many levels, from architectural patterns, through the application of specific solutions, down to the code level. it saves time over development, reduces errors and makes troubleshooting easier. within the field of application integration this can be done by: applying proper integration patterns - to apply standard development practices, identify components that need to be built or extended, assess workload, using industry standard protocols - using common and well known protocols such as soap, rest, jms, amqp, grpc, etc., agreeing on data formats - to ensure seamless data exchange between applications, regardless of their internal data structures, using common code libraries - using code wrappers dramatically shortens development time, especially for connectivity, logging, monitoring, error handling and security, considering integration tools - having a chosen integration tool (e.g. middleware, messaging service) can streamline the development process and reduce the learning curve for developers. loose coupling introducing and facilitating loose coupling enables the applications to have minimal (or minimized) dependencies on each other. this in turn allows them to be developed, deployed and scaled independently without impacting the entire ecosystem. this improves fault tolerance and makes maintenance easier. loose coupling can be supported by: facilitating asynchronous communication (event or message-driven) using queues or pubsub mechanisms, using api gateways as a means of single point of entry, separating the consumer applications from api providers using abstraction layers that hide the complexity of accessing data, or provide, tailored on a need-to-know basis, service apis to consumers, reusability reusability focuses on creating application integration components and services in a way that their functionality can be reused. this reduces the development time and cost of future integrations as well as promotes consistency across your it landscape. this can be supported by: microservices architecture - functionality broken down into small and independent services makes them easily reusable, but requires governance over microservice chaining, as that might as well be a pitfall, orchestration driven service oriented architecture - for larger landscapes, where a large variety of applications and systems are found and microservices architecture is not applicable to all, it is worth considering abstracting those applications with tailored services, integration patterns - use well-established integration patterns alongside proven technological components like api gateways, event brokers, api or developers portal - where deployed apis are discoverable and can be reviewed, so that developers can first look for an existing api to reuse instead of building a new one. plan to scale and perform application integration services are prone to handle growing volumes of data and user demands. this means that they need special care in terms of performance as they facilitate communication between multiple applications. this is especially true for any e-commerce (or any other) businesses that experience peak sales periods, leading to heightened traffic and surges that need to be handled without compromising performance. this can be supported by: choosing the right integration pattern - it is worth considering if the communication should be decoupled by using asynchronous communication or streaming protocols for high-volume data exchanges, designing for horizontal scaling - alongside vertical scaling, designing for horizontal scaling is crucial, to create a fault-tolerant environment quickly adapting to peak traffic by utilizing automation. this also requires applications to be designed as compact as possible, possibly in a microservice architecture, to provide additional outscaling capabilities, optimizing performance - gathering and analyzing performance data to identify bottlenecks or other crucial points that can be improved in order to ensure smooth data flow. security security is a crucial aspect of all communication and data exchange, so implementing tailored and robust security measures is essential. sensitive information needs to be protected from unauthorized access, breaches, and manipulation. this can be supported by: authentication and authorization - implementing mechanisms to verify calling system identity and control access to data, data encryption - encrypting data in transit and at rest to shield it from unauthorized access and use, enabling “least privilege access” - this can be done by abstracting services and providing data need-to-know tailored apis, enabling centralized logging and monitoring - this supports anomaly detection and provides data for regular security audits that can tighten the security. operational principles logging and monitoring logging and monitoring of all events occurring over application integration components (integration platform, as well as clients and providers) are crucial to have a comprehensive overview of what is happening in the it landscape. early detection of errors and anomalies can prevent data inconsistencies. this can be supported by: metadata - provide proper metadata for each communication, such as various unique identifiers, timestamp, data origin, selected http headers, etc., as this provides crucial information to analyze any errors or anomalies, log aggregation tools - all logs, payloads (anonymised as needed) and metadata need to meet in a single aggregation tool to process it and make it available to be searched and viewed on demand, monitoring tools - use industry standard monitoring tools and dashboards to visualize log data and create custom dashboards with specific alert triggers to enable proactive operations. governance and ownership defining clear governance to oversee the entire api and integration life cycle, starting with planning and development, up to deployment and ongoing production maintenance. this ensures consistent practices between projects, promotes collaboration between different teams and avoids the uncontrolled sprawl of integrations between applications. adding clear ownership of each integration business and technical-wise to specific teams or individuals further ensures accountability for performance and maintenance. this can be further supported by: api contracts - adopting api first approach to application integration development. establishing a contract helps ensure that apis have consistency and are made to be reusable, alongside allowing parallel development, fast mock creation and testing, design guidelines - standardizing api design will ensure that apis built within the organization will be consistent, versioning and api lifecycle management - is enabling to constantly improve services offered both internally and externally in an organized manner, at the same time improving the stability of the it ecosystem, api discovery - allowing the apis to be discoverable in the ecosystem is crucial with the ever growing number of interfaces deployed. providing additional tooling to track dependencies of api consumption will be invaluable to analyze future impact, testing proactive testing throughout the integration lifecycle helps identify and rectify issues early on, preventing problems from cascading into production environments where they can disrupt business operations. often omitted or not adequately supported, it is a crucial part of the development lifecycle that improves the reliability not only of the integration solutions, but also of the applications used by business. this can be supported by: api testing - while in design, using the api-first approach, a mock service api can be provided to test with interface consumers to ensure the definitions, scope of data, or formatting are correct. unit testing - individual integration components and services can be unit tested to ensure they function as expected in isolation. system integration testing - focuses on verifying how different components of the solution interact and exchange data. this ensures a smooth flow of information across the entire integration. (end to end) user acceptance testing - tests the overall functionality of the business solution from a business perspective. this validates if the integration delivers the intended business value. other non-functional testing - that evaluates solution characteristics such as performance (stress testing, load testing), scalability, and security (pen-testing). furthermore, testing is strongly supported by standardization, as standardized components and interfaces are easier to test, and logging and monitoring, as gathering statistics and observing data flows during testing provides valuable insights. this can be further improved by: defining a testing strategy - plan your testing approach based on the complexity of the overall solution and the potential risks involved. automating testing - repetitive tests can be automated to save time and ensure consistency. this is especially useful for regression testing. however, manual testing remains essential for exploratory testing and validating business scenarios. continuous integration/continuous delivery (ci/cd) - integrating testing into your ci/cd pipeline to provide continuous feedback on the health of your integrations. documentation hardly any it specialists enjoy writing documentation. keeping a clear and up-to-date documentation for all integration flows is something crucial to an application integration platform success. this documentation should involve: technical specifications - these should involve use cases, edge cases, validation rules, volumetric data or predictions, orchestration logic, api definitions, or any custom made logic solution and data flow diagrams - these should show: logical views for solution components, bpmn defining the business processes, sequence diagrams defining the flow of information, low level design pattern diagrams for reference architecture, operational procedures for day to day operations and emergency: build and deploy procedures, logging and monitoring, root cause analysis, disaster recovery, service-level agreement (sla) proper documentation facilitates good knowledge transfer, simplifies onboarding for new team members, and ensures that there are smooth handovers between development and operations. when the integrations are well documented, they enable others to better understand, troubleshoot and scale not only the application integration layer, but also the rest of the it landscape. wrapping up the principles while the list we present above may seem a bit generic, it is important to start somewhere. these principles might not be specific enough for each company, but we believe that they cover a broad basis and provide guidelines for what should be followed to achieve a success with application integration."
  },
  {
    "title": "System vs Ecosystem Architectural Styles",
    "raw": "system vs ecosystem architectural styles many flavors of architecture when we think about architecture, we usually think of building a system. there are no right answers to how to build, most of the time we use our experience and expertise, or we look towards more advanced techniques like trade-off analysis to choose “the least worst” solution. luckily there are shortcuts that can help make decisions over what type of architectural style (or mix of those) you could use to build your system based on the architectural capabilities rather than functional behavior.   ip: neal ford, architecture foundations - styles and patterns  while it is beneficial to understand and know how to properly use and mix those architectural styles, when working with their capabilities and architectural characteristics, they focus mostly on a single domain system, monolithic or distributed alike. when we are working on enterprise application integration, the focus is usually on cross-domain functionality and enabling the data to be used in different contexts, often outside of their domain of origin (e.g. customer data from a customer domain may be needed by the order domain to fulfill orders or an anonymised version would be very useful to the marketing domain for market trend research). ecosystem abstraction layer cross-domain interoperability, while must be supported on a system level by exposing apis, encapsulating logic, should be addressed on a different abstraction level - ecosystem (or landscape).  if we consider a system to be defined as a set of elements or parts that are coherently organized and interconnected in a pattern or structure that produces a characteristic set of behaviors, often classified as its function or purpose, commonly focused around specific business needs, then what would be an ecosystem?  given that an ecosystem would be a higher abstraction layer from the system layer, this would mean that the aforementioned system becomes a part of the ecosystem alongside other systems with varied behaviours and functions. all of these systems considered as units, facilitating business needs, make up a large part of the business, company or enterprise themselves, creating its it landscape or as we name it, its ecosystem. now if we consider that the key aspect of the system is serving its functional purpose to facilitate business needs, then the main aspect of the ecosystem is how those systems interact with each other and cooperate to support the company as a whole.  this in itself brings different architectural styles that can be leveraged to ease the burdens of interoperability, amongst many other troubles.    looking from a perspective of an integration architect, that knows a few integration technologies, we can identify four distinct architectural styles, that can be used in various scenarios: point-to-point, event-driven architecture, integration broker, api-led architecture,  point-to-point an implicit architecture, where it is taken by usually without proper consideration simply by creating code. it is very common with small ecosystems of startups and is a very useful approach for quick wins. unfortunately with each new connection the complexity grows, adding to the number items on the maintenance list (e.g. credentials for each system, firewall rules, duplicate system access logic) and creating more and more coupling within the ecosystem as all of the logic is contained within the domain systems themselves. if the communication is standardized (which is rare) and proxied via an api management solution, this could be still valid for medium sized organizations.  the biggest pitfall of a point-to-point approach is that often it is not governed properly, leading to something called the spaghetti architecture, which is a state where most organizations struggle with their interoperability capabilities and seek help to resolve their maintenance issues, development bottlenecks or lack of data visibility. event-driven architecture event-driven architecture is a broker architecture, where an event-broker (or a message broker) is utilized to facilitate communication between domain systems, usually in an asynchronous fashion. in essence it is used similarly on a system level to the extent that some of the architectural patterns can be used exactly the same way.  the key to event-driven is to utilize events that are defined as facts that have occurred in the domain systems (e.g. a customer created an account, a new product bundle configuration was created, order was processed and sent). those events are transmitted through queues and/or topics, which are the transport layer that facilitates the communication. this architecture relies on business, error handling and orchestration or choreography logic to be contained within the domain systems and the event broker remains as an infrastructural system. integration broker integration brokers in comparison to point-to-point and event-driven architecture have an additional key value - the capability to create communication logic outside of the domain systems. this in turn gives this architectural style the capability to create an abstraction layer between domain systems and provide additional decoupling capabilities both with synchronous and asynchronous communication modes. this makes it a viable option for medium to large organizations. the fact that in this architecture we use a central mediator that facilitates transport logic and orchestration enables it also to provide more visibility and auditability within the ecosystem itself.  the key downside of this architecture is that while you can create many development standards, patterns and reuse, it does not have any architectural patterns as each integration flow is a custom workflow that supports a business process. while it is really hard to create flows that are entirely reusable, it is still possible in a specific set of circumstances (e.g. identical systems, with identical apis within different territories). api-led architecture api-led architecture, being a cousin to orchestration-driven service oriented architecture (odsoa), is the most mature and robust approach to enterprise application integration, enabling it to provide a great insight into the it landscape as well as individual systems and the data they provide or consume. the super power of this architecture is the capability to provide abstraction layers, that decouple the systems to an extent that in a mature it environment you can replace a whole domain system without impacting other systems in that it landscape.  the major pitfall of this architecture is that it is a lot more complex than its counterparts and requires a lot more effort and resources to implement at first. that said, the cost lowers over time as more and more data and services are exposed within the ecosystem creating reusability on a functional level, rather than only code. to be able to work with such an architecture devops capabilities are essential as with its complexity it will become hard to manage over time without proper automation to make the development, testing and deployment processes streamlined. architecture comparison it is very hard to compare those architectures between various implementations of many companies, as switching between them would require reimplementing a multitude of business systems to communicate differently based on the underlying business processes corresponding to the architectural style. that is why, while not impossible, it is not realistic to compare these architectural styles implementations within a standardized set of boundaries (a.k.a. quantitative analysis). it is however possible to provide a qualitative analysis, based on an overview of architectural characteristics of those architectural styles, which can be derived from professional experience, facilitating subject matter discussions, hosting focus groups and long analysis of those topics. that said, we would like to introduce such a comparison, which we created based on our experience and independent research. a more detailed description of the characteristics and the score for each architectural style will be provided in separate articles!   <<disclaimer>> before you jump into analyzing this table please note that while these are a result of our research, the actual outcomes in implementation may vary depending on several factors (e.g. organization and it maturity, used technology, ecosystem composition, business processes). this table should not be treated as a source of definitive truth, but rather as generalized guidelines by which you may choose an architectural style for ecosystem wide interoperability.  <</disclaimer>>  architectural characteristic name point-to-point spaghetti architecture event-driven architecture broker (mediator topology) api-led architecture if you wish to learn more about architectural styles for system architecture, you can do so by reading this book or attending a training/webinar run by mark richards and neal ford."
  },
  {
    "title": "Qualitative Analysis of Ecosystem Architectural Styles",
    "raw": "qualitative analysis of ecosystem architectural styles where do we begin? as we were preparing to write some of the articles and outlined our collective knowledge about enterprise application integration (eai), we identified many gaps in our understanding of the field we work in. some were assumptions made over time, others simply lack of working experience in certain areas, simply by a lack of customers that wanted to do something in a specific way. usually those gaps were easy to fill in, by discussion, a bit of research or reaching out to other specialists for an architectural sparring. one of the things we did not consider earlier was a proper division of architectural styles that could describe the various approaches eai specialists have towards solving interoperability issues within organizations. we tried to name those, but the names were always associated with some kind of technology and, to an unhealthy extent, their marketing practices and the jargon created by them. and that makes sense especially that no one is teaching eai in a technology agnostic way. so when we sat down to describe the architecture, it was in constant flux, with each new aspect of it creating doubt, attempts to find a better name or questioning if perhaps there is another architectural style that we’re missing. those struggles in the end lead us to create a first version of a comparison between architectural styles for it ecosystems. the bumpy way to set the tone we will not write here about the comparison itself, but rather about the process leading up to it. it was not an easy one to deal with. context switch when we usually think about architecture we mostly focus on system architecture (or design). that means we’re usually dealing with a single business domain and its subdomains. while when working on interoperability topics on the scale of an it ecosystem, the focus is a lot wider and cross-domain. if we consider this as a main factor when describing the architectural styles, this means that we need to figure out how we understand architectural characteristics that describe them. redefining architectural characteristics we took a moment to decide which architectural characteristics might be relevant for an it ecosystem wide. the key problem here is that there are numerous architectural characteristics and most people have their own definition of all of them. sometimes they do not match, not without a reason! to make them usable it is very hard to apply a generic definition, as a lot of them will be context specific, so applying something like iso25010 is not exactly doable. this itself was a process that required quite a few sessions to discuss the problem, with time in between to rethink what this all means. that said, let’s have a look at the definitions we have come up with for architectural characteristics that we have chosen for the ecosystem abstraction layer: cost related cost is an important factor in all it endeavors, as it often sets a specific direction as to what is feasible. at first we considered a single characteristic of cost to show in a simple way what implementing a specific architectural style would mean in terms of finance. while discussing it, we decided to split the cost into three distinct categories as that reflects better what might be the consequences of choosing a particular architectural style. development cost is understood as the cost of developing new integrations in the ecosystem, including the cost of changes needed to be made within business applications. this also includes the cost of testing new integrations and potential later changes. operational cost is understood as the cost of maintaining the interoperability between business systems within a specific architectural style. this includes the cost of maintaining, operating and monitoring the chosen eai tooling (integration platform, api management, event broker, devops tools, etc.) as well as the cost of providing a root cause analysis (rca) and bug fixing. architectural changes cost is understood as the cost of making significant changes to the ecosystem like replacing a whole application (e.g. moving from salesforce to m365 crm) or changing the data model of a communication and its impact. in essence it is about the degree of coupling between the eai tools and business systems connected through it. architectural and design time when discussing architectural characteristics around the ecosystem abstraction layer we divided them into two groups. starting with the architectural and design time considerations which relate to more static aspects of an it ecosystem and catering to interoperability, which relate to architecture and later design of specific solutions. abstraction is understood as the capability to hide complexity, logic and implementation details behind a well-defined interface, where high abstraction promotes loose coupling, that is mostly low level data coupling with protocol agnostic interoperability. this makes it easier to build systems in different technologies and makes it possible to evolve or even replace them independently. contract resilience is understood as the capability of a system (here mostly an integration platform) to adapt to changes in interface definitions, data models and data formats of the systems it interacts with. high contract resilience means that changes, even considered to be breaking in a point to point communication, might not impact the end consumer system. breaking changes are hidden and mitigated in the abstraction layers, e.g. a change adding mandatory fields in a response with a system, while reflected in api-led architecture in an adapter and service layers, might remain unmapped in the channel layer, not to change the customers contract). this is often critical to manage for ensuring continued interoperability in the face of system upgrades or changes in business requirements. simplicity is understood as the strive to minimize the number of components and/or interactions between them within an ecosystem, which in turn reduces the cognitive load of understanding its dependencies. by focusing on simplicity it is possible to better support maintainability and keep the ecosystem's architecture easier to understand and change. composability is understood as the capability of an architectural style to facilitate reuse, combining and recombining of data, code, libraries up to whole services or applications to provide more business value and new functionality, as well as the degree to which it is easy to do. focusing on composability makes the creation of flexible solutions easier by leveraging the existing functionalities across the ecosystem. extensibility is understood as the capability to add new features, systems, integrations without impacting or with limited impact when extending the existing functionality. this is not only understood as architectural extensibility where structures like messaging topics enable to add new consumers easily, but also the ease at which new systems can be integrated into the ecosystem (e.g. being protocol agnostic, providing api abstraction). it is the measure of the ecosystem's ability to grow while maintaining interoperability between business systems and domains. operational the second group of architectural characteristics that we have considered within our research into ecosystem architectural styles is tackling the dynamic aspects of what happens between it systems, so in essence topics surrounding the implementation and use of it systems and the interaction with integration platforms. testability is understood as the capability to easily and sufficiently test the business processes that rely on interoperability to complete successfully. this involves a wide range of tests, including regression testing of existing features and testing of new functionalities, starting with unit testing the integrated systems and integration flows, through end-to-end connectivity testing, user acceptance testing and up to performance and load testing. scalability is understood as the capability of the ecosystem to support business systems and handle increased load or volume of traffic without a significant performance degradation. this is critical for ensuring that the ecosystem can accommodate further growth and increased use of shared services. performance is understood as the ecosystem's capacity to handle high-volume data exchange with minimal latency. this is essential for ensuring that integrated systems can respond promptly and effectively, thereby supporting efficient business processes and a positive user experience. security is understood as the ecosystem's capability to prevent unauthorized access and data breaches by providing sufficient countermeasures. this can be done by addressing least privileged access, proper encryption, adhering to zero trust, etc.. it is essential for maintaining data integrity and confidentiality as data flows across system boundaries. observability is understood as the ecosystem’s capability to provide metadata and logs that enables the operations team to understand and observe the ecosystem’s behavior in real-time to support troubleshooting, operational and security anomaly detection and identifying performance bottlenecks. auditability is understood as the capability of the ecosystem to provide evidence, track and audit the ecosystem's activity for compliance and security purposes, by creating visibility into data flows and interactions between various domain systems. the next step with the context based definitions of architectural characteristics we were able to have proper discussions about the qualities of specific architectural styles for ecosystems. but first we need to define those architectural styles to be able to compare them. at first we defined this simply as: point to point - a simple architectural style for interoperability, where there is no need for mediators between systems. while some of you might question whether it is even an architectural style, we categorized this as an implicit style, where the architecture is assumed as it is developed. event-driven architecture - an ecosystem representation of a common system design architecture, governed mostly by the same rules. orchestration driven service-oriented architecture - an architectural style that is known to many integration architects and developers that spent years in the industry, as many practices of modern integration platforms derived from soa. as we discussed these architectural styles and their implementations we have encountered over the years, we realized that this list was somewhat wrong and did not match correctly with our experience and what we see happening in the industry today. we were missing at least one style in the comparison. digging further into the topic, we realized that what we consider service-oriented architecture, has evolved quite much over the years and can no longer be called that. as a result we have created a new list: point to point - as defined above, broker architectures: event-driven architecture - an architectural style that supports asynchronous communication by providing a technical broker to facilitate communication between systems, integration broker architecture - an architectural style that provides workflow like capabilities for application integration flows, enabling easily both synchronous and asynchronous communication (when paired with a message or event broker), api-led architecture - a somewhat distant relative to service-oriented architecture that provides capabilities to compose and reuse services built in three different abstraction layers of an integration platform. it enables both synchronous and asynchronous capabilities (when paired with a message or event broker), additionally we decided to add “spaghetti architecture” to the comparison, just to show how neglecting interoperability issues and architectural governance on an ecosystem abstraction level can change architectural characteristics of the ecosystem. this part was a long thought exercise that started with drafting an initial proposal of a comparison and then submitting it to several reviews with multiple independent experts in the field of eai or system architecture that we reached out to via linkedin, conference networking or previous work engagements. each review was followed by a sparring session, where the understanding of those architectural characteristics was further refined, a common dictionary established and the qualitative scoring on each characteristic for each architectural style was discussed, challenged and reassigned based on arguments provided by each side of the discussion. the changes made to the comparison were then communicated to other reviewers, further discussed and acknowledged. conclusion as a result of this long process of discussions, arguments and rethinking what architecture is on a level of an ecosystem abstraction we have finally completed a first version of the architectural styles qualitative analysis."
  },
  {
    "title": "Synchronous vs Asynchronous",
    "raw": "synchronous vs asynchronous? lifeblood of modern communication while communication was always the key to success for humankind, since 1969 and the creation of arpanet, communication has become something very different, quicker and more robust with every single decade, ever speeding up. the forecasts for 2025 are that we will produce 147 zb of data over the year, this means about 407200000 tb a day. to put that in perspective, youtube alone currently uses approximately 440000 tb daily, and your message on whatsapp is under the size of 5kb = 5120 characters (1 tb = 1073741824 kb, so around 214.7 million messages). we rely on data being moved around between systems and users. we consume more and more of it, and for most users this is just viewing things in a browser window on the screen. but deeper in the depths of the it systems, architects and developers need to make the decisions on how to move the relevant data between applications, for those users to see. communication mode while there are many ways to communicate between systems, using apis, integration platforms, message brokers, file transfers, etc., in the implementation details it comes down to a choice between two modes of communication: synchronous and asynchronous. let’s try to define those in simple terms, starting with a proper definition: synchronous 1 : happening, existing, or arising at precisely the same time 2 : recurring or operating at exactly the same periods [...] looking at those two definitions, if we try to find a real-life analog to a communication that is synchronous, the thing that comes to mind is a dialog. a situation where there are at least two parties involved in an exchange of information, where saying something usually is followed by waiting and actively listening to a response. asynchronous 1 : not simultaneous or concurrent in time : not synchronous [...] if this communication is defined as “not concurrent in time”, this means that the two participants do not communicate at the same time, so while one transmits, this is where the communication ends for the producer, and then the consumer becomes active. if the message is not picked up immediately, it is either lost or stored somewhere. a real-life analog to an asynchronous communication would be listening to a radio broadcast or exchanging messages over mail. blocking vs non-blocking communication one of the key concepts to understand when dealing with communication modes and at the same time the key differentiator between synchronous and asynchronous communication is understanding whether the communication is blocking or non-blocking. synchronous is blocking what does it mean that a communication is “blocking”? as defined above about synchronous communication, it is happening for all participants at the same time, where we have a sequence of requests and responses. the key aspect is that the party requesting is waiting for the response, so the resources used to make the call (ram, thread, etc.) are locked and the process will not move on until it gets a response or times out. the key consideration here is that the blocking time is directly dependent on the called application and the time that it needs to process the request to produce a response. this can be also applied to a chain dependency of multiple services calling each other (e.g. microservice chaining), so the time needed is a sum of processing times of those services or applications. while processing times may differ between implementations, the important consideration is how long the calling system can wait for the response (license limitations, process execution time, user experience, etc.) blocking communication is beneficial when feedback is needed as fast as possible. asynchronous is non-blocking on the other hand we have the “non-blocking” communication, which, by elimination, is asynchronous. based on what we already know about asynchronous communication, the participants are not usually active in the data exchange at the same time. they are often separated by a message broker, file server or other persistence, which provides a separation between the provider and the consumer making it possible for them to behave this way. while sending a message to a message broker is also a synchronous operation as the message broker responds with an acknowledgement of getting the message, the application sending that message does not wait for the consuming application to process the message. meaning that with asynchronous messaging the time to send out a message and get an acknowledgement is reduced, so this may lead to a significant improvement in processing speed, but the trade-off is the lack of immediate response. data volume when choosing the communication mode it is important to consider payload sizes and hope they might impact communication performance as well as application responsiveness. synchronous mode when considering a synchronous communication mode it is important to note that by making a synchronous call we are creating coupling, resulting in a dependency impacting performance and responsiveness. the larger the payload or processing time needed to produce it the bigger the impact on latency, that might be resulting in timeouts if not handled properly. looking at the ranges of data volume the impact will be as follows: messages below 10 kb or simple data retrieval will usually have a negligible impact on performance, messages between 10 kb and 100 kb or minor processing, like simple search, might cause problems if timeouts are not managed properly, messages above 100 kb or complex processing, e.g. patterns matching, while it will still work when communication parameters on both sides are properly managed, the latency might not be acceptable for most use cases, causing various responsiveness and performance issues. asynchronous mode asynchronous communication allows a lot more flexibility in handling payload sizes or processing time. because it provides decoupling, there is no direct dependency, which mitigates all impact on latency, performance and responsiveness that comes with synchronous communication. each system communicating will be able to fine tune this separately. larger payloads (files, batch transfers) will also benefit from asynchronous communication as the decoupling will allow longer processing times by creating a buffer for the consumer. use cases both synchronous and asynchronous communication has lots of applicable uses. it is impossible to mention all of them, as every single piece of equipment nowadays communicates, but we can group them into specific categories: sync list common use cases: real-time interactions, transactional communication (related to acid), situations where immediate feedback is essential, like data retrieval if data has to be presented to the user, async list common use cases: sending notifications or alerts background processing of tasks data synchronization between systems (eventual consistency) situations where decoupling and scalability are important, handling long processing times in one of the systems so which communication mode should you choose? well, to be honest we cannot really tell out. the key is to do a proper analysis of what the requirements are and choose based on those findings. synchronous and asynchronous communications differ and have different trade-offs to consider. the key aspects to analyze should be: is an immediate response needed? what is the payload size? what is the required responsiveness and tolerated latency? keep in mind that there are many more aspects related to choosing a communication mode, but they are also tied to specific architectural styles and patterns (e.g. there will be different issues between synchronous and asynchronous call within an api-led architecture than event-driven architecture) and we will be touching upon those as we explore those topics!"
  },
  {
    "title": "From Point-to-point to Spaghetti Architecture",
    "raw": "from point-to-point to spaghetti architecture starting with some basics while we’d like to discuss enterprise application integration it is important to start with the most basic approach to data exchange and why is it crucial to understand it fully before jumping into more complex architectural styles that support ecosystem interoperability. point-to-point as an architectural approach is exactly that. this is the most commonly known approach used by all developers and architects. so let’s take the time to explore the benefits and pitfalls of this architectural style. what is point-to-point (p2p)? to put this simply, point-to-point should be understood as a system or an application directly calling a different systems or applications api understood broadly. it can be a jdbc procedure call, an invocation of a rest or soap api, etc.. commonly found in every kind of environment, it is impossible to avoid, as it serves as a basis for all communication. the lack of a mediator forces all of the aspects of communication upon the business systems. those become responsible for the technical implementation of the communication, its security, governance over data and data models, etc.. implicit architecture since point-to-point, as a dominant form of interoperability in an ecosystem interoperability focus is mostly common within young, small or not tech driven organizations (one could say those with low it maturity), it is often created as a side effect of lack of architectural governance. in essence, it happens by accident, a sheer need to integrate applications that were introduced into the ecosystem without interoperability in mind, for example: “ok, so we have this custom application which we built that helps us manage assigning staff to specific client projects, but it would be so much easier if we would not have to input employee details by hand but get them from our workday instance!”. that way a p2p connection is created with every similar need and the interoperability is not governed but driven by at-the-time business needs. the road to chaos point-to-point, among other things, has a big pitfall that often, when unmanaged, as described above, leads to severe complexity that can even cripple business. this is often referred to as spaghetti architecture, because of the large quantity of tangled up interactions that, when charted out, often look more like a dish of spaghetti, rather than a diagram showing actual communication. we take this specific “architecture” as a reference point of what we do not want in our ecosystem and to be able to explain how this complexity can be resolved by various approaches, architectural styles and patterns. image source qualitative analysis before point-to-point leads us to complexity that could kill the business, it is worthwhile to explore what may be its cause and what other pitfalls may be awaiting us when implementing this architectural style. we can avoid some of the pitfalls by understanding what are the key architectural characteristics and how they are relevant in this setting. for that we will be using a comparison table that was produced through a qualitative analysis of architectural styles taking several architectural characteristics into account. if you would like to learn more about this analysis or read how we define those characteristics, you can do so by reading this article, where we explain how this comparison was created. architectural characteristic name point-to-point spaghetti architecture event-driven architecture broker (mediator topology) api-led architecture development cost \uD83D\uDCB2 \uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 operational cost \uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 architectural changes cost \uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2 abstraction ⭐ ⭐ ⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐⭐ contract resilience ⭐ ⭐ ⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ simplicity ⭐⭐⭐⭐⭐ ⭐ ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐ composability ⭐ ⭐ ⭐⭐⭐ ⭐⭐ ⭐⭐⭐⭐⭐ extensibility ⭐ ⭐ ⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ testability ⭐⭐⭐⭐⭐ ⭐ ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐ scalability ⭐⭐ ⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ performance ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐ security ⭐⭐⭐ ⭐ ⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ observability ⭐⭐ ⭐ ⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐⭐ auditability ⭐⭐ ⭐ ⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ cost analysis let’s start with the costs of point-to-point architectural style. there is no middleware used in this architectural style and it relies on direct communication between two applications, which means that development cost as well as operational cost are pushed onto the specific systems that wish to exchange data. what that means is that in this scenario we do not have any additional licensing costs, no specialized development or support team responsible for application integration. this lands p2p with a score of a single \uD83D\uDCB2 in the development and operational costs. architectural change cost is the cost that will vary the most depending on the size of the ecosystem and the number of interactions within it. the higher the number of systems and applications and the dynamic dependencies between them, the higher the architectural change cost will be. but if we assume a positive scenario that there actually is some governance (e.g. use of standardized, industry-specific protocols) over the ecosystem and its interoperability, that will limit the number of interactions between systems and as needed switch to a different architectural style, then it will not be allowed to grow to become a spaghetti architecture. in that case it will be feasible to introduce architectural changes with ease. this in turn gives p2p a single \uD83D\uDCB2 score in this category. architectural and design time analysis the first group of architectural characteristics that we can take a look at when analyzing architectural styles describes the qualities of the blueprint of the architecture. if we’d take a closer look at what makes point-to-point really good, we’d find that it is first of all its simplicity (5⭐) . this means that when implementing this architectural style, it will be quite simple to understand and test, as there is no added complexity of a mediator between the business systems. this comes with a specific corollary that this architectural style becomes more complex the bigger it grows, which means that if left ungoverned it will become the spaghetti architecture. the architectural downsides of this architectural style come from the same source as the benefits. having systems or applications interacting directly with each other means that we face tight coupling between them. the level of coupling that we’d usually like is “data coupling”, where at this point we’re facing not only “stamp coupling”, as in being bound by a specific data model, but also “external coupling” that forces the calling application into using a specific communication protocol. this in turn means that architectural characteristics such as abstraction (⭐) and contract resilience (⭐) will suffer greatly. this in turn impacts extensibility (⭐), because it is very hard to add new features, functionality and systems, when each time an integration is needed, the system engineers need to learn a substantial piece of a different system and the way it’s implemented. lastly, due to all of the previous downsides, composability (⭐) as an architectural characteristic also suffers, because as far as it is doable to reuse an api exposed by a specific business application, composing new functionality from existing components is pushed into the responsibility of the calling systems, which means that all those ned functionalities are encapsulated within those systems and not reusable by other applications. operational analysis the second group of architectural characteristics we can take a look at focuses around operational aspects of the ecosystem. the first and biggest benefit of a point-to-point architectural style is its testability (5⭐). due to the high simplicity of the ecosystem and the fact that the responsibility of testing lies solely with the business applications, testing the communication, analyzing the results and being able to report outcomes is fairly quick. unfortunately this ties in again to the aforementioned corollary, where with the growth of this architectural style, the complexity heavily impacts testability, which again is reflected over the spaghetti architecture qualitative analysis. the second operational aspect of this style that works well is performance (4⭐). this again relates to the lack of mediators, so as the communication is simpler, the latencies for communication are dependent only on the response time of the called systems. this can be managed and optimized, so the performance of the ecosystem is overall high. the reason that this is not the highest score is that this style usually does not enable efficient asynchronous communication.. the operational architectural characteristics that fall behind in the trade-off in this architectural style are observability (2⭐) and auditability (2⭐). these heavily rely on how the business systems are implemented and what metadata they allow to be extracted out of them. in essence if we’re dealing with legacy custom systems, they might be very hard to refactor to provide sufficient amounts of metadata (logs, correlation ids, etc.). a similar situation might happen with software as a service (saas) solutions, where customizing those to provide the needed information might be a costly endeavor, severely impacting those characteristics. furthermore scalability (2⭐) as an architectural characteristic is also low, because communication done over a synchronous protocol, that is the cornerstone of point-to-point, does not support horizontal scalability sufficiently, especially due to the fact that it requires additional tools to be introduced to support features such as load-balancing. that hinders the capability of the ecosystem to have automated scaling, which is also done differently with each business system. lastly, security (3⭐) in this architectural style is scored as average due to the fact that usually there is little to no overarching governance. this means that implementing least privileged access or following zero trust guidelines might be hard to achieve if not impossible. conclusions point-to-point is and will remain a valid architectural style for the ecosystem level. when implementing this, one should remember that it is not an architecture defined for extensive growth. it is valid for start-ups, small projects or data processing pipelines, where 2 or more systems can be treated as one from the perspective of the \"rest of the world\" in context of a given process. without proper governance it might quickly turn into a spaghetti architecture, leading to unwanted complexity and negative impact on business processes and their execution."
  },
  {
    "title": "Degrees of coupling in IT ecosystems",
    "raw": "degrees of coupling in it ecosystems eai and coupling one of the key problems enterprise application integration (eai) is addressing in various ways is coupling. while we explore how we can address it by applying specific architectural styles and patterns in other articles, we’d like to explore what is coupling and define, from a variety of types, which coupling types are relevant in ecosystem interoperability topics. what is coupling? to take a closer look at coupling, let’s start by looking at a few definitions:  in software engineering, coupling is the degree of interdependence between software modules; a measure of how closely connected two routines or modules are; the strength of the relationships between modules. coupling is not binary but it is multi-dimensional. wikipedia  coupling describes the independent variability of connected systems, i.e., whether a change in system a affects system b. if it does, a and b are coupled. gregor hohpe  if a change in one system, service or component, defined broadly, might facilitate a change in another system, service or component, they are coupled. mark richards  if we consider the above definitions, there are two specific statements we can derive from them, showing the dual nature of coupling:  no system is free from coupling, as there is always some sort of a dependency, e.g. to the operating system, infrastructure or other components, modules or applications. it is a matter of understanding the strength, distance and volatility of coupling.  therefore a system, on its own, cannot be decoupled from its environment. coupling is binary by design, meaning it is either intentional or implicit. components and applications are either coupled to each other or not, meaning they either have or do not have a dependency to one another (e.g. two systems exchanging data with each other have a dependency, whereas a mobile game on your phone and your hr system at your workplace most likely have no dependencies between each other). in most cases, those dependencies can be derived from architectural blueprints, architectural decision records, documentation, or other knowledge sources. however, the challenge arises when those dependencies are hidden or indirect, so we cannot rule out their existence, especially if we have limited knowledge of the (eco)system at hand.  therefore the system can be coupled or decoupled with another system.  in essence coupling cannot be treated as an on-off switch, it has multiple dimensions and degrees along those. furthermore coupling is contextual, which means that as always it depends on several factors whether or not a specific type or degree of coupling is wanted. so in essence any time you see a line between two boxes in a diagram depicting something about architecture or system design, that is some sort of a coupling that can be described. different types and characteristics of coupling there are various types of coupling, each with distinct dimensions and strengths. discussing all of the coupling possibilities would be a very long and difficult process worthy of a book. fortunately that work has already been done by vlad khononov in balancing coupling in software design. as we are going to dive into the topic we will be focusing only on those that bear significant relevance for interoperability on an ecosystem abstraction level.   as we are diving into architectural styles and patterns in eai we can witness all types and forms of coupling. some of them are very much wanted, others implicit and manageable, while some will be considered undesirable in specific conditions. let’s take a moment here to classify them: types of coupling while there are various types of coupling, we discuss them on different abstraction levels. some may be seen in the fine details of designs, others on a much broader scale. we can clearly differentiate between two distinct groups of coupling types based on where they derive from: architectural (static) coupling - describes dependencies that are directly deriving from the design in place. contract - described by the agreed scope of data that will be transferred between involved parties. while contracts can be one-sided or negotiated between parties, all involved are coupled this way, data format and type - deriving from specific contracts between modules and applications where data models are described in conjunction with respective formats, semantic - the generalized understanding and meaning of transferred data, both business and metadata, on the level of users, business processes, purpose and usage,  interaction - is the communication done in the synchronous or asynchronous fashion, conversation - the specifics of how the communication happens (e.g. pagination, caching, retry policies, technical and business error handling), operational (dynamic) coupling - describes dependencies that result from operating the systems and applications, and the behavior patterns that come with how the users use them. temporal - the sequence of events, how the logic is executed, whether or not there are race conditions, what is the processing time, are there other processes dependent on another process being finished (might lead to pathological coupling), interaction - extending the static coupling, this relates to the implementation of the interaction using, or being forced to use a specific protocol and how the system is dependent on it, technology - the inner workings of a specific technology used to implement the system, the way it works, how it interacts with the underlying operating system and infrastructure, location - the geographic location of where the systems are deployed, where are they available, what is the network configuration between them, semantic - extending the architectural aspect of this coupling, the meaning of an instance of data being exchanged and how that data influences the behavior of the implementation through data validation, transformation and error handling, coupling direction while discussing coupling it is important to also understand the direction of coupling, meaning which component is the one providing a dependency, and which one is consuming it. this can be later used to provide two specific measures: efferent coupling - a measure of how many components the analyzed component is dependent on, afferent - a measure of how many components are dependent on the analyzed component, measuring efferent and afferent coupling in the ecosystem is a useful tool to spot potential bottlenecks, components that might need to be split functionally or operated with more care, e.g. by applying automated scaling.  coupling strength one of the three main characteristics of coupling is its strength. over the years we were accustomed to having somewhat meaningless terms like “decoupled” or “loosely coupled”, which could have been interpreted in various ways depending on the context of the conversation or the individual interpreting them. that’s why a more precise classification is required. strength in this case would be classified as one of the following ordered from weakest to strongest: contract - components are contract-coupled if they rely on communicating through a data model that is integration specific. the contract outlines the terms and conditions of collaboration and is usually defined as a communication protocol between two systems.  model - happens when the data model related to a business domain and its data model is shared by multiple applications/systems, e.g. internal data model either used for communication or reused in the processes of more than one system. functional - two modules are functionally coupled if their functionalities are interrelated, which means that if the business requirements change and so does functionality in an application, the coupled application will most likely be affected, temporal (sequential) coupling - when logic needs to be executed in a specific order (may be also known as pathological coupling), transactional coupling - when several operations have to be carried out as a single unit of work, thus a transaction, symmetric coupling - happens when two or more applications implement the same functionality independently, giving the same goal by means of different implementations. when the requirements for the shared behaviour change, all applications need to reimplement the logic at the same time at risk of introducing bugs (e.g. mismatch in data cohesion), intrusive - instead of communicating through public interfaces, the client  communicates through, and thus depends on, the implementation details of the provider,  when it comes to eai, the usual and wanted coupling strength is contract coupling which is supported by the capability of integration tools and platforms to provide abstraction. model coupling may at times occur if the data models are not managed properly and leak through the abstraction layers to contracts. moving on to functional coupling, none of those types occur solely in the integration layers, as they derive from the business process logic, usually (hopefully) outside of the integration platform, which in turn only facilitates these coupling forms. coupling distance while it is possible to create monolithic software that is just a single block of statements with a lot of ifs, that is not the reality we are currently facing. over time various types of encapsulation were introduced to provide boundaries. in essence, coupling distance is bound to how closely related are the dependencies. we can distinguish distance levels against which we can measure coupling: statements, methods, objects, namespace or package, libraries, services, systems or applications,  there are two key, linear qualities we should track related to this: cost of change over distance - the greater the distance between components that have to change together, the higher the effort, and with that the cost of the shared change, e.g. changing a small statement in a method is far less costly than changing two systems that are coupled with each other with a contract coupling influenced by new requirements and changes to the data models. the cost is usually proportionate to the distance, lifecycle coupling - all software is involved in some sort of a lifecycle that may have multiple stages, ranging from requirements gathering, through design, implementation, testing, deployment and finally maintenance and updates. the lifecycle coupling is usually inversely proportional to the distance, coupling volatility the last characteristic that is crucial in understanding coupling, and especially the risks involved with certain tighter coupling instances, is volatility. this is a bit of a compound characteristic of two particular qualities: frequency of changes - how often a coupled component changes within the software development lifecycle due to new features, floating requirements, bugs and fixes, customization, etc., impact - how much of a ripple effect a change is going to cause, and by that any cascading additional changes that will influence other components and teams working on them whether by chance or deliberately, volatility reshapes the focus of investigating coupling to discuss its influence on the software in time, as well as how this software may and will evolve over time. focusing on volatility invokes a discussion about whether or not certain instances of very tight coupling are acceptable, or can we allow ourselves to cut-corners and simplify design (introducing more coupling),  where for example a solution might be only temporary. conclusions coupling is a multifaceted concept that is quite difficult to grasp fully, and it is an intrinsic part of any it system. while it is impossible to completely eliminate coupling, understanding its various types and degrees is crucial for designing and maintaining efficient, adaptable, and maintainable systems. by carefully considering factors such as architectural characteristics, data models and contracts, interaction between components, operational considerations, business logic, as well as coupling strength, distance, and volatility, architects and developers can make informed decisions to minimize unnecessary dependencies and promote system resilience."
  },
  {
    "title": "Event-Driven Architecture for IT ecosystems",
    "raw": "trying to avoid the italian cuisine problems modern businesses rely on a diverse ecosystem of applications, including custom-built systems and cloud-based services. integrating these disparate systems can be challenging, often leading to tightly coupled (spaghetti) integrations that are difficult to maintain and scale. event-driven architecture (eda) offers a more flexible approach to interoperability. by leveraging events as the foundation for communication, eda enables systems to exchange data more loosely, improving maintainability and facilitating easier adaptation to changing business requirements. at the beginning there was messaging there are various types of objects and information that pass through message brokers. to understand event-driven architecture (eda) it is crucial to be able to navigate easily between them: message - is the core object of communication, which can be literally anything. following the examples given by jacqui read at ddd europe 2024 in the talk “every event everywhere all at once”, this might as well be “high five! well done!”. a message can have more narrowly refined subtypes that serve specific interoperability function: command - is a subtype of a message where we tell the other side of the communication to do something, like “create customer”, this literally means that the data set transferred between the systems is sent with the intent to invoke an action on the receiving end, state - is another subtype, with a passive message, transferring a state of something, telling us for example “ticket 123qwe is in progress”, this is often reflected by a very light payload, sent periodically, most commonly containing an identifier, described state and some metadata, event - is another subtype of message that states a fact that occurred. this means that the message sent between the systems contains metadata and a payload (or identifier of one) that describe something that happened business or operations wise. but since we’ll be focusing on it a little bit more, we’ll get back to it in a moment, given the above, we can notice that a very common mistake happens, that there is a tendency to name something eda, where it is not actually event-driven, so it uses message types that are not events. to make this clear and before we take a deep dive into eda, we need to define what is an event and further explore the other types of “objects” that can go through a message broker. events vs the world let’s start with a generic definition. as we already established, the root of all things going through a message broker is a message. it is the most simple option and it has the least boundaries of what it can be. simply put a string as payload and that is your message! but events are not just some random messages. they are specialized in terms of the contents and meaning, so let’s look at a few definitions: event 1 a : something that happens : occurrence b : a noteworthy happening [...] essentially in programming an event is the fact that something happened. which means it would be associated with verbs in the past tense like: created, started, completed, updated, failed, etc.. this is reflected in the messages metadata and payload. for example, if you use topic hierarchy the metadata would be hidden in the name of the topic, e.g.: /eshop/order/created/v1/{orderid}, and the payload would be the details of that order, even if it is just an id of a record for later polling. as mentioned earlier, there are other types of messages that are common to it ecosystems. for comparison, the implementation philosophy of soap was commands and responses to them. the messages were sent to an api that was focused around command operations based on imperative verbs, like get, set, update, which resulted in names like e.g. getcustomer, updateorder, setstatus. similarly in rest, while the api represents the resource, the imperative verbs of http methods (get, post, put, patch, delete) represent the command nature of the invoked operation. commands imply that the calling system is in control of the transactional behavior of the api provider. another option is to use messages to transfer state, where it can be described as statuses like those found on a common kanban board (e.g. to do, in progress, done). and lastly messages can transfer whole binaries of documents, which is a nice feature, but lands more in the spectrum of mass transfer topics, rather than real-time application integration. what is event-driven architecture? as we now know what an event is, we can define what event-driven architecture is. the most basic understanding would be that it is an architecture that detects, reacts to and processes events. it is a distributed architecture that is composed of event producers and event consumers, which are the components (applications, systems, modules of applications) that contain business logic. the communication between them, as mentioned, happens usually via a message broker, so a dedicated technical application that facilitates asynchronous message exchange. context: integration vs. distributed systems as we have defined roughly what eda is, there is one more distinction to make. context matters! when using eda as an architectural style, there are different considerations an architect must take into account when designing a distributed system and when designing application integration. while with a single system this is usually a single team that has a high level of control over development and the domain, for integration that would turn into dealing with cross-domain connections, coordinating between several different development teams and system architects. this brings additional complexity and certainly will influence several aspects of implementation. furthermore it is worth noting that in distributed systems asynchronous communication may happen without the use of a specialized message broker, by using e.g. in memory queues. qualitative analysis as we did with point-to-point, let’s explore the qualities of event-driven architecture and look into a few pitfalls that can be crucial to consider when trying to apply this architectural style. for that we will be using a comparison table that was produced through a qualitative analysis of architectural styles taking several architectural characteristics into account. if you would like to learn more about this analysis or read how we define those characteristics, you can do so by reading this article, where we explain how this comparison was created. cost analysis let’s start with cost considerations of event-driven architecture as an architectural style. with this approach to ecosystem interoperability a message or event broker is used as a mediator between different domain systems. this brings an implication of development and operational costs being higher than with point-to-point. the use of a technical broker brings additional complexity. depending on the choice of a broker, this means that all domain systems that wish to communicate need to learn the brokers protocol as well as learn to properly produce and consume events. this increases the cognitive strain on development teams. on the other side message brokers are incapable of building sophisticated processing logic as they only facilitate communication, so using them does not require a dedicated development team that would implement changes. this brings the development cost to be higher than point-to-point, resulting in \uD83D\uDCB2\uD83D\uDCB2 for eda in this category. operational cost is higher as well, as there is another component in the ecosystem, which often requires a license to run as well as a team to maintain and operate it. this also increases overall operational effort in all environments to provide analysis and fixes for any issues that might occur, as there are more components that need to be looked into. this gives the operational costs a score of \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2. architectural change cost, even if it is an unpredictable variable, is fairly low (\uD83D\uDCB2\uD83D\uDCB2). this is due to the fact that a message broker is providing a separation layer between domain systems, splitting a dynamic quantum, that would be common for a point-to-point connection, into two or more quanta, depending on the architectural pattern used. this makes the domain systems loosely coupled between each other, so in turn easier to modify or entirely replaced. architectural and design time analysis let’s first take a look at the design time qualities of eda. like with point-to-point, the superpower of eda, in this area, is its simplicity (4⭐). implementing this architectural style is fairly simple in terms of architectural components. it is very clear and easy to understand. the reason the score is only 4⭐ is that this style forces all the domain systems to learn how to work with events alongside making additional p2p calls while processing them. secondly, the use of message brokers, especially patterns related to the use of topics as a communication medium, enables composability and by that also a certain level of reusability of events and related data models. however the downsides of eda as an ecosystem architectural style, listed below, bring the composability score to 3⭐. while overall the scores are higher than with p2p, there are aspects that should be looked into. if we explore the capability of eda to provide abstraction (2⭐), we can see that, like with p2p, we are bound by several types of coupling: semantic coupling being the most basic and unavoidable, contract coupling, which means being bound by the same data exchange contract afferent, as in for producers, when using queues to communicate many to one, efferent, as in for consumers, when using topics to communicate one to many or many to many, conversation coupling, if the chosen message broker does not allow to be protocol agnostic, this means there are many things to consider when putting this architectural style in a context of a specific implementation and choosing the right broker for a job. these would be the skills available in the organization or on the market, the protocol capabilities of systems that are to be integrated, etc.. then the various degrees of coupling influence the contract resilience (2⭐). if the communication is other than one to one, there is a design consideration that each change in the data model (contract) may be a breaking change, depending on how producers and consumers parse payloads and what is the data format used. this impact needs to be assessed early on, because it will influence every integration. lastly we should take a look into extensibility (2⭐), and here’s where the context of integration vs. distributed systems comes into play leading to a lower score than often expected from eda. this quality seems to be a lot stronger when designing a single domain distributed system, because it is a lot easier to extend the system with new modules or components that work within the same bounded context. when we’re dealing with ecosystem interoperability, while the same extensibility mechanisms apply (patterns to do many to one, one to many and many to many), they work cross-domain, with the cost of using various technologies, different bounded contexts or effort to adapt specific protocols. these cannot be disregarded just as implementation details and need to be considered beforehand. operational analysis moving on to the second group architectural characteristics, that describe the operational qualities of the architectural style. the foremost benefit of eda in this area is its performance (5⭐). some people might wonder why we scored it higher than p2p. there is one key difference here that is worth noting - responsiveness. synchronous communication that is the key element of p2p is always a blocking communication, which means that it impacts responsiveness, because it creates a dynamic coupling between the caller and the provider of the api. as mentioned before, using a message broker splits this quantum into two quanta, which means that the event producer does not rely on the performance of the consuming system(s) and can provide data to a group of consumers in parallel instead of calling them in sequence. this provides a performance boost, freeing up resources as soon as the communication with the broker is done. looking into further characteristics we can see that scalability (4⭐) is the next characteristic that is scored high in the comparison. this architectural style makes it a lot easier architecturally to scale-out your systems, because it does not require additional components for load balancing! using message broker queues enables the use of a competing consumers operational pattern, that by itself provides load balancing capabilities based on resource availability of consumers. depending on the message broker chosen, this may be also available with topics and configuring consumer groups. it is also possible to provide auto-scaling capabilities in the ecosystem based on monitoring of queues or topics of the message broker and the load on those communication structures. another superpower of eda that derives from its simplicity is testability (4⭐). the overall setup is fairly straightforward, as the only added component is a broker that technically facilitates communication and sometimes routing based on metadata, so it does not provide any additional logic (e.g. validation rules, content based routing, that needs to be specifically tested. looking further into operational characteristics, observability (3⭐) and auditability (2⭐) change slightly compared to p2p. while we can observe more within the ecosystem if proper monitoring is implemented to account for the message broker and its communication structures, it does not influence auditability. this is due to the fact that those characteristics still heavily depend on what can be extracted from business systems. the composition of the ecosystems and types of applications within it will have a great impact on what kind of data and metadata describing its operations can be extracted. lastly, what requires additional attention with eda as an architectural style is security (2⭐). while all of the components can be separately secured, the concern is not with those components, but rather the implications of using message brokers. if we want to leverage the use of topics, it is important to understand the consequences of “contract coupling” mentioned before. the consideration is not only around the brittleness of the contract, but also the fact that if we are broadcasting data via topics to several consumers, it means that all of them receive the same scope of data, without consideration of the actual scope used by a particular system. this means that while the usage might be proper and as designed, full scope of data will be most likely at least logged somewhere. this means that the data can be accessed by someone who is not supposed to do so, resulting in a security breach. conclusions event-driven architecture is a very useful architectural style, especially for ecosystems that are required to be highly performant and at the same time remain simple, testable and scalable. while it has its downsides, if properly governed and mitigated, it is a viable architectural option for small to medium companies that rely on real-time interactions within their business processes."
  },
  {
    "title": "Event-Driven One-way point-to-point",
    "raw": "event-driven one-way point-to-point integration patterns as we describe the architectural styles for ecosystems, for some of them we can identify distinct and repeatable architectural patterns that we can leverage to build better interoperability. as we dive into event-driven architecture, let’s take a look at the first of the integration patterns deriving from this architectural style.  pattern nameplate let’s start with a small summary, we dubbed the “pattern nameplate” (an analog to a device nameplate that you can find on any electrical device, that describes its basic characteristics).  name: one-way point-to-point communication mode: asynchronous architectural style: event-driven architecture common use cases:  decoupling from long running or unpredictable time processes - preventing blocking communication and therefore increasing responsiveness by decoupling time-consuming processes or processes that have unstable response time of backend systems from their consumers, competing consumers pattern, load balancing - distributing workload efficiently across multiple consumers for improved throughput and scalability, sequence processing (fifo) - ensuring messages are processed in the order they were received, which is crucial in certain scenarios. this requires a broker that has a fifo capability, queue based load leveling - providing the capability to smooth out the spikes in demand by temporarily storing excess events. architectural coupling:  contract coupling - the provider and consumer of the event are locked by an agreed contract, it is wanted within the bounds of a p2p communication, might become tighter, if it is extended into a broadcast or multicast pattern with additional consumers that only use a subset of the data model, data type and format coupling - the provider and consumer must have the same understanding of the data model types and format (e.g. json, xml, csv) conversation coupling - depending on the broker implementation, the consumer and provider may be locked by the protocol of the event broker, semantic coupling - unavoidable with any data exchange, operational coupling: there are distinct architectural quanta, one for the event producer, the other for event consumer, both overlapping on the event broker structure used (topic or queue).   diagram(s)  one-way point-to-point using a queue  alternative: one-way point-to-point using a topic  alternative: one-way point-to-point using a topic bridged to a queue pattern analysis one-way point-to-point, while fairly common, is not the most used pattern from the eda toolbox. it can be implemented in three ways, where each of those has distinct trade-offs. one-way p2p using a queue the simplest and most obvious form of point-to-point communication, where a queue (jms) or a direct exchange (amqp), both with persistence, is being used to facilitate communication and decouple the consumer from a provider. this essentially means that the consumer does not have to operate in the exact same window as the provider of the event. architectural considerations using this kind of structure enables the competing consumers pattern, enabling easy scaling of the consuming application, without the need for any special configuration. furthermore, the persistence of the queue enables load leveling and decoupling long running processes.  the trade-off of using a queue is that the extensibility on the consumer side is hindered, as only one-to-one and many-to-one communication is available. operational considerations this approach, in most event brokers, is easy to monitor and manage. they provide a lot of insight into the performance of the consuming system, which may prove beneficial in detecting anomalies, peaks or operational incidents, before they impact business. one-way p2p using a topic an uncommon practice for actual point-to-point communication is to facilitate it over a topic (jms) or a fanout exchange to a single subscriber (amqp). it is truly only applicable if the communication is to be extended by additional consumers. it is similar to using a queue, but only if the producer and consumer operate in the same time window, otherwise things get a little bit more complicated. architectural considerations using a topic is perfect to support extensibility, especially when considering a broadcast (fanout exchange) or multicast (topic exchange in amqp), so one-to-many communication. it provides the same level of extensibility as a queue towards multiple providers, so there is no issue in building many-to-one or many-to-many relationships by extending this pattern later on.  the trade-offs for using a topic are quite different. topics by default are not persistent, as they are the true fire-and-forget mechanisms. persistence in the form of a durable topic subscription or durable message (if such is available in a message broker), needs to be deliberately configured per consumer, which means that load leveling is not done by default. furthermore, using topics does not allow the competing consumers pattern by default and again it needs to be deliberately configured using consumer groups (again, if such option is available in the chosen message broker). this brings more complexity to this communication. operational considerations not all message brokers support monitoring a topic per subscriber, so while using a topic for a p2p communication is not an issue in terms of monitoring, it may be problematic if the communication is extended. if a durable subscription is enabled, then insight into the consumer performance, communication anomalies and detecting peaks is also available, as the number of messages awaiting pickup will be easily trackable.  one-way p2p using a topic bridged to a queue a third and more common option is to do p2p communication with a topic that is bridged (routed) to a queue. this option is a default operational option with message brokers implementing amqp (fanout or topic exchange routed to a queue) as their native protocol, although it is also possible with jms implementations, it requires additional configuration. architectural considerations using a topic bridged to a queue enables the benefits of using both structures. publishing the events to the topic enables extensibility for future uses as it is fairly easy to add additional queues to which data can be routed in a broadcast or multicast. additionally, consumers listening on a dedicated queue can utilize the competing consumers pattern by that scale easier.   the key trade-off here is that the communication becomes quite a bit more complex, and requires more attention to details in setup. operational considerations due to the routing there is less emphasis on monitoring the topic, because most information about consumption will be available over the dedicated queues. the downside is that it requires more attention to set up and maintain as more broker structures are involved in the communication.  additional considerations using a message broker as a mediator in communication means that the application producing the event does not have (or at least should not have) any knowledge of who the consumer of that event is. this also means that there is no mechanism within this communication for the producer to know if the event was delivered to the consuming party. while this communication may seem very simple, this implicitly means that the error handling in such communication needs to be handled by a third party that does not participate in it. this ranges from manual handling to automated workflows managing data cleanup and compensating updates, all of which depend on the business process and its requirements. conclusions the one-way point-to-point patterns are a useful thing to have in the integration toolbox that, in combination with other eda patterns as well as broker or api-led architecture, provides an easy and quick way to provide meaningful, real-time data."
  },
  {
    "title": "EDA Broadcast and Multicast",
    "raw": "event-driven broadcast and multicast a common pattern when you dive into the world of event-driven architecture, the phrase you hear most often is “pub-sub” or “publish-subscribe”, which seems to be the most commonly used data distribution pattern in this architecture. and for a lot of cases this will be very true, especially when we'd take a look at mqtt, where the only available structure on the broker is a topic. in essence, this pattern could cover all possible cases for the number of communication participants: one-to-one, one-to-many, many-to-one, and many-to-many. pattern nameplate name: broadcast and multicast communication mode: asynchronous architectural style: event-driven architecture common use cases:  distribution of data object events (create, update, delete), for example new product configuration, or an update done on a customer profile, architectural coupling:  contract coupling - the provider and consumers of the event are locked by an agreed contract schema, while it is wanted within the bounds of a p2p communication, might prove problematic with a broadcast or multicast pattern and multiple consumers that only use a subset of the payload described by the contract. this, if left unmanaged, may lead to potential data security problems and cause contract brittleness, data type and format coupling - the provider and consumers must have the same understanding of the data model types and format (e.g. json, xml, csv) conversation coupling - depending on the broker implementation, the consumer and provider may be locked by the protocol of the event broker, semantic coupling - unavoidable with any data exchange, operational coupling: there are two or more distinct architectural quanta, one for each event producer, and one for each event consumer, both overlapping on the event broker structure used (topic or queue).  diagram(s)  asynchronous broadcast/multicast using a topic  alternative: asynchronous broadcast/multicast using a topic bridged to queues  pattern analysis asynchronous broadcast and multicast are very common patterns, known also as publish-subscribe or pubsub. they are the most used pattern in the eda toolbox and not without a reason.  asynchronous broadcast/multicast using a topic using topics for broadcasting events is a very old pattern originating from jms1.0 (dated 5th of october 1988). while new protocols and analogous structures like exchanges in amqp emerged over time, and the standards developed further, the core idea for this type of asynchronous communication remains unchanged. the main use case for this pattern is to distribute data to many consumers at the same time decoupling them from the producer(s). it can be used like a queue for one-to-one communication, but by default it lacks persistence, which means that the producer(s) and consumers need to operate in the same time frame for the communication to be successful. that can be mitigated by setting up a durable subscription. architectural considerations using a topic (in jms) or a fanout exchange (in amqp) is a good way to support extensibility for data distribution, as it supports the topology of one-to-many and many-to-many communication. topics provide an additional benefit of having a selective subscription, meaning that the communication does not always have to be to all of the subscribers. this can be achieved by utilizing the selector in jms, or topic exchange routing key in amqp with a topic exchange (or other mechanisms like header/system exchanges) for each subscriber based on metadata passed through the message headers.  the trade-offs for using a topic are quite different than with a queue. topics by default do not support persistence, as they are the true fire-and-forget mechanisms. persistence in the form of a durable topic subscription (if such is available in a message broker), needs to be deliberately configured per consumer, which means that load leveling is not done by default. furthermore, using topics does not allow the competing consumers pattern by default and again it needs to be deliberately configured using consumer groups (again, if such option is available in the chosen message broker). this brings more complexity to this communication in the form of configuration that needs to be managed and maintained. operational considerations not all message brokers support monitoring a topic per subscriber, so while using a topic for a p2p communication is not an issue in terms of monitoring, it may be problematic if the communication is extended. if a durable subscription is enabled, then insight into the consumer performance, communication anomalies, and detecting peaks is also available to the extent of the implementation, as the number of messages awaiting pickup will be more easily trackable.  asynchronous broadcast/multicast using a topic bridged to a queue a second, less common option in jms, option for broadcast and multicast is using topics bridged (routed) to queues. this option is a default operational option with message brokers implementing amqp as their native protocol, although it is also possible with jms implementations, it requires additional configuration. architectural considerations using a topic bridged to a queue enables the benefits of using both queues and topics. publishing the events to the topic enables extensibility as it is fairly easy to add additional queues to which data can be routed in a broadcast or multicast. consumers listening on a dedicated queue can utilize the competing consumers pattern and with that enhance scalability and performance of consuming systems. if we wish to utilize the filtering within the subscription, this is done either by configuring a selector over the bridge per queue (jms) or using routing keys with topic exchanges (amqp). some broker technologies enable naming of structures that resembles a rest uri path, enabling filtering based on variables in the names and using wildcards.  the key trade-off here is that the communication becomes quite a bit more complex, and requires more attention to details in setup. luckily most of that can be automated to a degree. operational considerations due to the routing there is less emphasis on monitoring the topic, because most information about consumption will be available over the dedicated queues. the downside might be that it requires more attention to set up and maintain as more broker structures are involved in the communication. a crucial element that might be beneficial with jms servers is that in this scenario topic durables are not needed, so that reinforces maintainability as well as simplicity. additional considerations using a message broker as a mediator in communication means that the application producing the event does not have (or at least should not have) any knowledge of who the consumers of the broadcast event are . this also means that there is no mechanism within this communication for the producer to know if the event was delivered to the consuming parties. this implicitly means that the error handling in such communication needs to be handled by a third party that does not participate in it. this ranges from manual handling to automated workflows managing data cleanup and compensating updates, all of which depend on the business process and its requirements. while this pattern is immensely useful it needs to be thought out with this consideration in mind.  using the broadcast / multicast pattern may damage the security of your ecosystem. it is not due to the fact that topics or exchanges are insecure, but rather as a consequence of using them. while the producer and consumer are contract-coupled, when distributing events over a topic to multiple consumers, they all consume exactly the same message in terms of data scope, format and data model. there is no in transit adaptation (that is part of a different architectural style), so even if the consumer uses only a small subset of the data provided in the event, the full scope is delivered. this means that all consumers need to pay attention to what happens to that data in their system (logging, writing to storage, etc.), because if left unchecked and the security of the application is compromised, then that additional data, that came from the event, may be leaked. conclusions in conclusion, the broadcast and multicast pattern is a fundamental component of event-driven architecture, offering a flexible and scalable approach to data distribution. by leveraging topics or topic-to-queue bridges or proper exchanges, systems can effectively disseminate events to multiple consumers, creating looser coupling between components and enhancing system resilience. however it is worthwhile exploring all of the considerations, as some variants, depending on the implementation of applications or the chosen message broker, might be worse in terms of maintainability or operability of solutions. it is also important to consider the security implications of broadcasting sensitive data. this is something that would require additional careful considerations. by understanding the trade-offs and best practices associated with this pattern, architects can effectively design and implement event-driven systems that are both efficient and secure. and lastly it is crucial to understand that while we’re writing here about topics and exchanges, these are common names used outside of the field of application integration, and are also common in closely related data integration, where they have a bit different behavior and use cases."
  },
  {
    "title": "Event-Driven Callbacks",
    "raw": "event-driven callbacks a spark for discussion callbacks are the asynchronous analogs of the on-demand call. it is as simple and as complex as that. they are used when those synchronous capabilities to retrieve or process data cannot adhere to the requirements of responsiveness and performance. structurally they are a sequential composition of two basic eda patterns, e.g. broadcast and point-to-point or two point-to-point, etc.   callbacks are one of those patterns that might be a bit troublesome to form a philosophical point of view. they are fairly straightforward technically speaking, but it is debatable whether or not they fall under the category of event-driven. the key question to explore here is that if we are sending a message and we are expecting a response, perhaps not an instantaneous one, rather over time or in an undetermined time, is that still event-driven? if we consider the callback alone, without the triggering subscription or event, then it seems to always be an event of sorts, marking the completion of a processing task, an occurrence in a system, etc. let us consider this when exploring this pattern.  pattern nameplate name: callback communication mode: asynchronous architectural style: event-driven architecture common use cases:  data quality assurance based on api contracts, real-time subscribed data delivery as a result of an earlier data subscription, confirmation of event processing, decoupling processing with long or non deterministic processing time, architectural coupling:  contract coupling - the provider and consumer of the event are locked by an agreed data model, it is wanted within the bounds of a p2p communication, might not be, if it is extended into a broadcast or multicast pattern with additional consumers that only use a subset of the data model. furthermore, there are at least two contract couplings present in this pattern, depending on the variant (e.g. if dedicated callback queues are used) data type and format coupling - the provider and consumer must have the same understanding of the data model types and format (e.g. json, xml, csv) conversation coupling - depending on the broker implementation, the consumer and provider may be locked by the protocol of the event broker, semantic coupling - unavoidable with any data exchange, operational coupling: there are two or more distinct architectural quanta, one for each event producer, one for each event consumer, both overlapping on the event broker structure used (topic or queue), temporal coupling - for a callback to occur successfully, there must be first a successful event/command/message that results in said callback, related to: one-way point-to-point, broadcast and multicast, diagrams pattern diagrams  simple callback   callback to a broadcast with a single contract queue  callback to a broadcast with dedicated queues behavioral diagram  pattern analysis callback patterns at times seem like not the most common patterns in interoperability on the ecosystem level, and sometimes their limited or common use can be considered industry specific. in either case, they may prove extremely useful, both from the real-time data delivery and real-time operations points of view. they are the asynchronous equivalent of an on-demand call in some use cases, but can also serve several operational functions, e.g. delivery confirmation, statuses. it is also important to note that while using synchronous communication it is implied that there will be a response, issuing a callback is optional and left to the discretion of the callback producer and based on the business process. simple callback the simplest form of a callback is an exchange between two systems, where the callback producer first receives a message, state, event or a command from the callback consumer that triggers processing and the creation of a callback event. this is a fairly easy way to decouple two systems, by providing a separation in the form of a message broker between them.  architectural considerations while any messaging structure can be used to implement this pattern, let’s consider implementing it over queues only. this will be a dedicated flow, tailored to the specific exchange need. it has limited extensibility as it may be turned into a service fronted with queues as its api implementation, changing the topology from one-to-one-to-one to many-to-one-to-one. this means that there may be several callback consumers that produced a trigger for the producer to create a callback. in that case additional configuration needs to happen over the callback queue to enable proper filtering based on metadata (e.g. message headers, correlators, topic/queue taxonomy) so that the right consumer receives the right response. it is similar to a one-way point-to-point over a queue, because this is this exact pattern used twice in sequence.  operational considerations utilizing this pattern will provide easy scalability if needed for each party consuming messages from them, especially if there might be multiple threads producing callbacks to various triggers coming in from the callback consumer. this pattern, while may have varied performance due to the communication characteristics, processing time of the callback producer, which involves domain complexity, it is useful for supporting responsiveness of the callback consumer as the process is not confined to a single thread that becomes blocked awaiting the callback.  callback to a broadcast with a single contract queue a different callback pattern, following one-to-many-to-one topology, that has more of an operational use, chaining a broadcast/multicast and an one-way point-to-point patterns in sequence. one of the key use cases for this pattern is mitigating the operational lack of delivery or processing confirmation, which makes error handling and it operations a bit more difficult. this might at times be crucial for specific business areas, e.g. product configuration, where the business needs to be sure that all sales channels and order processing have the same product definitions (e.g. bundles, promotions). in that case a lack of callback to the data master confirming successful processing or a callback signalling processing errors will give more operational agility. this unfortunately comes at a trade-off that the data master must be aware of all consumers that need to send an operational callback. these callback messages could be considered to be events, but might as well be state. architectural considerations given the broadcast nature of a callback trigger, this pattern will support extensibility. with a callback originating from multiple producers to a single queue the key consideration is that all of those producers are bound to a single contract managed by the callback consumer. this needs to be considered when designing, as this contract coupling may be a problem with high volatility of requirements, as any change to the scope of data will impact all of the communication participants.  operational considerations from an operational perspective this kind of callback using the queue topology of many-to-one might be troublesome to monitor, as there are many applications writing to a single queue. that would require more effort from the operations teams in terms of monitoring the message broker for which producers actually wrote to the queue. callback to a broadcast with dedicated queues callback to a broadcast with dedicated queues is probably the rarest of the callback patterns. not only does it require the most effort to set up, but also facilitates more refined and deliberate processes. this key use case is that all the callback producers react to an event published and that triggers their processing resulting with a callback that is specific to what the functionality of each of those event processors is. if each processor delivers different data as a callback to a trigger in this scenario, this means that each one will have a different schema for said data, to properly separate objects and avoid contract brittleness. this also means that ownership of the schema and each callback queue lies within the scope of the respective callback producer as they are governing the data transferred. architectural considerations first thing that comes to mind is that this setup is complex, and while it may seem simple for each callback producer, it is a lot more complicated for the consumer of all those callbacks as now that application needs to implement processing logic to consume multiple endpoints as well as various contracts. additionally if we’d  consider replacing the dedicated queue with a topic or topic bridged to a queue (changing the topology from one-to-many-to-one, to one-to-many-to-many), we can create extensibility and reusability of the callback produced. this later can be used in additional contexts for whichever application this data might bear relevance. this in turn contributes to even better real-time communication within the ecosystem. operational considerations turning again to the operational perspective, this variation of the callback pattern will be easier to maintain and operate as there is a clear separation for each callback producer, so it will be easier to spot if for some reason there is a communication breakdown. otherwise, operationally they do not differ from one-way point-to-point or broadcast/multicast patterns, which are the basis for this communication. conclusions callback is a useful pattern that enables the organization and the data transfers between applications to be more agile and run in real-time, with the business applications acting on events as they happen rather than over time polled with a delay. while it may present some additional complexity and operational challenges, the benefit outweighs the cost.   additionally, here we can see that it is not straightforward whether all messages we send are actual events. it is worth considering what is the process as a whole as in some specific cases we will be dealing with commands and states that will be triggering the event-driven behaviour."
  },
  {
    "title": "Event-Driven Waiter Pattern",
    "raw": "event-driven waiter pattern processing, please do not wait... as we dive deeper into event-driven architecture (eda), there is a specific scenario that deserves attention on its own. as we already established that eda patterns can be used to loosen the coupling between different applications and systems so they can operate in a more independent manner. the waiter pattern builds on top of that capability allowing the applications that produce data over a long span of time to deliver them in an organized and standardized way.  pattern nameplate name: waiter pattern communication mode: asynchronous architectural style: event-driven architecture common use cases:  real-time requested data delivery in chunks as they are produced as a result of an earlier command or event, delivery of larger amount of data in parts, parallel complex processing chunked based on a single event/command. architectural coupling:  contract coupling - the provider and consumer of the event are locked by an agreed data model, it is tolerable within the bounds of a p2p communication, might not be, if it is extended into a broadcast or multicast pattern with additional consumers that only use a subset of the data model, conversation coupling - depending on the broker implementation, the consumer and provider may be locked by the protocol of the event broker, operational coupling: semantic coupling - unavoidable with any data exchange, there are two or more distinct architectural quanta, one for each event producer, one for each event consumer, both overlapping on the event broker structure used (topic or queue). temporal coupling - for a subscription callback to occur successfully, there must be first a successful event/command/message that results in said callback. related to: callback pattern diagram(s) behavioral diagrams  waiter pattern behaviour pattern analysis the core function of the waiter pattern is to provide value with produced data as soon as possible. depending on the business scenario, and in this case quite a specific one, this data will be delivered at least once, meaning that this pattern behaves like a callback in its simplest form. the real value of it is in the more advanced scenario, where the callback producer’s processing time is very long - minutes, up to hours, but the callback data can be partitioned into usable chunks and sent as separate callbacks. this might seem like a subscription to events like webhooks, but there is one key difference - the processing and publication of callbacks is temporally coupled to the requesting party. without the initial trigger message (command or event), no data will be produced.   an example of such a mechanism would be a callback function for a web crawler, which is charged with crawling through news sites in search of specific keywords, like news about companies. usually these news are copied from site to site with very little addition, so the job of such crawlers is to identify a theme of an article and aggregate all news on the same theme, then deliver a summary and sources. if you span that across a time frame of for example two years, that is a lot of data to cover and it will take a lot of time. packaging each theme into a single deliverable callback brings value faster to the business as their actions are not blocked entirely by waiting for the processing to finish. and sometimes partial results might be sufficient to finish work, such as risk assessment. architectural considerations given the scope of use cases for the waiter pattern, essentially all callback patterns are viable, as they largely depend on the number of event processors producing callbacks and whether or not they all produce data in the same data model. in that sense architectural considerations are the same as described for callbacks.  what can be an additional consideration to be had, is that since the number of callbacks will be usually more than one per event triggering the processing, this requires mindful handling of persistence and errors, especially when running parallel jobs. as there might be a connectivity problem (as per the first fallacy of distributed computing), the number of messages pending being sent from the callback producer or delivered and temporarily stored on the message broker, will gradually increase. this in turn might pose a risk of overwhelming the message broker and disabling the event based communication for all involved, making it important to be managed on an architectural level first. operational considerations turning to operational aspects of the waiter pattern, other than the aforementioned inheritance from callbacks, communication metadata might play a more important role. correlation identifiers may be especially useful, not only to track and correlate log data between various instances of callbacks from a single processing job, but also to provide robust statistics and reporting on services and data usage. this is especially crucial if the callbacks are turned into broadcasts enabling other consumers, than the original requestor, to subscribe to that data, as this may provide an easy way to track all consumption. conclusions waiter pattern as a specific case of a callback pattern is a very useful tool in an integration architects toolbelt as it provides additional capabilities to balance long processing times clashing with low time to market, that seems to be a very common business driver of the information age. it enhances the agility of data consumers to provide value to the end user in a timely manner.  however it is important to consider the error handling, persistence and operational aspects of this pattern, especially in high volume scenarios, as, if not addressed, they may lead to critical failures, severely impacting communication. this needs to be defined and implemented in cohesion with the business process, as these things will be case specific. luckily if managed properly, this pattern can provide significant value to any organization."
  },
  {
    "title": "Broker Architecture",
    "raw": "when complexity calls there comes a certain point in an ecosystem's growth, when due to its expansion and business growth, operating and maintaining point-to-point, or event-driven interoperability becomes a nightmare of tangled dependencies for each system involved in the communication. all of this is the result of complexity that is not only specific to the business domain, which dictates functional requirements for each of those systems, but also due to the communication overhead that forces the systems to manage connections, various api contracts, data access logic, or credentials. all of this builds up to a hefty load of work, shifting the balance of time consumed on operations and development. this can further lead to bottlenecks in project development or complete stop of further growth as the team will be busy with solving production issues or locked in a fight between development and operations. this is where we can find help by looking into a law postulated in the mid-1980s.  “every application must have an inherent amount of irreducible complexity. the only question is who will have to deal with it.” law of conversion of complexity (tesler’s law)  while larry tesler was mostly concerned over the interactions between the software and its users, that is why his law is mostly known in the ux discipline, there is a very important lesson we can learn from it in terms of interoperability as interaction between two different systems. we can move most of the communication complexity away from the domain applications. it will not be reduced, but managed elsewhere.  “complexity is not bad. it’s confusion that’s bad. forget about simplicity; long live well-managed complexity.”  don norman, author of the design of everyday things.  complexity managed by mediation if we assume that complexity can only be moved, not reduced, then we can certainly move the complexity of communication and reduce it to the bare minimum for all participants. the easiest way to do so is to introduce a broker (a.k.a. a mediator).  broker 1\t: someone who acts as an intermediary [...] 3\t: someone who sells or distributes something  mediator 1\ta : one that mediates \tespecially : one that mediates between parties at variance [...]  if we look at those definitions from a perspective of it ecosystems, an integration broker in a broker architecture acts as an intermediary between two parties that have differing requirements. the key word here is ‘differing’, as this is where the complexity hides. communication is fairly simple if everyone speaks the same language and has the exact same definition of every single word. this translates to applications being developed in the same coding languages, having data models, represented by data in a specific format like json or xml and lastly managing the semantic coupling on the level of data. unfortunately this kind of situation is rarely the case in the modern it landscape, and this is where an integration broker and broker architecture come into play. what is broker architecture broker architecture is an ecosystem architectural style that introduces an infrastructural component, the integration broker, into the it ecosystem, that is responsible for handling communication complexity, that is:  systems using different communication protocols,  various contract data models,  mismatched data formats,  orchestration, observability, abstraction or extensibility capabilities. it inherits certain qualities from the previous architectural styles and builds new ones on top of those already existing. the use of point-to-point communication now shifts from system calling each other to invoking services exposed by the integration broker that orchestrates communication. on top of that we gain new capabilities, such as the ability to initiate the communication completely externally of the business systems by means of schedulers or change data capture mechanisms. lastly the integration flows in an integration broker have a workflow nature that is usually tailored to the business process requirements which it needs to support. qualitative analysis as we did with point-to-point and event-driven architecture, let’s explore the qualities of broker architecture and look into a few pitfalls that can be crucial to consider when trying to apply this architectural style. for that we will be using a comparison table that was produced through a qualitative analysis of architectural styles taking several architectural characteristics into account. if you would like to learn more about this analysis or read how we define those characteristics, you can do so by reading this article, where we explain how this comparison was created.  architectural characteristic name point-to-point spaghetti architecture event-driven architecture broker (mediator topology) api-led architecture development cost \uD83D\uDCB2 \uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 operational cost \uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 architectural changes cost \uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2\uD83D\uDCB2 \uD83D\uDCB2\uD83D\uDCB2 abstraction ⭐ ⭐ ⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐⭐ contract resilience ⭐ ⭐ ⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ simplicity ⭐⭐⭐⭐⭐ ⭐ ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐ composability ⭐ ⭐ ⭐⭐⭐ ⭐⭐ ⭐⭐⭐⭐⭐ extensibility ⭐ ⭐ ⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ testability ⭐⭐⭐⭐⭐ ⭐ ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐ scalability ⭐⭐ ⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ performance ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐ security ⭐⭐⭐ ⭐ ⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ observability ⭐⭐ ⭐ ⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐⭐ auditability ⭐⭐ ⭐ ⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐  cost analysis considering how broker architecture changes the landscape in comparison to point-to-point or event-driven architecture, it is clear that the cost overall will be higher in comparison. as we stated at the beginning of this article, we are moving complexity away from the domain systems, which means, we are moving development and operational cost to an integration broker. this comes with a need to hire or retrain developers to have the right skill onboard. next to that, the business systems development team can work protocol agnostic, as the integration broker is capable of providing a protocol that is best suited to the business system needs and technology used to build it. this limits the sprawl of development costs to an extent and lands broker architecture at 2\uD83D\uDCB2.    operational costs in broker architecture are similar to eda (3\uD83D\uDCB2), mostly due to the effect of moved complexity. there is the cost of licensing, which can influence the overall cost as it will vary between different technology providers and their licensing models that depend on several factors (e.g. usage, deployment mode). the operational effort on environments and operating the integration broker itself can be partially compensated within the technology licenses if an ipaas technology is chosen.   architectural change cost is a bit higher for the broker architecture (3\uD83D\uDCB2) due to the fact that it is a completely separate system with a distinct workflow nature. all integration flows are dedicated, which means they are more coupled to the business systems and do not provide as much abstraction as is possible. this means that while changes might be entirely contained within the integration broker, they will impact the whole integration flow. the changed business system needs to be deployed with it as a single deployment unit. architectural and design time analysis moving on to architectural and design time qualities of broker architecture, we can see from the qualitative analysis that it is a well-rounded architecture having average scores in all characteristics in this category. let’s start with the one that scores the lowest - composability. the reason why it is scored with only 2⭐ is because of the integration broker workflow nature. the reusability in each integration flow is mostly limited to code, e.g. reusable wrappers, connectors, perhaps some standardized mapping functions or transformations, but it is rarely feasible to create reusable services, hence it is harder to use business systems and integration flows as composable building blocks of the ecosystem. removability, as part of composability, is a bit easier to achieve, given that all changes are encapsulated in the integration platform, but it has an operational impact on all integration flows and systems related to the system that is replaced, triggering full regression tests with each change. it is the downside of moving the communication complexity from those systems into the integration broker.   moving on to simplicity (3⭐), it is quite obvious that the landscape overall became a bit more complex, giving broker architecture a one point lower score than it was with eda. it is worth noting that this complexity is only superficial, as introducing a new system does not have to translate directly to higher complexity, as it is usually the number of interactions (operational coupling) that is the better measure. the key is the scale as broker architecture will be used in more complex environments than eda and point-to-point, so it aligns with the growth of the business complexity partially moved to the integration broker. the simplicity of this architectural style supports time to market very well, as usually the integration flows are easier to develop with the right technology and a dedicated team of developers that specialize in such tasks.   the element that sets this architectural style apart from the previous styles is the capability to provide abstraction (3⭐) of business systems contracts, data models, data access logic, credentials, etc., which, if used properly, can help lessen the workload on development teams, so they can focus on functional requirements and not try to solve communication riddles. this is directly tied to contract resilience (3⭐) that is a lot easier to achieve as a lot of the changes to contracts with downstream systems can be encapsulated within the broker and will not impact the upstream applications. lastly, with the integration brokers enabling protocol agnostic communication, there is more support for extensibility (3⭐), as it is a lot easier to add new services and systems to the ecosystem if there is less adjustment needed from them to be integrated.  operational analysis let’s now take a look at the operational characteristics of the broker architecture, where things are a little more varied. starting with testability (3⭐), we can see that it is lower proportionally to simplicity. introducing a new system that facilitates communication makes testing a bit more difficult. yet, due to the fact that all orchestration, transformation and data access logic is contained within a single integration flow per use case, the effort increase needed to test communication end-to-end is not that significant, which means that there is a fairly low impact on time to market.  moving on to characteristics describing how this architectural style operates under load of messages it will be less performant than point-to-point or eda, due to the fact that we are introducing latency in a form of an integration broker, which has its logic to execute, the overall performance (3⭐) of the ecosystem will be impacted, which is a trade-off compared to the previous architectural styles. how severe this impact will differ between implementations is based on a number of factors, like, but not limited to:  deployment mode - cloud vs on premise,  runtime operations - self-hosted runtime vs managed services, chosen technology - is it chosen to facilitate the right needs, like on-demand, event communication or batch transfers, integration flow complexity - simple p2p flows, simple orchestration or complex bpm like orchestrations (which we do not advise), the end result might noticeably differ from a qualitative analysis result as some of the factors may be improved in various ways. which leads us to a very similar situation with scalability (4⭐), as it will also be dependent on similar factors. luckily since most modern integration brokers are built in a microservices architecture or as serverless functions, they have robust scalability features, including automatic horizontal scaling if needed. this, combined with good load balancing capabilities, enables the integration flows to be separately scalable, providing availability that easily matches requirements of all systems involved in the communication.   looking further into operational characteristics we find observability (3⭐) which does not really differ from eda in regards to score, but with broker architecture it will provide a completely different set of observable data and metadata. since nearly all data is supposed to pass through the integration broker it is a great place to gather intelligence as to where data is used, as well as how often it is requested or distributed, and what is the performance of business systems in terms of interoperability. the only downside is that since each integration flow is a dedicated workflow, that metadata per system will be distributed among many flows and needs to be aggregated and classified before it can be used. tying it to auditability (4⭐), when observability is properly managed, the integration broker becomes a valuable source of information about the ecosystem and often a source of truth on general it operations and the consumption of data. these are very important aspects when there is a need for proof for root cause analysis (rca) or audits. if combined with managed observability from business systems, aggregated to a single logging and monitoring, or an analytics platform, it can give a very wide overview of all processes. lastly we have security (4⭐), which is scored considerably higher in the qualitative analysis. the sole fact that a mediator is used, limiting the access to other systems from any business application, is a big step towards securing the ecosystem. if a system and its data are compromised, that limits the access to that particular system, without giving any footholds to other systems that are bound with the breached one by communications. combined with observability and real-time traffic and metadata analytics, broker architecture can enable anomaly detection, helping to automate and boost the speed of reaction to potential security breaches. if a breach is identified, the compromised system can be isolated swiftly by stopping specific integration flows that are inbound or outbound to that system, further limiting the damage.  conclusions tesler's law highlights the inevitable presence of complexity in any software system. broker architecture offers a pragmatic approach to managing this complexity by strategically shifting it away from individual applications and towards a dedicated integration layer. by centralizing communication logic, data transformation, and protocol handling within the broker, this approach reduces the cognitive load on application developers and simplifies the development and maintenance of individual systems. this not only enhances developer productivity but also improves business systems maintainability, reduces the risk of errors, and ultimately enables organizations to adapt more effectively to the ever-changing demands of the modern business landscape. while the introduction of a broker introduces a new layer of complexity, it is a managed complexity, allowing for better control, observability, and scalability of the overall system."
  },
  {
    "title": "Pattern of an antipattern",
    "raw": "in the sea of patterns working in it we hear a lot of jargon, phrases that have a specific meaning often only in this industry. while those terms are fairly common, and we hear them a lot, everyone has their own definitions as to what they actually mean. a large part of the role of an it specialist, an architect especially, is to clarify those definitions and foster ubiquitous language in conversations within it and when talking with any stakeholders.  one of such words that constantly escapes a proper definition in common use is an antipattern. we hear it all the time in meetings, discussions over code and changes made to systems. most commonly it is used as a description of something that should not be done that particular way, meaning it’s been done badly.  an uncharted island the word antipattern in all of its various spellings is not found in common language as it is limited to software engineering, project management, or business processes.  andrew koenig, who coined the term ‘antipattern’ back in 1995 in the “journal of object-oriented programming, vol 8”, defined the the term as:  an antipattern is just like a pattern, except that instead of a solution it gives something that looks superficially like a solution but isn’t one.  a lot has changed since 1995, so the definition also shifted a bit here and there:  an antipattern is a technique that is intended to solve a problem but that often leads to other problems. an antipattern is practiced widely in different ways, but with a thread of commonality.  - sql antipatterns volume 1 bill karwin antipatterns are common approaches to recurring problems that ultimately prove to be ineffective.  - rails antipatterns: best practice ruby on rails refactoring chad pytel, tammer saleh an antipattern is a repeatable process that produces negative results.  - fundamentals of software architecture, 2nd edition neal ford, mark richards the quest for the treasure all of those definitions above, despite being slightly different, have a common theme: the antipattern is not the solution to the problem or might lead to additional problems that greatly outweigh the benefits of the solution. someone might generalize, and we believe many do, that antipatterns are simply bad and you should avoid them.  like all things that began to have a life of its own, antipatterns have a grain of truth in them, as they must have been used somewhere and worked benefiting the overall solution at a certain point. otherwise they would not have the characteristics of commonality attached to them in all of the definitions. to understand the nature of antipatterns it is crucial to understand their context and how they came to be and there might be several various scenarios for their origins:  outdated gold - a common solution or pattern from a different time, as there are still a lot of solutions or ideas circulating in the industry that were born and implemented a long time before the first microservices and large scale distributed systems. trying to apply some of those solutions, e.g. trying to create a file cache, might prove highly ineffective in distributed solutions, while it worked perfectly fine with monolithic systems hosted on bare metal. there might be different and better ways to do things or the business drivers that created them already shifted too far for the original idea, for them to support the business in a meaningful way, it worked so it must be universal (false equivalence fallacy) - there are cases where a specific solution worked for a specific use case and it was known to be of great benefit to the business. there are bound to be some that will try to replicate the exact or very similar solution in the context of their own company without first building an understanding of why that solution worked in the first place and what was the context of the original solution's use and creation. this can lead to completely mismatched solutions, implemented just because someone mistakenly thought that they’re solving the same problem, we have always done it this way (status quo bias) - a tendency to preserve the ways of working, used patterns, if not from fear of change and loss, then from rigidness of the change process and the overwhelming effort needed to produce a stable change (e.g. acceptance processes, board reviews, sign-offs, security assessments). as deviating from the norm is risky, this often results in inaction, leaving us with patterns that may be outdated and inadequate. additionally, there is the aspect of feeling overwhelmed by the number or complexity of available options, opting towards the status quo helps to avoid the stress of decision making. others are already doing it! (bandwagon effect) - a phenomenon where a certain approach, behavior, technology is adopted simply because others are doing so. it is even greater if the solution is presented by a well known company that is considered to be some sort of an industry leader (e.g. netflix, google, salesforce), where the presentation often has no goal of an actual knowledge transfer, but more of a marketing aspect. by that all trade-offs, operational and organizational problems, costs are hidden behind the story tailored to bring attention and revenue to the company. all of these villain origin stories have one single action that was missing - a trade-off analysis, that would check if those solutions actually match the requirements and context of the ecosystem they are to be implemented in. but that also leads to another important conclusion showing the nature of an antipattern - it is still a pattern and it will surely be applicable in a specific, probably niche, scenario! the assumption that it is always bad is simply false. as with everything: it depends.  ‘x’ that marks the spot if we consider all of the above we can actually refine the definition of an antipattern. from our perspective it should look something like this:  an antipattern is a common solution to a problem that is not the right fit in a specified context or a solution that might lead to additional problems that greatly outweigh its benefits.  let's delve deeper into this definition and explore all the clues on the map of patterns, exploring the implications:  common solution: antipatterns aren't difficult to comprehend or unusual; they're often widely recognized and readily adopted solutions. this popularity can stem from their initial effectiveness in certain scenarios, their simplicity, or their promotion by influential figures or companies. problem-solution mismatch: the core issue with antipatterns lies in their incorrect use. a solution that works well in one context can be disastrous in another. this mismatch can arise from differing requirements, constraints, or unforeseen circumstances. contextual sensitivity: the effectiveness of a solution is highly dependent on the context in which it's applied. factors such as the problem domain, the available resources, the organizational culture, and the time constraints can all influence the suitability of the solution. hidden costs: antipatterns often come with hidden costs that aren't immediately apparent. these costs can include increased complexity, reduced maintainability, performance degradation, or unintended consequences that ripple through the ecosystem. benefit-cost imbalance: even when a solution provides some benefits, it can still be considered an antipattern if the associated various costs significantly outweigh those benefits. this highlights the importance of considering the long-term implications of a solution. conclusions the concept of antipatterns, while often misunderstood, plays a crucial role in the ever-evolving landscape of software engineering and it architecture. by recognizing the common origins of antipatterns and understanding their contextual nature, we can navigate the complexities of solution design and implementation more effectively. the key lies in striking a balance between leveraging established patterns and adapting them to the specific requirements and constraints of each unique scenario. ultimately, by fostering a culture of critical thinking and informed decision-making, where solutions are not blindly adopted but thoughtfully evaluated and tailored to achieve optimal outcomes, we can avoid costly and problematic patterns which we would later name antipatterns."
  }
]
